{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c76c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2626c359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ File not found: /Users/fabiancordenod/code/fqbq69/BIMpredict-/data_finetuned/murs_concat_fine_tuned.csv\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/Users/fabiancordenod/code/fqbq69/BIMpredict-/data finetuned/murs_concat_fine_tuned.csv\"\n",
    "if os.path.exists(csv_path):\n",
    "\tdf = pd.read_csv(csv_path)\n",
    "\tprint(df.shape)\n",
    "\tprint(df.head())\n",
    "else:\n",
    "\tprint(f\"❌ File not found: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4e3099",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VarianceThreshold\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Supposons que ton dataframe est déjà chargé :\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# =======================\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 1️⃣ Nettoyage avancé\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# =======================\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Remplir les valeurs manquantes basiques\u001b[39;00m\n\u001b[1;32m     15\u001b[0m df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Supposons que ton dataframe est déjà chargé :\n",
    "df = df.copy()\n",
    "\n",
    "# =======================\n",
    "# 1️⃣ Nettoyage avancé\n",
    "# =======================\n",
    "\n",
    "# Remplir les valeurs manquantes basiques\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Supprimer les colonnes très manquantes (ex : plus de 80% de NaN initialement)\n",
    "missing_ratio = df.isna().mean()\n",
    "cols_to_drop = missing_ratio[missing_ratio > 0.8].index.tolist()\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "print(f\"Colonnes supprimées pour trop de NaN : {cols_to_drop}\")\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 4️⃣ Réduction de dimension simple\n",
    "# =======================\n",
    "\n",
    "# Filtrer uniquement les features numériques pour la variance\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "selector.fit(df[num_cols])\n",
    "\n",
    "low_variance_cols = num_cols[~selector.get_support()]\n",
    "print(f\"Colonnes à faible variance supprimées : {low_variance_cols.tolist()}\")\n",
    "\n",
    "df.drop(columns=low_variance_cols, inplace=True)\n",
    "\n",
    "# =======================\n",
    "# 5️⃣ Résultat final\n",
    "# =======================\n",
    "\n",
    "print(\"✅ Data fine-tuning terminé. Nouvelle forme du dataset :\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# =======================\n",
    "# 6️⃣ Sauvegarde optionnelle\n",
    "# =======================\n",
    "\n",
    "df.to_csv(\"murs_concat_fine_tuned.csv\", index=False)\n",
    "print(\"✅ Données sauvegardées sous murs_concat_fine_tuned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les targets multi-label\n",
    "targets = [\n",
    "    \"011ec_lot\",\n",
    "    \"012ec_ouvrage\",\n",
    "    \"013ec_localisation\",\n",
    "    \"014ec_mode_constructif\"\n",
    "]\n",
    "\n",
    "# Nettoyer les noms de colonnes si besoin (reprends ta fonction clean_col)\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_col(col):\n",
    "    col = unicodedata.normalize('NFKD', str(col)).encode('ascii', 'ignore').decode('utf-8')\n",
    "    col = col.lower()\n",
    "    col = re.sub(r'[^a-z0-9]+', '_', col)\n",
    "    col = col.strip('_')\n",
    "    return col\n",
    "\n",
    "df.columns = [clean_col(c) for c in df.columns]\n",
    "\n",
    "# Vérifier la présence des targets\n",
    "targets_in_df = [col for col in targets if col in df.columns]\n",
    "if not targets_in_df:\n",
    "    raise ValueError(f\"Aucune colonne cible trouvée dans le CSV. Colonnes disponibles : {df.columns.tolist()}\")\n",
    "\n",
    "X = df.drop(columns=targets_in_df)\n",
    "y_multi = df[targets_in_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559229df",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_features = X.select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline entraîné et sauvegardé sous randomforestmurspipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "# Convertir les colonnes catégorielles en string\n",
    "for col in cat_features:\n",
    "    X[col] = X[col].astype(str)\n",
    "\n",
    "# Supprimer les colonnes numériques entièrement vides\n",
    "num_features = [col for col in num_features if not X[col].isnull().all()]\n",
    "# Supprimer les doublons dans la liste des colonnes et s'assurer qu'elles existent dans X\n",
    "all_features = num_features + cat_features\n",
    "all_features = list(dict.fromkeys(all_features))  # préserve l'ordre et retire les doublons\n",
    "# S'assurer que chaque colonne n'apparaît qu'une seule fois dans X\n",
    "all_features = [col for col in all_features if col in X.columns]\n",
    "X = X.loc[:, ~X.columns.duplicated()]  # retire les colonnes dupliquées dans X\n",
    "X = X[all_features]\n",
    "\n",
    "# Pipeline de prétraitement\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), num_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), cat_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pipeline complet\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', MultiOutputClassifier(RandomForestClassifier(n_estimators=500, random_state=42)))\n",
    "])\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_multi, test_size=0.2, random_state=42)\n",
    "\n",
    "# Supprimer les lignes avec NaN dans les targets\n",
    "train_notna = y_train.notna().all(axis=1)\n",
    "test_notna = y_test.notna().all(axis=1)\n",
    "X_train, y_train = X_train[train_notna], y_train[train_notna]\n",
    "X_test, y_test = X_test[test_notna], y_test[test_notna]\n",
    "\n",
    "# Entraînement\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Sauvegarde du pipeline\n",
    "joblib.dump(pipeline, 'randomforestmurspipeline.pkl')\n",
    "print(\"✅ Pipeline entraîné et sauvegardé sous randomforestmurspipeline.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIMpredict2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
