{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12644fea",
   "metadata": {},
   "source": [
    "# Bim_Predict_Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4d62f0",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define project folder paths\n",
    "# Data directories\n",
    "BASE_DIR = \"../../\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "PREDICTED_DATA_DIR = os.path.join(DATA_DIR, \"predicting_data\")\n",
    "TESTING_DATA_DIR = os.path.join(DATA_DIR, \"testing_data\")\n",
    "\n",
    "# Model directories\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ML_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/machine_learning\")\n",
    "DL_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/deep_learning\")\n",
    "OTHER_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/other\")\n",
    "\n",
    "# Python modules and plots directories\n",
    "PYTHON_MODULES_DIR = os.path.join(BASE_DIR, \"python_modules\")\n",
    "PLOTS_DIR = os.path.join(BASE_DIR, \"plots\")\n",
    "\n",
    "# List of directories to create\n",
    "directories = [\n",
    "    RAW_DATA_DIR, PROCESSED_DATA_DIR, PREDICTED_DATA_DIR,\n",
    "    MODELS_DIR, ML_MODELS_DIR, DL_MODELS_DIR, OTHER_MODELS_DIR,\n",
    "    PYTHON_MODULES_DIR, PLOTS_DIR\n",
    "]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# List all Excel files in RAW_DATA_DIR\n",
    "excel_files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith(\".xlsx\") or f.endswith(\".xls\")]\n",
    "\n",
    "# Dictionary to store DataFrames for each file and sheet\n",
    "dataframes = {}\n",
    "\n",
    "# Process each Excel file\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(RAW_DATA_DIR, file)\n",
    "    print(f\"Loading: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # Load Excel file\n",
    "        excel_data = pd.ExcelFile(file_path)\n",
    "\n",
    "        # Load all sheets dynamically\n",
    "        for sheet_name in excel_data.sheet_names:\n",
    "            df = excel_data.parse(sheet_name)\n",
    "\n",
    "            # Save DataFrame with a unique identifier\n",
    "            dataframes[f\"{file}_{sheet_name}\"] = df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# Display summary of loaded data\n",
    "print(f\"\\nTotal files processed: {len(dataframes)}\")\n",
    "for key, df in dataframes.items():\n",
    "    print(f\"Loaded DataFrame: {key}, Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022be65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define required columns dynamically\n",
    "required_columns = {\n",
    "    \"Murs\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"Hauteur\",\n",
    "             \"Epaisseur\", \"AI\", \"AS\", \"Sols en intersection\", \"Sols coup√©s (u)\", \"Sols coup√©s (Ids)\",\n",
    "             \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Sol au-dessus\", \"Sol en-dessous\", \"Fen√™tres\", \"Portes\",\n",
    "             \"Ouvertures\", \"Murs imbriqu√©s\", \"Mur multicouche\", \"Mur empil√©\", \"Profil modifi√©\", \"Extension inf√©rieure\",\n",
    "             \"Extension sup√©rieure\", \"Partie inf√©rieure attach√©e\", \"Partie sup√©rieure attach√©e\", \"D√©calage sup√©rieur\",\n",
    "             \"D√©calage inf√©rieur\", \"Mat√©riau structurel\"],\n",
    "\n",
    "    \"Sols\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"Murs en intersection\",\n",
    "             \"Murs coup√©s (u)\", \"Murs coup√©s (Ids)\", \"Murs coupants (u)\", \"Murs coupants (Ids)\", \"Poutres en intersection\",\n",
    "             \"Poutres coup√©s (u)\", \"Poutres coup√©s (Ids)\", \"Poutres coupants (u)\", \"Poutres coupants (Ids)\",\n",
    "             \"Poteaux en intersection\", \"Poteaux coup√©s (u)\", \"Poteaux coup√©s (Ids)\", \"Poteaux coupants (u)\",\n",
    "             \"Poteaux coupants (Ids)\", \"Ouvertures\", \"Sol multicouche\", \"Profil modifi√©\", \"D√©calage par rapport au niveau\",\n",
    "             \"Epaisseur\", \"Li√© au volume\", \"Etude de l'√©l√©vation √† la base\", \"Etude de l'√©l√©vation en haut\",\n",
    "             \"Epaisseur du porteur\", \"El√©vation au niveau du noyau inf√©rieur\", \"El√©vation au niveau du noyau sup√©rieur\",\n",
    "             \"El√©vation en haut\", \"El√©vation √† la base\", \"Mat√©riau structurel\"],\n",
    "\n",
    "    \"Poutres\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"AI\", \"AS\",\n",
    "                \"Hauteur totale\", \"Hauteur\", \"Sols en intersection\", \"Sols coup√©s (u)\", \"Sols coup√©s (Ids)\",\n",
    "                \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Sol au-dessus\", \"Sol en-dessous\", \"Poteaux en intersection\",\n",
    "                \"Poteaux coup√©s (u)\", \"Poteaux coup√©s (Ids)\", \"Poteaux coupants (u)\", \"Poteaux coupants (Ids)\",\n",
    "                \"Etat de la jonction\", \"Valeur de d√©calage Z\", \"Justification Z\", \"Valeur de d√©calage Y\", \"Justification Y\",\n",
    "                \"Justification YZ\", \"Mat√©riau structurel\", \"El√©vation du niveau de r√©f√©rence\", \"El√©vation en haut\",\n",
    "                \"Rotation de la section\", \"Orientation\", \"D√©calage du niveau d'arriv√©e\", \"D√©calage du niveau de d√©part\",\n",
    "                \"El√©vation √† la base\", \"Longueur de coupe\", \"Longueur\", \"hauteur_section\", \"largeur_section\"],\n",
    "\n",
    "    \"Poteaux\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"AI\", \"AS\",\n",
    "                \"Hauteur\", \"Longueur\", \"Partie inf√©rieure attach√©e\", \"Partie sup√©rieure attach√©e\", \"Sols en intersection\",\n",
    "                \"Sols coup√©s (u)\", \"Sols coup√©s (Ids)\", \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Poutres en intersection\",\n",
    "                \"Poutres coup√©s (u)\", \"Poutres coup√©s (Ids)\", \"Poutres coupants (u)\", \"Poutres coupants (Ids)\",\n",
    "                \"Mat√©riau structurel\", \"D√©calage sup√©rieur\", \"D√©calage inf√©rieur\", \"Diam√®tre poteau\", \"h\", \"b\",\n",
    "                \"hauteur_section\", \"largeur_section\"]\n",
    "}\n",
    "\n",
    "# Filter multiple dataframes dynamically\n",
    "cleaned_dataframes = {}  # Store cleaned versions\n",
    "\n",
    "for df_name, df in dataframes.items():\n",
    "    print(f\"\\nüü¢ Original shape of {df_name}: {df.shape}\")\n",
    "\n",
    "    # Automatically detect the correct category for filtering\n",
    "    for category, columns in required_columns.items():\n",
    "        if category.lower() in df_name.lower():  # Match dynamically\n",
    "            try:\n",
    "                filtered_df = df[columns]  # Keep only the required columns\n",
    "            except KeyError as e:\n",
    "                missing_columns = set(columns) - set(df.columns)\n",
    "                print(f\"‚ö†Ô∏è Missing columns in {df_name}: {missing_columns}. Skipping this dataframe.\")\n",
    "                continue\n",
    "            cleaned_dataframes[df_name] = filtered_df\n",
    "            print(f\"‚úÖ Shape after filtering {df_name}: {filtered_df.shape}\")\n",
    "            break  # Stop looping once the correct match is found\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No matching category for {df_name}, skipping filtering.\")\n",
    "\n",
    "# Add prefixes to column names based on the dataframe category and update index\n",
    "for name, df in cleaned_dataframes.items():\n",
    "    if \"murs\" in name.lower():\n",
    "        prefix = \"murs_\"\n",
    "    elif \"sols\" in name.lower():\n",
    "        prefix = \"sols_\"\n",
    "    elif \"poutres\" in name.lower():\n",
    "        prefix = \"poutres_\"\n",
    "    elif \"poteaux\" in name.lower():\n",
    "        prefix = \"poteaux_\"\n",
    "    else:\n",
    "        prefix = \"\"\n",
    "\n",
    "    # Rename columns with the prefix\n",
    "    df.rename(columns=lambda col: f\"{prefix}{col}\" if col.lower() != \"id\" else f\"{prefix}id\", inplace=True)\n",
    "\n",
    "    # Drop the existing index and set the prefixed ID column as the new index\n",
    "    id_column = f\"{prefix}id\"\n",
    "    if id_column in df.columns:\n",
    "        df.set_index(id_column, inplace=True)\n",
    "        print(f\"‚úÖ Set '{id_column}' as index for {name}.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è '{id_column}' column not found in {name}, skipping index setting.\")\n",
    "\n",
    "    # Update the cleaned_dataframes dictionary\n",
    "    cleaned_dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3447152",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_feature_names(cleaned_dataframes, required_columns):\n",
    "    \"\"\"Maps cleaned dataframe column names to match required training feature names.\"\"\"\n",
    "    mapped_dataframes = {}\n",
    "\n",
    "    for df_name, df in cleaned_dataframes.items():\n",
    "        for category, expected_columns in required_columns.items():\n",
    "            if category.lower() in df_name.lower():  # Match dynamically\n",
    "                # Create mapping: {cleaned_col_name: expected_col_name}\n",
    "                col_mapping = {cleaned_col: expected_col for cleaned_col in df.columns for expected_col in expected_columns if cleaned_col.lower() == expected_col.lower()}\n",
    "\n",
    "                # Apply mapping to rename columns\n",
    "                df_mapped = df.rename(columns=col_mapping)\n",
    "\n",
    "                print(f\"‚úÖ Feature names mapped for {df_name}\")\n",
    "                mapped_dataframes[df_name] = df_mapped\n",
    "                break  # Stop looping once category is matched\n",
    "\n",
    "    return mapped_dataframes\n",
    "\n",
    "# Example usage:\n",
    "mapped_dataframes = map_feature_names(cleaned_dataframes, required_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e00fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_column_names(df):\n",
    "    # Ensure all column names are lowercase, replace spaces with underscores, and remove special characters\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.lower()\n",
    "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "        .str.replace(r\"[^\\w_]\", \"\", regex=True)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Clean column names in all provided DataFrames\n",
    "cleaned_dataframes = {name: clean_column_names(df) for name, df in cleaned_dataframes.items()}\n",
    "print(\"‚úÖ Column names cleaned successfully across all cleaned dataframes!\")\n",
    "\n",
    "TARGET_COLUMNS = ['011ec_lot', '012ec_ouvrage', '013ec_localisation', '014ec_mode_constructif']\n",
    "final_cleaned_dataframes = {}\n",
    "target_columns_found = set()\n",
    "exception_keywords = [\"coup√©s\", \"coupants\", \"011ec_lot\", \"012ec_ouvrage\", \"013ec_localisation\", \"014ec_mode_constructif\"]\n",
    "\n",
    "for df_name, df in cleaned_dataframes.items():\n",
    "    print(f\"\\nüü¢ Processing {df_name}...\")\n",
    "    df = df.copy()\n",
    "    initial_shape = df.shape\n",
    "    print(f\"üìå Initial shape: {initial_shape}\")\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates:\n",
    "        print(f\"‚ö†Ô∏è Found {duplicates} duplicate rows. Removing...\")\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    else:\n",
    "        print(\"‚úÖ No duplicate rows found.\")\n",
    "\n",
    "    # Drop columns that are 100% missing unless they match exception keywords\n",
    "    missing_cols = df.columns[df.isnull().mean() == 1]\n",
    "    cols_to_drop = [col for col in missing_cols if not any(keyword in col.lower() for keyword in exception_keywords)]\n",
    "    if cols_to_drop:\n",
    "        print(f\"‚ö†Ô∏è Dropping {len(cols_to_drop)} completely empty columns: {cols_to_drop}\")\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "    else:\n",
    "        print(\"‚úÖ No fully missing columns detected (or all are exceptions).\")\n",
    "\n",
    "    mid_shape = df.shape\n",
    "\n",
    "    # Ensure each target column exists, adding it with NaNs if missing (with naming policy)\n",
    "    for target in TARGET_COLUMNS:\n",
    "        target_col = f\"{df_name.split('_')[-1].lower()}_{target.lower()}\"\n",
    "        if target_col not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è Target column '{target_col}' missing in '{df_name}'. Adding it.\")\n",
    "            df[target_col] = float('nan')\n",
    "\n",
    "    final_shape = df.shape\n",
    "    if mid_shape != final_shape:\n",
    "        print(f\"üìä Shape adjustment: before {mid_shape}, after {final_shape}\")\n",
    "\n",
    "    # List and accumulate target columns found in the current DataFrame\n",
    "    target_cols_in_df = [col for col in df.columns if any(t.lower() in col.lower() for t in TARGET_COLUMNS)]\n",
    "    print(f\"üéØ Target columns in '{df_name}': {target_cols_in_df}\")\n",
    "    target_columns_found.update(target_cols_in_df)\n",
    "\n",
    "    final_cleaned_dataframes[df_name] = df\n",
    "    print(f\"üìå Final shape after cleaning: {final_shape}\")\n",
    "\n",
    "print(f\"\\nTarget columns detected across datasets: {target_columns_found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure missing values are filled in the processed datasets unless in TARGET_COLUMNS\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    print(f\"\\nüü¢ Filling missing values for {df_name}...\")\n",
    "\n",
    "    # Display shape before filling missing values\n",
    "    initial_shape = df.shape\n",
    "    print(f\"üìå Initial shape before filling NaN: {initial_shape}\")\n",
    "\n",
    "    # Fill missing values with 0 for non-target columns\n",
    "    non_target_columns = [col for col in df.columns if col not in TARGET_COLUMNS]\n",
    "    df[non_target_columns] = df[non_target_columns].fillna(0)\n",
    "\n",
    "    # Store updated dataframe back\n",
    "    final_cleaned_dataframes[df_name] = df\n",
    "\n",
    "    # Display shape after processing\n",
    "    final_shape = df.shape\n",
    "    print(f\"‚úÖ Final shape after filling NaN: {final_shape}\")\n",
    "\n",
    "print(\"üöÄ Missing values successfully handled across all datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a326b0a",
   "metadata": {},
   "source": [
    "## EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c40ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to remove low-variance & highly correlated features\n",
    "def optimize_feature_selection(df, variance_threshold=0.02, correlation_threshold=0.98):\n",
    "    print(f\"\\nüîç Processing {df.shape[0]} rows & {df.shape[1]} columns\")\n",
    "\n",
    "    # Step 1: Remove Low-Variance Features\n",
    "    selector = VarianceThreshold(variance_threshold)\n",
    "    numeric_df = df.select_dtypes(include=[\"number\"])  # Focus only on numerical columns\n",
    "    selector.fit(numeric_df)\n",
    "\n",
    "    low_variance_cols = numeric_df.columns[~selector.get_support()]\n",
    "    keep_cols = [col for col in low_variance_cols if any(keyword in col.lower() for keyword in [\"coup√©s\", \"coupants\"])]\n",
    "    drop_cols = [col for col in low_variance_cols if col not in keep_cols and col not in TARGET_COLUMNS]\n",
    "\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "    print(f\"‚ö†Ô∏è Dropped {len(drop_cols)} low-variance columns (excluding 'coup√©s' and target columns): {drop_cols}\")\n",
    "\n",
    "    # Step 2: Remove Highly Correlated Features\n",
    "    numeric_df = df.select_dtypes(include=[\"number\"])\n",
    "    correlation_matrix = numeric_df.corr().abs()\n",
    "    upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "    correlated_features = [\n",
    "        col for col in upper_triangle.columns\n",
    "        if any(upper_triangle[col] > correlation_threshold) and col not in TARGET_COLUMNS\n",
    "    ]\n",
    "\n",
    "    df.drop(columns=correlated_features, inplace=True)\n",
    "    print(f\"‚ö†Ô∏è Dropped {len(correlated_features)} highly correlated columns (excluding target columns): {correlated_features}\")\n",
    "\n",
    "    print(f\"‚úÖ Final shape after filtering: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Apply optimized feature selection to all datasets\n",
    "final_cleaned_dataframes = {name: optimize_feature_selection(df) for name, df in final_cleaned_dataframes.items()}\n",
    "\n",
    "print(\"üöÄ Optimized feature selection completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics for all cleaned sheets\n",
    "for df_name, df in cleaned_dataframes.items():\n",
    "    print(f\"\\nSummary statistics for {df_name}:\")\n",
    "\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef3949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot histograms for numerical columns\n",
    "for df_name, df in cleaned_dataframes.items():\n",
    "    df.hist(figsize=(15,10), bins=20)\n",
    "    plt.suptitle(f\"Distribution of Features in {df_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd2193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrices for numeric columns\n",
    "for df_name, df in cleaned_dataframes.items():\n",
    "    numeric_df = df.select_dtypes(include=[\"number\"])\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "    plt.title(f\"Correlation Matrix for {df_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f938645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure base plots directory exists\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# Function to generate subfolder paths for each Excel file\n",
    "def get_plot_subfolder(file_name):\n",
    "    subfolder_name = f\"{file_name.replace('.xlsx', '').replace('.xls', '')}_Plots\"\n",
    "    subfolder_path = os.path.join(PLOTS_DIR, subfolder_name)\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "    return subfolder_path\n",
    "\n",
    "# Helper function to extract file name and subfolder\n",
    "def get_subfolder_and_path(df_name, suffix):\n",
    "    file_name = df_name.split(\"_\")[0]\n",
    "    plot_subfolder = get_plot_subfolder(file_name)\n",
    "    plot_path = os.path.join(plot_subfolder, f\"{df_name}_{suffix}.png\")\n",
    "    return plot_path\n",
    "\n",
    "# Save histograms and correlation matrices\n",
    "for df_name, df in cleaned_dataframes.items():\n",
    "    # Histogram\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    df.hist(bins=20)\n",
    "    plt.suptitle(f\"Distribution of Features in {df_name}\")\n",
    "    plt.savefig(get_subfolder_and_path(df_name, \"histogram\"))\n",
    "    print(f\"Saved histogram in: {get_subfolder_and_path(df_name, 'histogram')}\")\n",
    "    plt.close()\n",
    "\n",
    "    # Correlation matrix\n",
    "    numeric_df = df.select_dtypes(include=[\"number\"])\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "    plt.title(f\"Correlation Matrix for {df_name}\")\n",
    "    plt.savefig(get_subfolder_and_path(df_name, \"correlation\"))\n",
    "    print(f\"Saved correlation matrix in: {get_subfolder_and_path(df_name, 'correlation')}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75838edd",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target columns dynamically across all DataFrames\n",
    "target_columns_found = set()\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    found_targets = [\n",
    "        col for col in df.columns\n",
    "        if any(target.lower() in col.lower() for target in TARGET_COLUMNS)\n",
    "    ]\n",
    "    target_columns_found.update(found_targets)\n",
    "\n",
    "print(f\"\\nTarget columns detected across datasets: {target_columns_found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f4f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Ensure SHAP initializes properly\n",
    "shap.initjs()\n",
    "\n",
    "# Function to generate subfolder paths for storing SHAP plots\n",
    "def get_plot_subfolder(file_name, base_dir=\"plots\"):\n",
    "    subfolder_path = os.path.join(base_dir, f\"{file_name}_Plots\")\n",
    "    os.makedirs(subfolder_path, exist_ok=True)  # Creates folder only if it doesn't exist\n",
    "    return subfolder_path\n",
    "\n",
    "# ‚úÖ Ensure SHAP is applied to the fully processed dataset\n",
    "final_shap_dataframes = final_cleaned_dataframes  # Using cleaned dataset after variance/correlation removal\n",
    "print(\"üöÄ SHAP analysis will now use the final processed data!\")\n",
    "\n",
    "for df_name, df in final_shap_dataframes.items():\n",
    "    print(f\"\\nüü¢ Processing SHAP for {df_name}...\")\n",
    "\n",
    "    # Identify available target columns (substring match with TARGET_COLUMNS)\n",
    "    existing_target_columns = [col for col in df.columns if any(target in col.lower() for target in TARGET_COLUMNS)]\n",
    "\n",
    "    if not existing_target_columns:\n",
    "        print(f\"‚ö†Ô∏è No valid target columns found in {df_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üéØ Target columns found in {df_name}: {existing_target_columns}\")\n",
    "\n",
    "    for target_column in existing_target_columns:\n",
    "        print(f\"üîç Analyzing SHAP for target: {target_column}\")\n",
    "        initial_shape = df.shape\n",
    "        print(f\"üìå Initial shape before SHAP processing: {initial_shape}\")\n",
    "\n",
    "        # Prepare the feature matrix and target variable\n",
    "        X = df.drop(columns=existing_target_columns)\n",
    "        X = X.apply(lambda col: col.astype(\"category\").cat.codes if col.dtypes == \"object\" else col)\n",
    "        y = df[target_column].astype(\"category\").cat.codes\n",
    "\n",
    "        # Train RandomForest model\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Compute SHAP values\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        shap_values = shap_values[0] if isinstance(shap_values, list) else shap_values\n",
    "\n",
    "        # Generate and save SHAP plot\n",
    "        plot_subfolder = get_plot_subfolder(f\"SHAP_{df_name}\")\n",
    "        plot_path = os.path.join(plot_subfolder, f\"{target_column}_SHAP.png\")\n",
    "        shap.summary_plot(shap_values, X, show=False)\n",
    "        plt.savefig(plot_path)\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"‚úÖ Saved SHAP plot for {target_column} in: {plot_path}\")\n",
    "\n",
    "    print(f\"üìå Final shape after SHAP processing: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each dataframe in the dictionary and print its dtypes\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "\tprint(f\"Dtypes for {df_name}:\")\n",
    "\tprint(df.dtypes)\n",
    "\tprint(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to convert ID strings into a numeric count feature\n",
    "def count_ids(id_string):\n",
    "    \"\"\"Convert string of IDs into a numeric count.\"\"\"\n",
    "    return len(id_string.split(\",\")) if isinstance(id_string, str) else 0\n",
    "\n",
    "# Apply processing to fully cleaned datasets\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    print(f\"\\nüîÑ Processing ID count transformation for {df_name}...\")\n",
    "\n",
    "    # Identify relevant ID columns\n",
    "    id_columns = [col for col in df.columns if any(keyword in col.lower() for keyword in [\"coup√©s_(ids)\", \"coupants_(ids)\"])]\n",
    "\n",
    "    if id_columns:\n",
    "        print(f\"üìå Found ID columns: {id_columns}\")\n",
    "\n",
    "        # Transform ID columns into numeric count and drop originals\n",
    "        df[[f\"{col}_count\" for col in id_columns]] = df[id_columns].applymap(count_ids)\n",
    "        df.drop(columns=id_columns, inplace=True)  # Remove original text-based ID columns\n",
    "\n",
    "    # Ensure only ID-related columns are converted to numeric\n",
    "    df[id_columns] = df[id_columns].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # Store the updated dataframe\n",
    "    final_cleaned_dataframes[df_name] = df\n",
    "\n",
    "    print(f\"‚úÖ Final shape after ID count transformation: {df.shape}\")\n",
    "\n",
    "print(\"üöÄ ID count transformation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each dataframe in the dictionary and print its dtypes\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "\tprint(f\"Dtypes for {df_name}:\")\n",
    "\tprint(df.dtypes)\n",
    "\tprint(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482be44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    print(f\"\\nüìå {df_name} - Categorical Columns Before Encoding: {categorical_cols.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd624fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize dictionaries to store encoders\n",
    "feature_encoders, target_encoders = {}, {}\n",
    "\n",
    "print(\"üöÄ Applying categorical encoding across all datasets...\")\n",
    "\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    print(f\"\\nüîÑ Processing {df_name}...\")\n",
    "\n",
    "    # Identify categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    target_cols = [col for col in categorical_cols if col in TARGET_COLUMNS]\n",
    "    feature_cols = list(set(categorical_cols) - set(target_cols))  # Exclude target columns\n",
    "\n",
    "    # Encode target columns\n",
    "    for col in target_cols:\n",
    "        encoder = LabelEncoder()\n",
    "        df[col] = encoder.fit_transform(df[col].astype(str))\n",
    "        target_encoders[f\"{df_name}_{col}\"] = encoder\n",
    "        print(f\"‚úÖ Target Encoder stored for {df_name} - {col}\")\n",
    "\n",
    "    # Encode feature columns using Label Encoding\n",
    "    one_hot_cols = []\n",
    "    for col in feature_cols:\n",
    "        encoder = LabelEncoder()\n",
    "        df[col] = encoder.fit_transform(df[col].astype(str))\n",
    "        feature_encoders[f\"{df_name}_{col}\"] = encoder\n",
    "        print(f\"‚úÖ Feature Encoder stored for {df_name} - {col}\")\n",
    "        one_hot_cols.append(col)\n",
    "\n",
    "    # Apply One-Hot Encoding to relevant categorical features\n",
    "    if one_hot_cols:\n",
    "        encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "        encoded_df = pd.DataFrame(\n",
    "            encoder.fit_transform(df[one_hot_cols]),\n",
    "            index=df.index,\n",
    "            columns=encoder.get_feature_names_out(one_hot_cols)\n",
    "        )\n",
    "        df.drop(columns=one_hot_cols, inplace=True)\n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "    # Save the updated dataframe\n",
    "    final_cleaned_dataframes[df_name] = df\n",
    "    print(f\"‚úÖ Completed categorical encoding for {df_name}. Updated shape: {df.shape}\")\n",
    "\n",
    "print(\"üéØ Final categorical encoding applied successfully across all datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1800a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    print(f\"\\nüìå {df_name} - Categorical Columns After Encoding: {categorical_cols.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b23cf6f",
   "metadata": {},
   "source": [
    "## Training and saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff70e5",
   "metadata": {},
   "source": [
    "### Machine Learning Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f13cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, learning_curve, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define ML models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(solver=\"saga\", max_iter=5000, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=3),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42, max_depth=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=50, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=50, random_state=42),\n",
    "}\n",
    "\n",
    "failed_models = []\n",
    "\n",
    "def process_data(final_cleaned_dataframes, TARGET_COLUMNS):\n",
    "    \"\"\"Detects missing values, merges datasets, and resets index.\"\"\"\n",
    "    all_X, all_y = [], []\n",
    "    print(\"\\n‚úÖ Checking available dataframes and target columns...\")\n",
    "\n",
    "    for df_name, df in final_cleaned_dataframes.items():\n",
    "        existing_targets = [col for col in df.columns if any(target in col for target in TARGET_COLUMNS)]\n",
    "        if not existing_targets:\n",
    "            print(f\"‚ö†Ô∏è {df_name}: No matching target columns found.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüîç Processing {df_name} - Found target columns: {existing_targets}\")\n",
    "\n",
    "        for target_column in existing_targets:\n",
    "            X, y = df.drop(columns=existing_targets), df[target_column]\n",
    "            if y.nunique() == 1:\n",
    "                print(f\"‚ö†Ô∏è Skipping {df_name}_{target_column}: Only one class present.\")\n",
    "                continue\n",
    "\n",
    "            all_X.append(X.reset_index(drop=True))\n",
    "            all_y.append(y.reset_index(drop=True))\n",
    "\n",
    "    if not all_X or not all_y:\n",
    "        raise ValueError(\"üö® No valid datasets found. Check TARGET_COLUMNS or ensure target values vary.\")\n",
    "\n",
    "    X_combined, y_combined = pd.concat(all_X, axis=0).reset_index(drop=True), pd.concat(all_y, axis=0).reset_index(drop=True)\n",
    "    print(f\"\\n‚úÖ Final merged dataset shape: {X_combined.shape}, {y_combined.shape}\")\n",
    "    return X_combined, y_combined\n",
    "\n",
    "def train_models(X_combined, y_combined):\n",
    "    \"\"\"Trains multiple ML models & evaluates performance.\"\"\"\n",
    "    print(\"\\nüîç Handling missing values...\")\n",
    "    X_combined = pd.DataFrame(SimpleImputer(strategy='mean').fit_transform(X_combined), columns=X_combined.columns)\n",
    "    y_combined.dropna(inplace=True)\n",
    "\n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_combined = pd.DataFrame(scaler.fit_transform(X_combined), columns=X_combined.columns)\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "    model_results = {}\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüöÄ Training {name}...\")\n",
    "        try:\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "            test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "            model_results[name] = test_accuracy\n",
    "\n",
    "            print(f\"‚úÖ {name}: Test Accuracy = {test_accuracy:.4f}\")\n",
    "\n",
    "            # Learning Curve\n",
    "            train_sizes, train_scores, test_scores = learning_curve(\n",
    "                model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1,\n",
    "                train_sizes=np.linspace(0.1, 1.0, 5)\n",
    "            )\n",
    "            plt.plot(train_sizes, np.mean(test_scores, axis=1), marker='o', label=f\"{name} (Acc: {test_accuracy:.2f})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error training {name}: {e}\")\n",
    "            failed_models.append(name)\n",
    "\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Learning Curve - All Models\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Rank models\n",
    "    ranked_models = sorted(model_results.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\nüìä Model Rankings by Test Accuracy:\")\n",
    "    print(pd.DataFrame(ranked_models, columns=[\"Model\", \"Test Accuracy\"]).to_string(index=False))\n",
    "\n",
    "    # Save top models\n",
    "    for name, _ in ranked_models[:2]:\n",
    "        models[name].fit(X_combined, y_combined)\n",
    "        joblib.dump(models[name], f'models/machine_learning/{name.replace(\" \", \"_\")}_combined.pkl')\n",
    "\n",
    "    print(\"\\nüöÄ Model evaluation, ranking, and saving completed!\")\n",
    "    print(f\"‚ö†Ô∏è Models that failed: {failed_models}\")\n",
    "\n",
    "# Run the pipeline\n",
    "X_combined, y_combined = process_data(final_cleaned_dataframes, TARGET_COLUMNS)\n",
    "train_models(X_combined, y_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea4cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define models and Bayesian hyperparameter search spaces\n",
    "models = {\n",
    "    \"Random Forest\": (RandomForestClassifier(random_state=42), {\n",
    "        'n_estimators': Integer(100, 1000), 'max_depth': Integer(3, 30),\n",
    "        'min_samples_split': Integer(2, 15), 'max_features': Categorical(['sqrt', 'log2', None])\n",
    "    }),\n",
    "    \"Logistic Regression\": (LogisticRegression(max_iter=5000, random_state=42), {\n",
    "        'C': Real(0.01, 10, prior='log-uniform'), 'solver': Categorical(['liblinear', 'lbfgs', 'saga'])\n",
    "    }),\n",
    "    \"SVM\": (SVC(probability=True, random_state=42), {\n",
    "        'C': Real(0.1, 10, prior='log-uniform'), 'gamma': Real(0.01, 1, prior='log-uniform'),\n",
    "        'kernel': Categorical(['linear', 'rbf'])\n",
    "    }),\n",
    "    \"KNN\": (KNeighborsClassifier(), {\n",
    "        'n_neighbors': Integer(3, 15), 'weights': Categorical(['uniform', 'distance']),\n",
    "        'metric': Categorical(['euclidean', 'manhattan', 'minkowski'])\n",
    "    }),\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(random_state=42), {\n",
    "        'max_depth': Integer(3, 20), 'min_samples_split': Integer(2, 10)\n",
    "    }),\n",
    "    \"AdaBoost\": (AdaBoostClassifier(random_state=42), {\n",
    "        'n_estimators': Integer(50, 500), 'learning_rate': Real(0.01, 1, prior='log-uniform')\n",
    "    }),\n",
    "    \"Gradient Boosting\": (GradientBoostingClassifier(random_state=42), {\n",
    "        'n_estimators': Integer(50, 500), 'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'max_depth': Integer(3, 15)\n",
    "    })\n",
    "}\n",
    "\n",
    "def optimize_model(model, param_space, X_train, y_train):\n",
    "    \"\"\"Bayesian hyperparameter optimization.\"\"\"\n",
    "    opt = BayesSearchCV(model, param_space, n_iter=50, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "    opt.fit(X_train, y_train)\n",
    "    return opt.best_estimator_, opt.best_score_\n",
    "\n",
    "def train_and_rank_models(X_train, y_train):\n",
    "    \"\"\"Tunes models, ranks them, and saves the best two.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for name, (model, param_space) in models.items():\n",
    "        print(f\"\\nüîç Optimizing {name}...\")\n",
    "        try:\n",
    "            best_model, best_score = optimize_model(model, param_space, X_train, y_train)\n",
    "            results[name] = (best_model, best_score)\n",
    "            print(f\"‚úÖ {name}: Accuracy = {best_score:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è {name} failed: {e}\")\n",
    "\n",
    "    # Sort models by accuracy and save the top two\n",
    "    top_models = sorted(results.items(), key=lambda x: x[1][1], reverse=True)[:2]\n",
    "    for name, (model, _) in top_models:\n",
    "        joblib.dump(model, f'models/machine_learning/{name.replace(\" \", \"_\")}_optimized.pkl')\n",
    "\n",
    "    print(\"\\nüöÄ Saved top 2 models:\", [name for name, _ in top_models])\n",
    "\n",
    "# Example dataset\n",
    "if \"train_test_data\" not in globals():\n",
    "    from sklearn.datasets import load_iris\n",
    "    iris = load_iris()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(iris.data), pd.Series(iris.target), test_size=0.2, random_state=42)\n",
    "\n",
    "# Run tuning\n",
    "train_and_rank_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add92591",
   "metadata": {},
   "source": [
    "### Deep Learning Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure dataset exists\n",
    "if \"train_test_data\" not in globals():\n",
    "    raise ValueError(\"üö® No train-test data found!\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_name = list(train_test_data.keys())[0]\n",
    "X_train, X_test, y_train, y_test = train_test_data[dataset_name]\n",
    "\n",
    "# Process text features\n",
    "def process_text(X_train, X_test):\n",
    "    text_cols = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "    if not text_cols.empty:\n",
    "        vectorizer = TfidfVectorizer(max_features=500)\n",
    "        return (\n",
    "            pd.DataFrame(vectorizer.fit_transform(X_train[text_cols].fillna(\"\").agg(\" \".join, axis=1)).toarray()),\n",
    "            pd.DataFrame(vectorizer.transform(X_test[text_cols].fillna(\"\").agg(\" \".join, axis=1)).toarray())\n",
    "        )\n",
    "    return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# Scale numerical features\n",
    "def scale_numeric(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    return (\n",
    "        pd.DataFrame(scaler.fit_transform(X_train.select_dtypes(exclude=[\"object\"]))),\n",
    "        pd.DataFrame(scaler.transform(X_test.select_dtypes(exclude=[\"object\"])))\n",
    "    )\n",
    "\n",
    "# Process and combine features\n",
    "X_train_text, X_test_text = process_text(X_train, X_test)\n",
    "X_train_scaled, X_test_scaled = scale_numeric(X_train, X_test)\n",
    "X_train_combined, X_test_combined = np.hstack([X_train_scaled, X_train_text]), np.hstack([X_test_scaled, X_test_text])\n",
    "\n",
    "# Define deep learning models\n",
    "deep_models = {\n",
    "    \"DNN\": [128, 64],\n",
    "    \"Wide & Deep\": [64, 128],\n",
    "    \"TabNet\": [256]\n",
    "}\n",
    "\n",
    "def build_model(layers):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(layers[0], activation='relu', input_shape=(X_train_combined.shape[1],)))\n",
    "    for units in layers[1:]:\n",
    "        model.add(keras.layers.Dense(units, activation='relu'))\n",
    "        model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(len(set(y_train)), activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Train models & track results\n",
    "model_results = {}\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for name, layers in deep_models.items():\n",
    "    model = build_model(layers)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train_combined, y_train, epochs=20, batch_size=32, validation_data=(X_test_combined, y_test), verbose=0)\n",
    "\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    model_results[name] = best_val_acc\n",
    "    plt.plot(history.history['val_accuracy'], label=f\"{name} ({best_val_acc:.2f})\")\n",
    "\n",
    "# Rank and save top 2 models\n",
    "top_models = sorted(model_results.items(), key=lambda x: x[1], reverse=True)[:2]\n",
    "for name, _ in top_models:\n",
    "    build_model(deep_models[name]).save(f'models/deep_learning/{name}_best_model.keras')\n",
    "\n",
    "print(\"\\nüöÄ Top models saved:\", [name for name, _ in top_models])\n",
    "\n",
    "# Plot comparison\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Deep Learning Model Comparison\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('plots/deep_learning_learning_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf71ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "if \"train_test_data\" not in globals():\n",
    "    raise ValueError(\"üö® No train-test data found!\")\n",
    "\n",
    "dataset_name = list(train_test_data.keys())[0]\n",
    "X_train, X_test, y_train, y_test = train_test_data[dataset_name]\n",
    "\n",
    "# Process text and numerical features\n",
    "def process_features(X_train, X_test):\n",
    "    text_cols = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "    vectorizer = TfidfVectorizer(max_features=500) if not text_cols.empty else None\n",
    "\n",
    "    X_train_text = pd.DataFrame(vectorizer.fit_transform(X_train[text_cols].fillna(\"\").agg(\" \".join, axis=1)).toarray()) if vectorizer else pd.DataFrame()\n",
    "    X_test_text = pd.DataFrame(vectorizer.transform(X_test[text_cols].fillna(\"\").agg(\" \".join, axis=1)).toarray()) if vectorizer else pd.DataFrame()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train.select_dtypes(exclude=[\"object\"])))\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test.select_dtypes(exclude=[\"object\"])))\n",
    "\n",
    "    return np.hstack([X_train_scaled, X_train_text]), np.hstack([X_test_scaled, X_test_text])\n",
    "\n",
    "X_train_combined, X_test_combined = process_features(X_train, X_test)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_space = {\n",
    "    'units_1': Integer(64, 256), 'units_2': Integer(32, 128),\n",
    "    'dropout_1': Real(0.1, 0.5), 'dropout_2': Real(0.1, 0.5),\n",
    "    'learning_rate': Real(0.0001, 0.01, prior='log-uniform'), 'batch_size': Integer(16, 64)\n",
    "}\n",
    "\n",
    "# Build model dynamically\n",
    "def build_model(units_1, units_2, dropout_1, dropout_2, learning_rate):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(units_1, activation='relu', input_shape=(X_train_combined.shape[1],)),\n",
    "        keras.layers.Dropout(dropout_1),\n",
    "        keras.layers.Dense(units_2, activation='relu'),\n",
    "        keras.layers.Dropout(dropout_2),\n",
    "        keras.layers.Dense(len(set(y_train)), activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Tune hyperparameters\n",
    "opt = BayesSearchCV(\n",
    "    estimator=keras.wrappers.scikit_learn.KerasClassifier(build_model),\n",
    "    search_spaces=param_space, n_iter=50, cv=3, scoring='accuracy',\n",
    "    n_jobs=-1, random_state=42\n",
    ")\n",
    "opt.fit(X_train_combined, y_train)\n",
    "best_model, best_params = opt.best_estimator_, opt.best_params_\n",
    "\n",
    "print(f\"\\n‚úÖ Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Train final model\n",
    "history = best_model.fit(X_train_combined, y_train, epochs=50, batch_size=best_params['batch_size'], validation_data=(X_test_combined, y_test))\n",
    "\n",
    "# Plot learning curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Deep Learning Training Curve - {dataset_name}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('plots/deep_learning_tuned_learning_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Save best model\n",
    "best_model.model.save('models/deep_learning/best_model_tuned.keras')\n",
    "print(\"\\nüöÄ Deep Learning Model Training & Hyperparameter Tuning Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43da3c1",
   "metadata": {},
   "source": [
    "## Testing new maquettes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf40557",
   "metadata": {},
   "source": [
    "### Importing Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_excel_files(directory):\n",
    "    \"\"\"Loads all Excel files from the specified directory and displays their heads.\"\"\"\n",
    "    new_dataframes = {}\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".xlsx\") or file.endswith(\".xls\"):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df_name = os.path.splitext(file)[0]  # Use filename as key\n",
    "            new_dataframes[df_name] = pd.read_excel(file_path)\n",
    "            print(f\"\\n‚úÖ Loaded {file}\")\n",
    "            print(new_dataframes[df_name].head())  # üëÄ Show first few rows\n",
    "\n",
    "    return new_dataframes\n",
    "\n",
    "new_dataframes = load_excel_files(TESTING_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4919e64",
   "metadata": {},
   "source": [
    "## Saving and Loading Encoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc285d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "ML_MODELS_DIR = os.path.join(MODELS_DIR, \"machine_learning\")\n",
    "DL_MODELS_DIR = os.path.join(MODELS_DIR, \"deep_learning\")\n",
    "\n",
    "ENCODERS_PATH = os.path.join(MODELS_DIR, \"feature_encoders.pkl\")\n",
    "TARGET_ENCODERS_PATH = os.path.join(MODELS_DIR, \"target_encoders.pkl\")\n",
    "MODEL_FEATURES_PATH = os.path.join(MODELS_DIR, \"model_features.pkl\")\n",
    "\n",
    "def save_encoders(feature_encoders, target_encoders):\n",
    "    \"\"\"Saves feature and target encoders for consistent data preprocessing.\"\"\"\n",
    "    joblib.dump(feature_encoders, ENCODERS_PATH)\n",
    "    joblib.dump(target_encoders, TARGET_ENCODERS_PATH)\n",
    "\n",
    "def load_encoders():\n",
    "    \"\"\"Loads stored feature and target encoders.\"\"\"\n",
    "    return (\n",
    "        joblib.load(ENCODERS_PATH) if os.path.exists(ENCODERS_PATH) else {},\n",
    "        joblib.load(TARGET_ENCODERS_PATH) if os.path.exists(TARGET_ENCODERS_PATH) else {}\n",
    "    )\n",
    "\n",
    "def encode_new_data(X_new, feature_encoders):\n",
    "    \"\"\"Encodes categorical features using stored encoders.\"\"\"\n",
    "    for col, encoder in feature_encoders.items():\n",
    "        if col in X_new:\n",
    "            X_new[col] = encoder.transform(X_new[col].astype(str))\n",
    "    return X_new\n",
    "\n",
    "def make_ml_predictions(X_new):\n",
    "    \"\"\"Loads and applies all machine learning models.\"\"\"\n",
    "    feature_encoders, _ = load_encoders()\n",
    "    X_encoded = encode_new_data(X_new.copy(), feature_encoders)\n",
    "\n",
    "    if os.path.exists(MODEL_FEATURES_PATH):\n",
    "        model_features = joblib.load(MODEL_FEATURES_PATH)\n",
    "        X_encoded = X_encoded.reindex(columns=model_features, fill_value=0)\n",
    "\n",
    "    models = {f.replace(\"_optimized.pkl\", \"\").replace(\"_combined.pkl\", \"\"): joblib.load(os.path.join(ML_MODELS_DIR, f))\n",
    "              for f in os.listdir(ML_MODELS_DIR) if f.endswith(\".pkl\")}\n",
    "\n",
    "    predictions = {name: model.predict(X_encoded) for name, model in models.items()}\n",
    "    return max(predictions.items(), key=lambda x: np.mean(x[1]))  # Returns best ML prediction\n",
    "def make_dl_predictions(X_new):\n",
    "    \"\"\"Loads and applies all deep learning models.\"\"\"\n",
    "    models = {f.replace(\"_best_model.keras\", \"\").replace(\"_tuned.keras\", \"\"): tf.keras.models.load_model(os.path.join(DL_MODELS_DIR, f))\n",
    "              for f in os.listdir(DL_MODELS_DIR) if f.endswith(\".keras\")}\n",
    "\n",
    "    predictions = {name: np.argmax(model.predict(X_new), axis=1) for name, model in models.items()}\n",
    "    return max(predictions.items(), key=lambda x: np.mean(x[1]))\n",
    "\n",
    "def predict_best_model(X_new):\n",
    "    \"\"\"Runs predictions across ML and DL models and picks the best one.\"\"\"\n",
    "    best_ml = make_ml_predictions(X_new)\n",
    "    best_dl = make_dl_predictions(X_new)\n",
    "\n",
    "    return best_ml if np.mean(best_ml[1]) > np.mean(best_dl[1]) else best_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c743c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_models(X_new):\n",
    "    \"\"\"Runs predictions across ML and DL models and returns all results.\"\"\"\n",
    "    predictions = {\"ML\": {}, \"Deep Learning\": {}}\n",
    "\n",
    "    # Machine Learning Predictions\n",
    "    for name, model in make_ml_predictions(X_new).items():\n",
    "        predictions[\"ML\"][name] = model.predict(X_new)\n",
    "\n",
    "    # Deep Learning Predictions\n",
    "    for name, model in make_dl_predictions(X_new).items():\n",
    "        predictions[\"Deep Learning\"][name] = np.argmax(model.predict(X_new), axis=1)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Generate model predictions\n",
    "predictions = predict_all_models(X_new)\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_predictions(predictions, new_dataframes, detected_targets)\n",
    "\n",
    "# Print final model rankings based on accuracy\n",
    "print(\"\\nüöÄ Final Model Evaluation Rankings:\")\n",
    "ranking_df = pd.DataFrame([\n",
    "    (df_name, model_name, acc) for df_name, models in evaluation_results.items() for model_name, acc in models.items()\n",
    "], columns=[\"Dataset\", \"Model\", \"Accuracy\"]).sort_values(by=\"Accuracy\", ascending=False)\n",
    "\n",
    "print(ranking_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727aa936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(predictions, target_encoders):\n",
    "    \"\"\"Decodes encoded model predictions back to original text values.\"\"\"\n",
    "    decoded_results = {}\n",
    "\n",
    "    for df_name, model_results in predictions.items():\n",
    "        decoded_results[df_name] = {}\n",
    "\n",
    "        # Decode ML model predictions\n",
    "        for model_name, encoded_preds in model_results[\"ML\"].items():\n",
    "            decoded_results[df_name][model_name] = target_encoders[df_name].inverse_transform(encoded_preds)\n",
    "\n",
    "        # Decode Deep Learning model predictions\n",
    "        for model_name, encoded_preds in model_results[\"Deep Learning\"].items():\n",
    "            decoded_results[df_name][model_name] = target_encoders[df_name].inverse_transform(encoded_preds)\n",
    "\n",
    "    return decoded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = predict_all_models(X_new)\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_predictions(predictions, new_dataframes, detected_targets)\n",
    "\n",
    "# Decode predictions\n",
    "target_encoders = joblib.load(TARGET_ENCODERS_PATH)  # Load stored target encoders\n",
    "decoded_results = decode_predictions(predictions, target_encoders)\n",
    "\n",
    "# Display final decoded predictions\n",
    "for df_name, models in decoded_results.items():\n",
    "    print(f\"\\nüìä Decoded Predictions for {df_name}:\")\n",
    "    for model_name, decoded_preds in models.items():\n",
    "        print(f\"‚úÖ {model_name}: {decoded_preds[:5]}\")  # Show first 5 decoded values for readability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BImpredict2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
