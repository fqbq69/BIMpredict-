{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a3dbdd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Testing Raw Cible WB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b088204a",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dfa8ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psyko/.pyenv/versions/BIMpredict/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Deep learning imports\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout, BatchNormalization\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "### ====================\n",
    "### Importing libraries\n",
    "### ====================\n",
    "# %matplotlib inline\n",
    "# %pip install openpyxl\n",
    "import openpyxl\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "import shap\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set up visualization and warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_palette('viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d491bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### File paths\n",
    "\n",
    "# Create directories for model saving\n",
    "models_dir = '../../models'\n",
    "for model_type in ['simple_models', 'ml_models', 'dl_models']:\n",
    "    models_dir = os.path.join(models_dir, model_type)\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "# Load all sheets from the Excel file\n",
    "# Load Excel file\n",
    "maquettes_path = \"../../data/raw/\"\n",
    "maquettes= [\"RawData-Cibles.xlsx\"]\n",
    "for maquette in maquettes:\n",
    "    maquettes_path = os.path.join(maquettes_path, maquette)\n",
    "sheets = [\"Mur\", \"Sols\", \"Poutre\", \"Poteaux\"]  # Adjusted based on your description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import data from the Excel file\n",
    "try:\n",
    "    mur_df = pd.read_excel(file_path, sheet_name='Murs', delimiter=';')\n",
    "    sol_df = pd.read_excel(file_path, sheet_name='Sols', delimiter=';')\n",
    "    poutre_df = pd.read_excel(file_path, sheet_name='Poutre', delimiter=';')\n",
    "    poteaux_df = pd.read_excel(file_path, sheet_name='poteaux', delimiter=';')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    # Handle missing sheets\n",
    "    available_sheets = pd.ExcelFile(file_path).sheet_names\n",
    "    print(f\"Available sheets: {available_sheets}\")\n",
    "    # Try to load available sheets\n",
    "    dfs = {}\n",
    "    for sheet in sheets:\n",
    "        if sheet in available_sheets:\n",
    "            dfs[sheet] = pd.read_excel(file_path, sheet_name=sheet, delimiter=';')\n",
    "        else:\n",
    "            print(f\"Sheet '{sheet}' not found in the Excel file.\")\n",
    "    mur_df = dfs.get('Murs', pd.DataFrame())\n",
    "    sol_df = dfs.get('Sols', pd.DataFrame())\n",
    "    poutre_df = dfs.get('Poutre', pd.DataFrame())\n",
    "    poteaux_df = dfs.get('poteaux', pd.DataFrame())\n",
    "\n",
    "# Display basic info about each dataframe\n",
    "print(\"Murs DataFrame Shape:\", mur_df.shape)\n",
    "print(\"Sols DataFrame Shape:\", sol_df.shape)\n",
    "print(\"Poutre DataFrame Shape:\", poutre_df.shape)\n",
    "print(\"Poteaux DataFrame Shape:\", poteaux_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd8d5c",
   "metadata": {},
   "source": [
    "## Part 2: Data Preprocessing and Relationship Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process ID columns\n",
    "def process_id_columns(df, id_cols):\n",
    "    for col in id_cols:\n",
    "        if col in df.columns:\n",
    "            # Fill NaN with '0' if corresponding quantity column exists and is 0\n",
    "            quantity_col = col.replace('(Ids)', '(u)')\n",
    "            if quantity_col in df.columns:\n",
    "                mask = (df[quantity_col] == 0) & (df[col].isna())\n",
    "                df.loc[mask, col] = '0'\n",
    "\n",
    "            # Convert string IDs to lists of integers\n",
    "            df[col] = df[col].astype(str).apply(\n",
    "                lambda x: [int(i.strip()) for i in x.split(',') if i.strip().isdigit()] if x != '0' and pd.notna(x) else []\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Column '{col}' not found in the DataFrame.\")\n",
    "    return df\n",
    "\n",
    "# Process Murs DataFrame\n",
    "mur_id_cols = ['Sols coupés (Ids)', 'Sols coupants (Ids)']\n",
    "mur_df = process_id_columns(mur_df, mur_id_cols)\n",
    "\n",
    "# Process Sols DataFrame\n",
    "sols_id_cols = [\n",
    "    'Murs coupés (Ids)', 'Murs coupants (Ids)',\n",
    "    'Poutres coupés (Ids)', 'Poutres coupants (Ids)',\n",
    "    'Poteaux coupés (Ids)', 'Poteaux coupants (Ids)'\n",
    "]\n",
    "sol_df = process_id_columns(sol_df, sols_id_cols)\n",
    "\n",
    "# Process Poutre DataFrame\n",
    "poutre_id_cols = [\n",
    "    'Sols coupés (Ids)', 'Sols coupants (Ids)',\n",
    "    'Poteaux coupés (Ids)', 'Poteaux coupants (Ids)'\n",
    "]\n",
    "poutre_df = process_id_columns(poutre_df, poutre_id_cols)\n",
    "\n",
    "# Process Poteaux DataFrame\n",
    "poteaux_id_cols = [\n",
    "    'Sols coupés (Ids)', 'Sols coupants (Ids)',\n",
    "    'Poutres coupés (Ids)', 'Poutres coupants (Ids)'\n",
    "]\n",
    "poteaux_df = process_id_columns(poteaux_df, poteaux_id_cols)\n",
    "\n",
    "# Function to create relationship features\n",
    "def create_relationship_features(main_df, related_df, relation_cols, prefix):\n",
    "    \"\"\"\n",
    "    Create features based on relationships between entities\n",
    "\n",
    "    Args:\n",
    "        main_df: The main DataFrame to enhance\n",
    "        related_df: The related DataFrame with information to merge\n",
    "        relation_cols: Dictionary of {relation_column: feature_columns}\n",
    "        prefix: Prefix to add to new feature columns\n",
    "\n",
    "    Returns:\n",
    "        Enhanced DataFrame with new relationship features\n",
    "    \"\"\"\n",
    "    for relation_col, feature_cols in relation_cols.items():\n",
    "        if relation_col in main_df.columns:\n",
    "            # Explode the relation column to have one row per related ID\n",
    "            exploded = main_df.explode(relation_col)\n",
    "            exploded[relation_col] = exploded[relation_col].astype(int)\n",
    "\n",
    "            # Merge with related_df to get features\n",
    "            for feature_col in feature_cols:\n",
    "                if feature_col in related_df.columns:\n",
    "                    merged = exploded[[relation_col]].merge(\n",
    "                        related_df[[feature_col]],\n",
    "                        left_on=relation_col,\n",
    "                        right_index=True,\n",
    "                        how='left'\n",
    "                    )\n",
    "\n",
    "                    # Group by original index and aggregate features\n",
    "                    agg_func = 'mean' if np.issubdtype(merged[feature_col].dtype, np.number) else lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan\n",
    "                    new_feature = merged.groupby(merged.index)[feature_col].agg(agg_func)\n",
    "\n",
    "                    # Add to main_df\n",
    "                    new_col_name = f\"{prefix}_{feature_col}\"\n",
    "                    main_df[new_col_name] = new_feature\n",
    "                else:\n",
    "                    print(f\"Feature column '{feature_col}' not found in related DataFrame.\")\n",
    "        else:\n",
    "            print(f\"Relation column '{relation_col}' not found in main DataFrame.\")\n",
    "\n",
    "    return main_df\n",
    "\n",
    "# Define relationships and features to extract\n",
    "mur_relations = {\n",
    "    'Sols coupés (Ids)': ['Hauteur', 'Epaisseur', 'Volume', 'Surface'],\n",
    "    'Sols coupants (Ids)': ['Hauteur', 'Epaisseur', 'Volume', 'Surface']\n",
    "}\n",
    "\n",
    "sol_relations = {\n",
    "    'Murs coupés (Ids)': ['Hauteur', 'Epaisseur', 'Volume', 'Surface'],\n",
    "    'Murs coupants (Ids)': ['Hauteur', 'Epaisseur', 'Volume', 'Surface'],\n",
    "    'Poutres coupés (Ids)': ['Longueur', 'Largeur', 'Hauteur', 'Volume'],\n",
    "    'Poutres coupants (Ids)': ['Longueur', 'Largeur', 'Hauteur', 'Volume'],\n",
    "    'Poteaux coupés (Ids)': ['Longueur', 'Largeur', 'Hauteur', 'Volume'],\n",
    "    'Poteaux coupants (Ids)': ['Longueur', 'Largeur', 'Hauteur', 'Volume']\n",
    "}\n",
    "\n",
    "poutre_relations = {\n",
    "    'Sols coupés (Ids)': ['Hauteur', 'Epaisseur', 'Volume', 'Surface'],\n",
    "    'Sols coupants (Ids)': ['Hauteur', 'Epaisseur', 'Volume', 'Surface'],\n",
    "    'Poteaux coupés (Ids)': ['Longueur', 'Largeur', 'Hauteur', 'Volume'],\n",
    "    'Poteaux coupants (Ids)': ['Longueur', 'Largeur', 'Hauteur', 'Volume']\n",
    "}\n",
    "\n",
    "poteaux_relations = {\n",
    "    'Sols coupés (Ids)': ['Hauteur', 'Epaisseur', 'Volume', 'Surface'],\n",
    "    'Sols coupants (Ids)': ['Hauteur', 'Epaisseur', 'Volume', 'Surface'],\n",
    "    'Poutres coupés (Ids)': ['Longueur', 'Largeur', 'Hauteur', 'Volume'],\n",
    "    'Poutres coupants (Ids)': ['Longueur', 'Largeur', 'Hauteur', 'Volume']\n",
    "}\n",
    "\n",
    "# Create relationship features for each DataFrame\n",
    "print(\"Creating relationship features for Murs...\")\n",
    "mur_df = create_relationship_features(mur_df, sol_df, mur_relations, 'sol')\n",
    "\n",
    "print(\"Creating relationship features for Sols...\")\n",
    "sol_df = create_relationship_features(sol_df, mur_df, {'Murs coupés (Ids)': ['Hauteur', 'Epaisseur'], 'Murs coupants (Ids)': ['Hauteur', 'Epaisseur']}, 'mur')\n",
    "sol_df = create_relationship_features(sol_df, poutre_df, {'Poutres coupés (Ids)': ['Longueur', 'Largeur'], 'Poutres coupants (Ids)': ['Longueur', 'Largeur']}, 'poutre')\n",
    "sol_df = create_relationship_features(sol_df, poteaux_df, {'Poteaux coupés (Ids)': ['Longueur', 'Largeur'], 'Poteaux coupants (Ids)': ['Longueur', 'Largeur']}, 'poteau')\n",
    "\n",
    "print(\"Creating relationship features for Poutre...\")\n",
    "poutre_df = create_relationship_features(poutre_df, sol_df, {'Sols coupés (Ids)': ['Hauteur', 'Epaisseur'], 'Sols coupants (Ids)': ['Hauteur', 'Epaisseur']}, 'sol')\n",
    "poutre_df = create_relationship_features(poutre_df, poteaux_df, {'Poteaux coupés (Ids)': ['Longueur', 'Largeur'], 'Poteaux coupants (Ids)': ['Longueur', 'Largeur']}, 'poteau')\n",
    "\n",
    "print(\"Creating relationship features for Poteaux...\")\n",
    "poteaux_df = create_relationship_features(poteaux_df, sol_df, {'Sols coupés (Ids)': ['Hauteur', 'Epaisseur'], 'Sols coupants (Ids)': ['Hauteur', 'Epaisseur']}, 'sol')\n",
    "poteaux_df = create_relationship_features(poteaux_df, poutre_df, {'Poutres coupés (Ids)': ['Longueur', 'Largeur'], 'Poutres coupants (Ids)': ['Longueur', 'Largeur']}, 'poutre')\n",
    "\n",
    "# Handle missing values in the target columns\n",
    "target_columns = [\"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\"]\n",
    "for col in target_columns:\n",
    "    if col in mur_df.columns:\n",
    "        if mur_df[col].isna().any():\n",
    "            print(f\"Column '{col}' has missing values. Filling with mode.\")\n",
    "            mur_df[col].fillna(mur_df[col].mode()[0], inplace=True)\n",
    "    else:\n",
    "        print(f\"Target column '{col}' not found in Murs DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f4d6b",
   "metadata": {},
   "source": [
    "## Part 3: Feature Engineering and Target Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd559d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for Murs DataFrame\n",
    "# We'll exclude the target columns and ID columns from features\n",
    "excluded_features = target_columns + ['Id', 'Sols coupés (Ids)', 'Sols coupants (Ids)']\n",
    "features = [col for col in mur_df.columns if col not in excluded_features]\n",
    "\n",
    "# Separate features and targets\n",
    "X = mur_df[features]\n",
    "y = mur_df[target_columns]\n",
    "\n",
    "# Handle categorical features (text with special French characters)\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "numeric_cols = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Numeric transformer\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical transformer\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# For multi-label classification, we'll use separate models for each target\n",
    "# Or we can combine them into a single target (less recommended due to different natures)\n",
    "# Here we'll proceed with separate models\n",
    "\n",
    "# Get feature names after one-hot encoding\n",
    "# For numeric features\n",
    "numeric_feature_names = numeric_cols.tolist()\n",
    "\n",
    "# For categorical features\n",
    "if len(categorical_cols) > 0:\n",
    "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    categorical_feature_names = ohe.get_feature_names_out(categorical_cols).tolist()\n",
    "    all_feature_names = numeric_feature_names + categorical_feature_names\n",
    "else:\n",
    "    all_feature_names = numeric_feature_names\n",
    "\n",
    "print(f\"Total features after preprocessing: {len(all_feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95fecb",
   "metadata": {},
   "source": [
    "## Part 4: Exploratory Data Analysis and Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for each target variable\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# For numeric features only (correlation requires numeric data)\n",
    "numeric_df = X[numeric_cols]\n",
    "\n",
    "# Add targets to the numeric_df for correlation\n",
    "for target in target_columns:\n",
    "    if target in mur_df.columns:\n",
    "        # Encode target for correlation\n",
    "        le = LabelEncoder()\n",
    "        encoded_target = le.fit_transform(mur_df[target])\n",
    "        numeric_df[target] = encoded_target\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot heatmap for each target\n",
    "for i, target in enumerate(target_columns, 1):\n",
    "    if target in numeric_df.columns:\n",
    "        plt.subplot(2, 2, i)\n",
    "        target_corr = corr_matrix[target].sort_values(ascending=False)\n",
    "        sns.barplot(x=target_corr.values[1:11], y=target_corr.index[1:11])\n",
    "        plt.title(f'Top 10 Features Correlated with {target}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Analyze distribution of target variables\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, target in enumerate(target_columns, 1):\n",
    "    if target in mur_df.columns:\n",
    "        plt.subplot(2, 2, i)\n",
    "        sns.countplot(y=mur_df[target], order=mur_df[target].value_counts().index)\n",
    "        plt.title(f'Distribution of {target}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# SHAP analysis for feature importance (sample for one target)\n",
    "if '012EC_Ouvrage' in mur_df.columns:\n",
    "    # Sample a subset for faster SHAP computation\n",
    "    X_sample = X_processed[:1000] if X_processed.shape[0] > 1000 else X_processed\n",
    "\n",
    "    # Train a model for this target\n",
    "    y_target = mur_df['012EC_Ouvrage']\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_target)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_sample, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Compute SHAP values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "    # Plot summary\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=all_feature_names, class_names=le.classes_)\n",
    "    plt.title('SHAP Summary for 012EC_Ouvrage Prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a9247",
   "metadata": {},
   "source": [
    "## Part 5: Model Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate models for a target variable\n",
    "def train_evaluate_models(X, y, target_name, models):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple models for a target variable\n",
    "\n",
    "    Args:\n",
    "        X: Features (processed)\n",
    "        y: Target variable\n",
    "        target_name: Name of the target variable\n",
    "        models: Dictionary of models to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of model performances\n",
    "    \"\"\"\n",
    "    # Encode target if categorical\n",
    "    if y.dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining {model_name} for {target_name}...\")\n",
    "\n",
    "        try:\n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Evaluate\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "            # Store results\n",
    "            results[model_name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': report['weighted avg']['precision'],\n",
    "                'recall': report['weighted avg']['recall'],\n",
    "                'f1': report['weighted avg']['f1-score']\n",
    "            }\n",
    "\n",
    "            print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            # Save model based on type\n",
    "            if hasattr(model, 'layers'):  # Keras model\n",
    "                model_path = f\"dlmodels/{target_name}_{model_name}.h5\"\n",
    "                model.save(model_path)\n",
    "            elif 'boost' in model_name.lower() or 'forest' in model_name.lower():\n",
    "                model_path = f\"mlmodels/{target_name}_{model_name}.pkl\"\n",
    "                import joblib\n",
    "                joblib.dump(model, model_path)\n",
    "            else:\n",
    "                model_path = f\"simplemodels/{target_name}_{model_name}.pkl\"\n",
    "                import joblib\n",
    "                joblib.dump(model, model_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {model_name}: {e}\")\n",
    "            results[model_name] = None\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(random_state=42),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, verbose=0)\n",
    "\n",
    "}\n",
    "\n",
    "# Add a simple neural network\n",
    "def create_nn_model(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(0.001),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# For each target variable, train and evaluate models\n",
    "all_results = {}\n",
    "\n",
    "for target in target_columns:\n",
    "    if target in mur_df.columns:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training models for target: {target}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        y_target = mur_df[target]\n",
    "\n",
    "        # Skip if all values are the same\n",
    "        if len(y_target.unique()) == 1:\n",
    "            print(f\"Skipping {target} - only one class present.\")\n",
    "            continue\n",
    "\n",
    "        # Add neural network to models\n",
    "        output_dim = len(y_target.unique())\n",
    "        nn_model = create_nn_model(X_processed.shape[1], output_dim)\n",
    "        models['NeuralNetwork'] = nn_model\n",
    "\n",
    "        # Train and evaluate\n",
    "        results = train_evaluate_models(X_processed, y_target, target, models)\n",
    "        all_results[target] = results\n",
    "\n",
    "        # Remove NN for next target (to recreate with correct output dim)\n",
    "        del models['NeuralNetwork']\n",
    "\n",
    "        # Plot model comparison\n",
    "        if results:\n",
    "            df_results = pd.DataFrame(results).T\n",
    "            df_results['accuracy'].plot(kind='bar', title=f'Model Accuracy for {target}')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Display all results\n",
    "for target, results in all_results.items():\n",
    "    print(f\"\\nResults for {target}:\")\n",
    "    if results:\n",
    "        display(pd.DataFrame(results).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b79ed",
   "metadata": {},
   "source": [
    "## Part 6: Model Interpretation and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490629b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interpret best model for each target\n",
    "def interpret_best_model(target, results, X_processed, y_target):\n",
    "    \"\"\"\n",
    "    Interpret the best model for a target using SHAP\n",
    "\n",
    "    Args:\n",
    "        target: Target variable name\n",
    "        results: Dictionary of model results\n",
    "        X_processed: Processed features\n",
    "        y_target: Target values\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "\n",
    "    # Find best model by accuracy\n",
    "    df_results = pd.DataFrame(results).T\n",
    "    best_model_name = df_results['accuracy'].idxmax()\n",
    "    best_model_accuracy = df_results.loc[best_model_name, 'accuracy']\n",
    "\n",
    "    print(f\"\\nInterpreting best model for {target}: {best_model_name} (Accuracy: {best_model_accuracy:.4f})\")\n",
    "\n",
    "    # Load the best model\n",
    "    if 'NeuralNetwork' in best_model_name:\n",
    "        model_path = f\"dlmodels/{target}_{best_model_name}.h5\"\n",
    "        best_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "        # For neural networks, we'll use a different explainer\n",
    "        # Sample data for faster computation\n",
    "        X_sample = X_processed[:100] if X_processed.shape[0] > 100 else X_processed\n",
    "\n",
    "        # Create a SHAP explainer\n",
    "        explainer = shap.DeepExplainer(best_model, X_sample)\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "        # Plot summary\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X_sample, feature_names=all_feature_names)\n",
    "        plt.title(f'SHAP Summary for {target} ({best_model_name})')\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        if 'boost' in best_model_name.lower() or 'forest' in best_model_name.lower():\n",
    "            model_path = f\"mlmodels/{target}_{best_model_name}.pkl\"\n",
    "        else:\n",
    "            model_path = f\"simplemodels/{target}_{best_model_name}.pkl\"\n",
    "\n",
    "        import joblib\n",
    "        best_model = joblib.load(model_path)\n",
    "\n",
    "        # Create SHAP explainer\n",
    "        X_sample = X_processed[:100] if X_processed.shape[0] > 100 else X_processed\n",
    "\n",
    "        if hasattr(best_model, 'predict_proba'):\n",
    "            explainer = shap.TreeExplainer(best_model)\n",
    "            shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "            # Plot summary\n",
    "            plt.figure()\n",
    "            shap.summary_plot(shap_values, X_sample, feature_names=all_feature_names)\n",
    "            plt.title(f'SHAP Summary for {target} ({best_model_name})')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Cannot create SHAP explainer for {best_model_name}\")\n",
    "\n",
    "# Interpret best models for each target\n",
    "for target, results in all_results.items():\n",
    "    y_target = mur_df[target]\n",
    "    interpret_best_model(target, results, X_processed, y_target)\n",
    "\n",
    "# Final recommendations\n",
    "print(\"\\nFinal Recommendations:\")\n",
    "print(\"1. The best performing models have been saved in their respective folders (simplemodels/, mlmodels/, dlmodels/)\")\n",
    "print(\"2. SHAP analysis has been provided for model interpretability\")\n",
    "print(\"3. Consider feature engineering based on the correlation and SHAP analysis\")\n",
    "print(\"4. For deployment, use the best model for each target variable\")\n",
    "print(\"5. Monitor model performance over time as new data becomes available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0590ec",
   "metadata": {},
   "source": [
    "## Part 7: Learning Curves and Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c2ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot learning curves\n",
    "def plot_learning_curve(model, X, y, model_name, target_name):\n",
    "    \"\"\"\n",
    "    Plot learning curves for a model\n",
    "\n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        X: Features\n",
    "        y: Target\n",
    "        model_name: Name of the model\n",
    "        target_name: Name of the target variable\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import learning_curve\n",
    "\n",
    "    # If y is categorical, encode it\n",
    "    if y.dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # Create CV training and test scores for various training set sizes\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X, y, cv=5, scoring='accuracy',\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5))\n",
    "\n",
    "    # Calculate mean and standard deviation for training set scores\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "    # Calculate mean and standard deviation for test set scores\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')\n",
    "    plt.plot(train_sizes, test_mean, 'o-', color='green', label='Cross-validation score')\n",
    "\n",
    "    # Draw bands\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\n",
    "\n",
    "    # Create plot\n",
    "    plt.title(f'Learning Curve for {model_name} ({target_name})')\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves for best models\n",
    "for target, results in all_results.items():\n",
    "    if results:\n",
    "        # Find best model by accuracy\n",
    "        df_results = pd.DataFrame(results).T\n",
    "        best_model_name = df_results['accuracy'].idxmax()\n",
    "\n",
    "        # Load the best model\n",
    "        if 'NeuralNetwork' in best_model_name:\n",
    "            model_path = f\"dlmodels/{target}_{best_model_name}.h5\"\n",
    "            best_model = tf.keras.models.load_model(model_path)\n",
    "        elif 'boost' in best_model_name.lower() or 'forest' in best_model_name.lower():\n",
    "            model_path = f\"mlmodels/{target}_{best_model_name}.pkl\"\n",
    "            import joblib\n",
    "            best_model = joblib.load(model_path)\n",
    "        else:\n",
    "            model_path = f\"simplemodels/{target}_{best_model_name}.pkl\"\n",
    "            import joblib\n",
    "            best_model = joblib.load(model_path)\n",
    "\n",
    "        # Get target data\n",
    "        y_target = mur_df[target]\n",
    "\n",
    "        # Plot learning curve\n",
    "        plot_learning_curve(best_model, X_processed, y_target, best_model_name, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad02ea",
   "metadata": {},
   "source": [
    "Explanation and Next Steps\n",
    "This comprehensive solution provides:\n",
    "\n",
    "Data Loading and Preprocessing: Handles the complex relationships between different BIM elements (Murs, Sols, Poutres, Poteaux) and processes the French text data with special characters.\n",
    "\n",
    "Feature Engineering: Creates relationship features between different BIM elements based on their intersections and cuts.\n",
    "\n",
    "Exploratory Data Analysis: Includes correlation analysis and target distribution visualization.\n",
    "\n",
    "Model Training: Evaluates multiple machine learning models (Logistic Regression, Random Forest, SVM, XGBoost, LightGBM) and a neural network for each target variable.\n",
    "\n",
    "Model Interpretation: Uses SHAP values to explain model predictions and identify important features.\n",
    "\n",
    "Model Saving: Saves the best models in appropriate folders based on their complexity (simplemodels/, mlmodels/, dlmodels/).\n",
    "\n",
    "Learning Curves: Visualizes model performance with increasing training data size.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "Deploy the best models for each target variable in your BIM system.\n",
    "\n",
    "Set up monitoring to track model performance over time.\n",
    "\n",
    "Consider implementing an ensemble approach if prediction accuracy needs improvement.\n",
    "\n",
    "Explore more sophisticated deep learning architectures if you have sufficient data.\n",
    "\n",
    "Regularly update the models with new project data to maintain accuracy.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIMpredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
