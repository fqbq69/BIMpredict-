{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1fae56",
   "metadata": {},
   "source": [
    "# Bim_Predict NoteBook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08c1ec",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227613f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define project folder paths\n",
    "# Data directories\n",
    "BASE_DIR = \"../../\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "PREDICTED_DATA_DIR = os.path.join(DATA_DIR, \"predicting_data\")\n",
    "TESTING_DATA_DIR = os.path.join(DATA_DIR, \"testing_data\")\n",
    "\n",
    "# Model directories\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ML_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/machine_learning\")\n",
    "DL_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/deep_learning\")\n",
    "OTHER_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/other\")\n",
    "\n",
    "# Python modules and plots directories\n",
    "PYTHON_MODULES_DIR = os.path.join(BASE_DIR, \"python_modules\")\n",
    "PLOTS_DIR = os.path.join(BASE_DIR, \"plots\")\n",
    "\n",
    "# List of directories to create\n",
    "directories = [\n",
    "    RAW_DATA_DIR, PROCESSED_DATA_DIR, PREDICTED_DATA_DIR,\n",
    "    MODELS_DIR, ML_MODELS_DIR, DL_MODELS_DIR, OTHER_MODELS_DIR,\n",
    "    PYTHON_MODULES_DIR, PLOTS_DIR\n",
    "]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e384cf",
   "metadata": {},
   "source": [
    "<!-- ### Paths Creating && Data Importing -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4763e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# List all Excel files in RAW_DATA_DIR\n",
    "excel_files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith(\".xlsx\") or f.endswith(\".xls\")]\n",
    "\n",
    "# Dictionary to store DataFrames for each file and sheet\n",
    "dataframes = {}\n",
    "\n",
    "# Process each Excel file\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(RAW_DATA_DIR, file)\n",
    "    print(f\"Loading: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # Load Excel file\n",
    "        excel_data = pd.ExcelFile(file_path)\n",
    "\n",
    "        # Load all sheets dynamically\n",
    "        for sheet_name in excel_data.sheet_names:\n",
    "            df = excel_data.parse(sheet_name)\n",
    "\n",
    "            # Save DataFrame with a unique identifier\n",
    "            dataframes[f\"{file}_{sheet_name}\"] = df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# Display summary of loaded data\n",
    "print(f\"\\nTotal files processed: {len(dataframes)}\")\n",
    "for key, df in dataframes.items():\n",
    "    print(f\"Loaded DataFrame: {key}, Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1514a",
   "metadata": {},
   "source": [
    "<!-- ### Data Cleaning && PreProcessing -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc959f",
   "metadata": {},
   "source": [
    "## PreProcessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define required columns dynamically\n",
    "required_columns = {\n",
    "    \"Murs\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"Hauteur\",\n",
    "             \"Epaisseur\", \"AI\", \"AS\", \"Sols en intersection\", \"Sols coup√©s (u)\", \"Sols coup√©s (Ids)\",\n",
    "             \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Sol au-dessus\", \"Sol au-dessous\", \"Fen√™tres\", \"Portes\",\n",
    "             \"Ouvertures\", \"Murs imbriqu√©s\", \"Mur multicouche\", \"Mur empil√©\", \"Profil modifi√©\", \"Extension inf√©rieure\",\n",
    "             \"Extension sup√©rieure\", \"Partie inf√©rieure attach√©e\", \"Partie sup√©rieure attach√©e\", \"D√©calage sup√©rieur\",\n",
    "             \"D√©calage inf√©rieur\", \"Mat√©riau structurel\"],\n",
    "\n",
    "    \"Sols\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"Murs en intersection\",\n",
    "             \"Murs coup√©s (u)\", \"Murs coup√©s (Ids)\", \"Murs coupants (u)\", \"Murs coupants (Ids)\", \"Poutres en intersection\",\n",
    "             \"Poutres coup√©s (u)\", \"Poutres coup√©s (Ids)\", \"Poutres coupants (u)\", \"Poutres coupants (Ids)\",\n",
    "             \"Poteaux en intersection\", \"Poteaux coup√©s (u)\", \"Poteaux coup√©s (Ids)\", \"Poteaux coupants (u)\",\n",
    "             \"Poteaux coupants (Ids)\", \"Ouvertures\", \"Sol multicouche\", \"Profil modifi√©\", \"D√©calage par rapport au niveau\",\n",
    "             \"Epaisseur\", \"Li√© au volume\", \"Etude de l'√©l√©vation √† la base\", \"Etude de l'√©l√©vation en haut\",\n",
    "             \"Epaisseur du porteur\", \"El√©vation au niveau du noyau inf√©rieur\", \"El√©vation au niveau du noyau sup√©rieur\",\n",
    "             \"El√©vation en haut\", \"El√©vation √† la base\", \"Mat√©riau structurel\"],\n",
    "\n",
    "    \"Poutres\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"AI\", \"AS\",\n",
    "                \"Hauteur totale\", \"Hauteur\", \"Sols en intersection\", \"Sols coup√©s (u)\", \"Sols coup√©s (Ids)\",\n",
    "                \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Sol au-dessus\", \"Sol au-dessous\", \"Poteaux en intersection\",\n",
    "                \"Poteaux coup√©s (u)\", \"Poteaux coup√©s (Ids)\", \"Poteaux coupants (u)\", \"Poteaux coupants (Ids)\",\n",
    "                \"Etat de la jonction\", \"Valeur de d√©calage Z\", \"Justification Z\", \"Valeur de d√©calage Y\", \"Justification Y\",\n",
    "                \"Justification YZ\", \"Mat√©riau structurel\", \"El√©vation du niveau de r√©f√©rence\", \"El√©vation en haut\",\n",
    "                \"Rotation de la section\", \"Orientation\", \"D√©calage du niveau d'arriv√©e\", \"D√©calage du niveau de d√©part\",\n",
    "                \"El√©vation √† la base\", \"Longueur de coupe\", \"Longueur\", \"hauteur_section\", \"largeur_section\"],\n",
    "\n",
    "    \"Poteaux\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"AI\", \"AS\",\n",
    "                \"Hauteur\", \"Longueur\", \"Partie inf√©rieure attach√©e\", \"Partie sup√©rieure attach√©e\", \"Sols en intersection\",\n",
    "                \"Sols coup√©s (u)\", \"Sols coup√©s (Ids)\", \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Poutres en intersection\",\n",
    "                \"Poutres coup√©s (u)\", \"Poutres coup√©s (Ids)\", \"Poutres coupants (u)\", \"Poutres coupants (Ids)\",\n",
    "                \"Mat√©riau structurel\", \"D√©calage sup√©rieur\", \"D√©calage inf√©rieur\", \"Diam√®tre poteau\", \"h\", \"b\",\n",
    "                \"hauteur_section\", \"largeur_section\"]\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store filtered dataframes\n",
    "cleaned_dataframes = {}\n",
    "\n",
    "for df_name, df in dataframes.items():\n",
    "    print(f\"\\nüü¢ Original shape of {df_name}: {df.shape}\")\n",
    "\n",
    "    # Automatically detect the correct category for filtering\n",
    "    for category, columns in required_columns.items():\n",
    "        if category.lower() in df_name.lower():  # Match dynamically\n",
    "            try:\n",
    "                filtered_df = df[columns]  # Keep only the required columns\n",
    "            except KeyError as e:\n",
    "                missing_columns = set(columns) - set(df.columns)\n",
    "                print(f\"‚ö†Ô∏è Missing columns in {df_name}: {missing_columns}. Skipping this dataframe.\")\n",
    "                continue\n",
    "            cleaned_dataframes[df_name] = filtered_df\n",
    "            print(f\"‚úÖ Shape after filtering {df_name}: {filtered_df.shape}\")\n",
    "            break  # Stop looping once the correct match is found\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No matching category for {df_name}, skipping filtering.\")\n",
    "\n",
    "# # Add prefixes to column names based on the dataframe category and update index\n",
    "# for name, df in cleaned_dataframes.items():\n",
    "#     if \"murs\" in name.lower():\n",
    "#         prefix = \"murs_\"\n",
    "#     elif \"sols\" in name.lower():\n",
    "#         prefix = \"sols_\"\n",
    "#     elif \"poutres\" in name.lower():\n",
    "#         prefix = \"poutres_\"\n",
    "#     elif \"poteaux\" in name.lower():\n",
    "#         prefix = \"poteaux_\"\n",
    "#     else:\n",
    "#         prefix = \"\"\n",
    "\n",
    "#     # Rename columns with the prefix\n",
    "#     df.rename(columns=lambda col: f\"{prefix}{col}\" if col.lower() != \"id\" else f\"{prefix}id\", inplace=True)\n",
    "\n",
    "#     # Drop the existing index and set the prefixed ID column as the new index\n",
    "#     id_column = f\"{prefix}id\"\n",
    "#     if id_column in df.columns:\n",
    "#         df.set_index(id_column, inplace=True)\n",
    "#         print(f\"‚úÖ Set '{id_column}' as index for {name}.\")\n",
    "#     else:\n",
    "#         print(f\"‚ö†Ô∏è '{id_column}' column not found in {name}, skipping index setting.\")\n",
    "\n",
    "    # Update the cleaned_dataframes dictionary\n",
    "    # cleaned_dataframes[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Cleaned DataFrames:\")\n",
    "for df_name, df in cleaned_dataframes.items():\n",
    "    print(f\" - {df_name}: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_feature_names(cleaned_dataframes, required_columns):\n",
    "#     \"\"\"Maps cleaned dataframe column names to match required training feature names.\"\"\"\n",
    "#     mapped_dataframes = {}\n",
    "\n",
    "#     for df_name, df in cleaned_dataframes.items():\n",
    "#         for category, expected_columns in required_columns.items():\n",
    "#             if category.lower() in df_name.lower():  # Match dynamically\n",
    "#                 # Create mapping: {cleaned_col_name: expected_col_name}\n",
    "#                 col_mapping = {cleaned_col: expected_col for cleaned_col in df.columns for expected_col in expected_columns if cleaned_col.lower() == expected_col.lower()}\n",
    "\n",
    "#                 # Apply mapping to rename columns\n",
    "#                 df_mapped = df.rename(columns=col_mapping)\n",
    "\n",
    "#                 print(f\"‚úÖ Feature names mapped for {df_name}\")\n",
    "#                 mapped_dataframes[df_name] = df_mapped\n",
    "#                 break  # Stop looping once category is matched\n",
    "\n",
    "#     return mapped_dataframes\n",
    "\n",
    "# # Example usage:\n",
    "# mapped_dataframes = map_feature_names(cleaned_dataframes, required_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a847f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Ensure all column names are lowercase and replace spaces & special characters\n",
    "# def clean_column_names(df):\n",
    "#     df.columns = df.columns.str.lower().str.replace(r\"\\s+\", \"_\", regex=True).str.replace(r\"[^\\w_]\", \"\", regex=True)\n",
    "#     return df\n",
    "\n",
    "# # Apply cleaning dynamically to filtered DataFrames instead of the original `dataframes`\n",
    "# cleaned_dataframes = {name: clean_column_names(df) for name, df in cleaned_dataframes.items()}\n",
    "\n",
    "# print(\"‚úÖ Column names cleaned successfully across all cleaned dataframes!\")\n",
    "\n",
    "# # Ensure duplicates & missing values are removed while storing cleaned versions\n",
    "# final_cleaned_dataframes = {}\n",
    "\n",
    "# for df_name, df in cleaned_dataframes.items():\n",
    "#     print(f\"\\nüü¢ Processing {df_name}...\")\n",
    "\n",
    "#     # Make a copy to prevent unintended modifications\n",
    "#     df = df.copy()\n",
    "\n",
    "#     # Display initial shape\n",
    "#     initial_shape = df.shape\n",
    "#     print(f\"üìå Initial shape: {initial_shape}\")\n",
    "\n",
    "#     # Remove duplicate rows\n",
    "#     duplicates = df.duplicated().sum()\n",
    "#     if duplicates > 0:\n",
    "#         print(f\"‚ö†Ô∏è Found {duplicates} duplicate rows. Removing...\")\n",
    "#         df.drop_duplicates(inplace=True)\n",
    "#     else:\n",
    "#         print(\"‚úÖ No duplicate rows found.\")\n",
    "\n",
    "#     # Detect & drop columns with 100% missing values, except specific columns\n",
    "#     missing_cols = df.columns[df.isnull().mean() == 1]\n",
    "#     exception_keywords = [\"coup√©s\", \"coupants\", \"011ec_lot\", \"012ec_ouvrage\", \"013ec_localisation\", \"014ec_mode_constructif\"]\n",
    "#     cols_to_keep = [col for col in missing_cols if any(keyword in col.lower() for keyword in exception_keywords)]\n",
    "#     cols_to_drop = [col for col in missing_cols if col not in cols_to_keep]\n",
    "\n",
    "#     if len(cols_to_drop) > 0:\n",
    "#         print(f\"‚ö†Ô∏è Dropping {len(cols_to_drop)} completely empty columns: {list(cols_to_drop)}\")\n",
    "#         df.drop(columns=cols_to_drop, inplace=True)\n",
    "#     else:\n",
    "#         print(\"‚úÖ No fully missing columns detected (or all are exceptions).\")\n",
    "\n",
    "#     # Store final cleaned DataFrame\n",
    "#     final_cleaned_dataframes[df_name] = df\n",
    "\n",
    "#     # Display final shape after cleaning\n",
    "#     final_shape = df.shape\n",
    "#     print(f\"üìå Final shape after cleaning: {final_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647fc193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET_COLUMNS = ['011ec_lot', '012ec_ouvrage', '013ec_localisation', '014ec_mode_constructif']\n",
    "# # Identify target columns dynamically across all DataFrames\n",
    "# target_columns_found = set()\n",
    "\n",
    "# for df_name, df in cleaned_dataframes.items():\n",
    "#     print(f\"\\nProcessing dataframe: {df_name}\")\n",
    "#     initial_shape = df.shape  # Store the initial shape of the dataframe\n",
    "\n",
    "#     # Check for missing target columns\n",
    "#     for target in TARGET_COLUMNS:\n",
    "#         target_column_name = f\"{df_name.split('_')[-1].lower()}_{target.lower()}\"  # Respect naming policy\n",
    "#         if target_column_name not in df.columns:\n",
    "#             print(f\"‚ö†Ô∏è Target column '{target_column_name}' does not exist in dataframe '{df_name}'.\")\n",
    "#             # Add the missing target column with default values (e.g., NaN)\n",
    "#             df[target_column_name] = float('nan')\n",
    "#             print(f\"‚úÖ Added missing target column '{target_column_name}' to dataframe '{df_name}'.\")\n",
    "\n",
    "#     final_shape = df.shape  # Store the final shape of the dataframe\n",
    "#     if initial_shape != final_shape:\n",
    "#         print(f\"üìä Shape before: {initial_shape}, Shape after: {final_shape}\")\n",
    "\n",
    "#     # Display the target columns in the dataframe\n",
    "#     target_columns_in_df = [col for col in df.columns if any(target.lower() in col.lower() for target in TARGET_COLUMNS)]\n",
    "#     print(f\"üéØ Target columns in '{df_name}': {target_columns_in_df}\")\n",
    "\n",
    "#     # Update the cleaned_dataframes dictionary\n",
    "#     cleaned_dataframes[df_name] = df\n",
    "\n",
    "# # Display all detected target columns across datasets\n",
    "# print(f\"\\nTarget columns detected across datasets: {target_columns_found}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates and 100% missing columns (with exceptions)\n",
    "def process_dataframe(df_name, exception_keywords):\n",
    "    # Access the dataframe from cleaned_dataframes\n",
    "    df = cleaned_dataframes[df_name].copy()\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    initial_shape = df.shape\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    duplicates_removed = initial_shape[0] - df.shape[0]\n",
    "    if duplicates_removed > 0:\n",
    "        print(f\"üü¢ Removed {duplicates_removed} duplicate rows from {df_name}.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ No duplicate rows found in {df_name}.\")\n",
    "\n",
    "    # Identify fully missing columns\n",
    "    missing_cols = df.columns[df.isnull().mean() == 1]\n",
    "    # Keep columns that contain an exception keyword or are in target_columns intact\n",
    "    cols_to_drop = [\n",
    "        col for col in missing_cols\n",
    "        if not any(keyword in col.lower() for keyword in exception_keywords)\n",
    "    ]\n",
    "\n",
    "    if cols_to_drop:\n",
    "        print(f\"üü† Dropping fully missing columns from {df_name}: {cols_to_drop}\")\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "    else:\n",
    "        print(f\"‚úÖ No columns dropped from {df_name}; all fully missing columns are exceptions.\")\n",
    "\n",
    "    # Fill missing values in columns containing \"coup√©\" or \"coupants\"\n",
    "    columns_to_fill = [col for col in df.columns if \"coup√©\" in col.lower() or \"coupants\" in col.lower()]\n",
    "    if columns_to_fill:\n",
    "        print(f\"üîµ Filling missing values with 0 for columns in {df_name}: {columns_to_fill}\")\n",
    "        df[columns_to_fill] = df[columns_to_fill].fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply processing on all dataframes in cleaned_dataframes\n",
    "exception_keywords = [\"coup√©s\", \"coupants\"]\n",
    "\n",
    "processed_dataframes = {}\n",
    "for df_name in cleaned_dataframes.keys():\n",
    "    print(f\"\\nüîç Processing dataframe: {df_name}\")\n",
    "    processed_dataframes[df_name] = process_dataframe(df_name, exception_keywords)\n",
    "    final_shape = processed_dataframes[df_name].shape\n",
    "    print(f\"‚úÖ Final shape of {df_name}: {final_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target columns dynamically across all DataFrames\n",
    "TARGET_COLUMNS = ['011EC_Lot', '012EC_Ouvrage', '013EC_Localisation', '014EC_Mode Constructif']\n",
    "\n",
    "# Check and add missing target columns\n",
    "for df_name, df in processed_dataframes.items():\n",
    "    print(f\"\\nProcessing dataframe: {df_name}\")\n",
    "    initial_shape = df.shape  # Store the initial shape of the dataframe\n",
    "\n",
    "    for target in TARGET_COLUMNS:\n",
    "        if target in df.columns:\n",
    "            print(f\"‚úÖ Target column '{target}' found in dataframe '{df_name}'.\")\n",
    "\n",
    "            # Check for missing data in the target column\n",
    "            missing_count = df[target].isnull().sum()\n",
    "            total_count = len(df)\n",
    "            missing_percentage = (missing_count / total_count) * 100\n",
    "            if missing_count > 0:\n",
    "                print(f\"‚ö†Ô∏è Target column '{target}' has {missing_count} missing values ({missing_percentage:.2f}%).\")\n",
    "\n",
    "                # Drop rows if missing data is less than 10%\n",
    "                if missing_percentage < 10:\n",
    "                    df = df[df[target].notnull()]\n",
    "                    print(f\"‚úÖ Dropped rows with missing values in '{target}' (less than 10%).\")\n",
    "            else:\n",
    "                print(f\"‚úÖ Target column '{target}' has no missing values.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Target column '{target}' does not exist in dataframe '{df_name}'. Adding it...\")\n",
    "            # Add the missing target column with default values (e.g., NaN)\n",
    "            df[target] = float('nan')\n",
    "            print(f\"‚úÖ Added missing target column '{target}' to dataframe '{df_name}'.\")\n",
    "\n",
    "    final_shape = df.shape  # Store the final shape of the dataframe\n",
    "    if initial_shape != final_shape:\n",
    "        print(f\"üìä Shape before: {initial_shape}, Shape after: {final_shape}\")\n",
    "\n",
    "    # Update the cleaned_dataframes dictionary\n",
    "    cleaned_dataframes[df_name] = processed_dataframes[df_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb298214",
   "metadata": {},
   "source": [
    "<!-- ### Exploratory Data Analysis (EDA) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9971ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure missing values are filled in the processed datasets unless in TARGET_COLUMNS\n",
    "for df_name, df in cleaned_dataframes.items():\n",
    "    print(f\"\\nüü¢ Filling missing values for {df_name}...\")\n",
    "\n",
    "    # Display shape before filling missing values\n",
    "    initial_shape = df.shape\n",
    "    print(f\"üìå Initial shape before filling NaN: {initial_shape}\")\n",
    "\n",
    "    # Fill missing values with 0 for non-target columns\n",
    "    non_target_columns = [col for col in df.columns if col not in TARGET_COLUMNS]\n",
    "    df[non_target_columns] = df[non_target_columns].fillna(0)\n",
    "\n",
    "    # Store updated dataframe back\n",
    "    cleaned_dataframes[df_name] = df\n",
    "\n",
    "    # Display shape after processing\n",
    "    final_shape = df.shape\n",
    "    print(f\"‚úÖ Final shape after filling NaN: {final_shape}\")\n",
    "\n",
    "print(\"üöÄ Missing values successfully handled across all datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f2a66",
   "metadata": {},
   "source": [
    "## EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to remove low-variance & highly correlated features\n",
    "def optimize_feature_selection(df, variance_threshold=0.02, correlation_threshold=0.98):\n",
    "    print(f\"\\nüîç Processing {df.shape[0]} rows & {df.shape[1]} columns\")\n",
    "\n",
    "    # Step 1: Remove Low-Variance Features\n",
    "    selector = VarianceThreshold(variance_threshold)\n",
    "    numeric_df = df.select_dtypes(include=[\"number\"])  # Focus only on numerical columns\n",
    "    selector.fit(numeric_df)\n",
    "\n",
    "    low_variance_cols = numeric_df.columns[~selector.get_support()]\n",
    "    keep_cols = [col for col in low_variance_cols if any(keyword in col.lower() for keyword in [\"coup√©s\", \"coupants\"])]\n",
    "    drop_cols = [col for col in low_variance_cols if col not in keep_cols and col not in TARGET_COLUMNS]\n",
    "\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "    print(f\"‚ö†Ô∏è Dropped {len(drop_cols)} low-variance columns (excluding 'coup√©s' and target columns): {drop_cols}\")\n",
    "\n",
    "    # Step 2: Remove Highly Correlated Features\n",
    "    numeric_df = df.select_dtypes(include=[\"number\"])\n",
    "    correlation_matrix = numeric_df.corr().abs()\n",
    "    upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "    correlated_features = [\n",
    "        col for col in upper_triangle.columns\n",
    "        if any(upper_triangle[col] > correlation_threshold) and col not in TARGET_COLUMNS\n",
    "    ]\n",
    "\n",
    "    df.drop(columns=correlated_features, inplace=True)\n",
    "    print(f\"‚ö†Ô∏è Dropped {len(correlated_features)} highly correlated columns (excluding target columns): {correlated_features}\")\n",
    "\n",
    "    print(f\"‚úÖ Final shape after filtering: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Apply optimized feature selection to all datasets\n",
    "final_cleaned_dataframes = {name: optimize_feature_selection(df) for name, df in cleaned_dataframes.items()}\n",
    "\n",
    "print(\"üöÄ Optimized feature selection completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics for all cleaned sheets\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    print(f\"\\nSummary statistics for {df_name}:\")\n",
    "\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a3756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot histograms for numerical columns\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    df.hist(figsize=(15,10), bins=20)\n",
    "    plt.suptitle(f\"Distribution of Features in {df_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute correlation matrices for numeric columns\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    numeric_df = df.select_dtypes(include=[\"number\"])\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "    plt.title(f\"Correlation Matrix for {df_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c272582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure base plots directory exists\n",
    "if not os.path.exists(PLOTS_DIR):\n",
    "    os.makedirs(PLOTS_DIR)\n",
    "\n",
    "# Function to generate subfolder paths for each Excel file\n",
    "def get_plot_subfolder(file_name):\n",
    "    subfolder_name = f\"{file_name.replace('.xlsx', '').replace('.xls', '')}_Plots\"\n",
    "    subfolder_path = os.path.join(PLOTS_DIR, subfolder_name)\n",
    "\n",
    "    # Create the subfolder if it doesn't exist\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        os.makedirs(subfolder_path)\n",
    "\n",
    "    return subfolder_path\n",
    "\n",
    "# Save histograms\n",
    "for df_name, df in cleaned_dataframes.items():\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    df.hist(bins=20)\n",
    "    plt.suptitle(f\"Distribution of Features in {df_name}\")\n",
    "\n",
    "    # Extract the corresponding Excel file name\n",
    "    file_name = df_name.split(\"_\")[0]  # Extracts RawData_Cibles.xlsx from \"RawData_Cibles.xlsx_Murs\"\n",
    "    plot_subfolder = get_plot_subfolder(file_name)\n",
    "\n",
    "    # Define save path\n",
    "    plot_path = os.path.join(plot_subfolder, f\"{df_name}_histogram.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"Saved histogram in: {plot_path}\")\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "# Save correlation matrices\n",
    "for df_name, df in cleaned_dataframes.items():\n",
    "    numeric_df = df.select_dtypes(include=[\"number\"])\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "    plt.title(f\"Correlation Matrix for {df_name}\")\n",
    "\n",
    "    # Extract Excel file name and subfolder\n",
    "    file_name = df_name.split(\"_\")[0]\n",
    "    plot_subfolder = get_plot_subfolder(file_name)\n",
    "\n",
    "    # Define save path\n",
    "    plot_path = os.path.join(plot_subfolder, f\"{df_name}_correlation.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"Saved correlation matrix in: {plot_path}\")\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabfb1e",
   "metadata": {},
   "source": [
    "<!-- ### Feature Selection -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96114934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62fbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target columns dynamically across all DataFrames\n",
    "target_columns_found = set()\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    found_targets = [\n",
    "        col for col in df.columns\n",
    "        if any(target.lower() in col.lower() for target in TARGET_COLUMNS)\n",
    "    ]\n",
    "    target_columns_found.update(found_targets)\n",
    "\n",
    "print(f\"\\nTarget columns detected across datasets: {target_columns_found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function to generate subfolder paths for storing SHAP plots\n",
    "def get_plot_subfolder(file_name):\n",
    "    subfolder_name = f\"{file_name}_Plots\"\n",
    "    subfolder_path = os.path.join(PLOTS_DIR, subfolder_name)\n",
    "\n",
    "    # Create the subfolder if it doesn't exist\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        os.makedirs(subfolder_path)\n",
    "\n",
    "    return subfolder_path\n",
    "\n",
    "# ‚úÖ Ensure SHAP is applied to the fully processed dataset\n",
    "final_shap_dataframes = final_cleaned_dataframes  # Use the cleaned dataset after variance/correlation removal\n",
    "\n",
    "print(\"üöÄ SHAP analysis will now use the final processed data!\")\n",
    "\n",
    "for target_column in TARGET_COLUMNS:\n",
    "    for df_name, df in final_cleaned_dataframes.items():\n",
    "        # Check if any column in the dataframe contains the target column name as a substring\n",
    "        matching_columns = [col for col in df.columns if target_column in col]\n",
    "        if matching_columns:\n",
    "            unique_values = df[matching_columns[0]].nunique()\n",
    "            print(f\"{target_column} in {df_name} has {unique_values} unique values.\")\n",
    "        else:\n",
    "            print(f\"{target_column} not found in {df_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "\n",
    "# Ensure SHAP initializes properly\n",
    "shap.initjs()\n",
    "\n",
    "# Function to create subfolder for SHAP plots\n",
    "def get_plot_subfolder(file_name):\n",
    "    subfolder_name = f\"{file_name}_Plots\"\n",
    "    subfolder_path = os.path.join(\"plots\", subfolder_name)  # Adjust path as needed\n",
    "\n",
    "    # Create subfolder if it doesn‚Äôt exist\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        os.makedirs(subfolder_path)\n",
    "\n",
    "    return subfolder_path\n",
    "\n",
    "# Loop through all fully processed dataframes\n",
    "for df_name, df in final_cleaned_dataframes.items():  # ‚úÖ Use fully processed dataset\n",
    "    print(f\"\\nProcessing SHAP for {df_name}...\")\n",
    "\n",
    "    # Identify available target columns in the current dataframe\n",
    "    # Identify target columns by checking if any target column is a substring of the dataframe's columns\n",
    "    existing_target_columns = target_columns_found\n",
    "\n",
    "    if existing_target_columns:\n",
    "        print(f\"üéØ Target columns found in {df_name}: {existing_target_columns}\")\n",
    "\n",
    "        for target_column in existing_target_columns:\n",
    "            print(f\"üîç Analyzing SHAP for target: {target_column}\")\n",
    "\n",
    "            # Display shape before training\n",
    "            initial_shape = df.shape\n",
    "            print(f\"üìå Initial shape before SHAP processing: {initial_shape}\")\n",
    "\n",
    "            # Prepare the data\n",
    "            X = df.drop(columns=existing_target_columns)  # Exclude target columns from features\n",
    "\n",
    "            # Convert categorical columns in X to numeric\n",
    "            for col in X.select_dtypes(include=[\"object\"]).columns:\n",
    "                X[col] = X[col].astype(\"category\").cat.codes\n",
    "\n",
    "            # Convert target column to numeric\n",
    "            y = df[target_column].astype(\"category\").cat.codes\n",
    "\n",
    "            # Train RandomForestClassifier\n",
    "            model = RandomForestClassifier()\n",
    "            model.fit(X, y)\n",
    "\n",
    "            # Compute SHAP values\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X)\n",
    "\n",
    "            # Verify SHAP output shape before plotting\n",
    "            print(f\"üìä SHAP values shape: {len(shap_values)}, Feature matrix shape: {X.shape}\")\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[0]  # Use first class for visualization in multi-class models\n",
    "\n",
    "            # Create subfolder for SHAP plots\n",
    "            plot_subfolder = get_plot_subfolder(f\"SHAP_{df_name}\")\n",
    "            plot_path = os.path.join(plot_subfolder, f\"{target_column}_SHAP.png\")\n",
    "\n",
    "            # Display & save SHAP summary plot\n",
    "            shap.summary_plot(shap_values, X, show=False)\n",
    "            plt.savefig(plot_path)\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"‚úÖ Saved SHAP plot for {target_column} in: {plot_path}\")\n",
    "\n",
    "            # Display shape after SHAP analysis\n",
    "            final_shape = X.shape\n",
    "            print(f\"üìå Final shape after SHAP processing: {final_shape}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No valid target columns found in {df_name}. Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34f0966",
   "metadata": {},
   "source": [
    "<!-- ## Training and testing  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8315951",
   "metadata": {},
   "source": [
    "<!-- Where to Go from Here?\n",
    "üîπ Feature engineering: If accuracy is low, refine features further\n",
    "üîπ Hyperparameter tuning: Optimize n_estimators, max_depth, etc.\n",
    "üîπ Compare multiple models: Try XGBoost or SVM for better performance\n",
    "\n",
    "Your Next Step\n",
    "Run the model training and evaluation, then let me know if you'd like tuning suggestions or deeper insights! üöÄüî•\n",
    "This is getting exciting‚Äîyou're building something powerful! üí° -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f087ee19",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each dataframe in the dictionary and print its dtypes\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "\tprint(f\"Dtypes for {df_name}:\")\n",
    "\tprint(df.dtypes)\n",
    "\tprint(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead2183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to convert ID strings into a numeric count feature\n",
    "def count_ids(id_string):\n",
    "    \"\"\"Convert string of IDs into a numeric count.\"\"\"\n",
    "    return len(id_string.split(\",\")) if isinstance(id_string, str) else 0\n",
    "\n",
    "# Apply processing to fully cleaned datasets\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    print(f\"\\nüîÑ Processing ID count transformation for {df_name}...\")\n",
    "\n",
    "    # Identify relevant ID columns\n",
    "    id_columns = [col for col in df.columns if any(keyword in col.lower() for keyword in [\"coup√©s_(ids)\", \"coupants_(ids)\"])]\n",
    "\n",
    "    if id_columns:\n",
    "        print(f\"üìå Found ID columns: {id_columns}\")\n",
    "\n",
    "        # Transform ID columns into numeric count and drop originals\n",
    "        df[[f\"{col}_count\" for col in id_columns]] = df[id_columns].applymap(count_ids)\n",
    "        df.drop(columns=id_columns, inplace=True)  # Remove original text-based ID columns\n",
    "\n",
    "    # Ensure only ID-related columns are converted to numeric\n",
    "    df[id_columns] = df[id_columns].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # Store the updated dataframe\n",
    "    final_cleaned_dataframes[df_name] = df\n",
    "\n",
    "    print(f\"‚úÖ Final shape after ID count transformation: {df.shape}\")\n",
    "\n",
    "print(\"üöÄ ID count transformation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0560218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each dataframe in the dictionary and print its dtypes\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "\tprint(f\"Dtypes for {df_name}:\")\n",
    "\tprint(df.dtypes)\n",
    "\tprint(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93439dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    print(f\"\\nüìå {df_name} - Categorical Columns Before Encoding: {categorical_cols.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Dictionaries to store encoders\n",
    "feature_encoders = {}  # Stores encoders for feature columns\n",
    "target_encoders = {}  # Stores encoders for target columns\n",
    "\n",
    "# Encode features and targets separately\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    print(f\"\\nüîÑ Encoding categorical features for {df_name}...\")\n",
    "\n",
    "    # Identify categorical columns again after ID transformation\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    target_cols = [col for col in categorical_cols if col in TARGET_COLUMNS]\n",
    "    feature_cols = [col for col in categorical_cols if col not in TARGET_COLUMNS]\n",
    "\n",
    "    # Encode target columns\n",
    "    for col in target_cols:\n",
    "        encoder = LabelEncoder()\n",
    "        df[col] = encoder.fit_transform(df[col].astype(str))\n",
    "        target_encoders[f\"{df_name}_{col}\"] = encoder\n",
    "        print(f\"‚úÖ Stored Target Encoder for {df_name} - {col}\")\n",
    "\n",
    "    # Encode feature columns\n",
    "    one_hot_cols = []\n",
    "    for col in feature_cols:\n",
    "        encoder = LabelEncoder()\n",
    "        df[col] = encoder.fit_transform(df[col].astype(str))\n",
    "        feature_encoders[f\"{df_name}_{col}\"] = encoder\n",
    "        print(f\"‚úÖ Stored Feature Encoder for {df_name} - {col}\")\n",
    "        one_hot_cols.append(col)  # Mark column for One-Hot Encoding if needed\n",
    "\n",
    "    # Apply One-Hot Encoding only to select categorical variables\n",
    "    if one_hot_cols:\n",
    "        encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "        encoded_values = encoder.fit_transform(df[one_hot_cols])\n",
    "\n",
    "        # Create a new DataFrame with proper column names\n",
    "        encoded_df = pd.DataFrame(encoded_values, index=df.index, columns=encoder.get_feature_names_out(one_hot_cols))\n",
    "\n",
    "        # Remove original one-hot columns and add encoded features\n",
    "        df.drop(columns=one_hot_cols, inplace=True)\n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "    # Save updated dataframe\n",
    "    final_cleaned_dataframes[df_name] = df\n",
    "\n",
    "    print(f\"‚úÖ Successfully encoded categorical features for {df_name}. New shape: {df.shape}\")\n",
    "\n",
    "print(\"üöÄ Final categorical encoding applied successfully across all datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    print(f\"\\nüìå {df_name} - Categorical Columns After Encoding: {categorical_cols.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a76bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_test_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd065a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RandomForestClassifier by Excel\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Train and evaluate models on each dataset\n",
    "# model_results = {}\n",
    "\n",
    "# for key, (X_train, X_test, y_train, y_test) in train_test_data.items():\n",
    "#     print(f\"\\nüöÄ Training model for {key}...\")\n",
    "\n",
    "#     # Train a Random Forest model\n",
    "#     model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     # Predictions\n",
    "#     y_train_pred = model.predict(X_train)  # Predictions on training data\n",
    "#     y_test_pred = model.predict(X_test)  # Predictions on test data\n",
    "\n",
    "#     # Evaluate accuracy\n",
    "#     train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "#     test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "#     model_results[key] = (train_accuracy, test_accuracy)\n",
    "\n",
    "#     print(f\"üìä Training Accuracy for {key}: {train_accuracy:.4f}\")\n",
    "#     print(f\"üéØ Test Accuracy for {key}: {test_accuracy:.4f}\")\n",
    "\n",
    "# # Plot Learning Curve\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# train_accs = [train_accuracy for train_accuracy, _ in model_results.values()]\n",
    "# test_accs = [test_accuracy for _, test_accuracy in model_results.values()]\n",
    "# datasets = list(model_results.keys())\n",
    "\n",
    "# plt.plot(datasets, train_accs, marker='o', label=\"Training Accuracy\", color=\"blue\")\n",
    "# plt.plot(datasets, test_accs, marker='s', label=\"Test Accuracy\", color=\"red\")\n",
    "\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Model Learning Curve Across Datasets\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e49c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split, learning_curve, cross_val_score\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Ensure necessary directories exist\n",
    "# os.makedirs('data/processed_data', exist_ok=True)\n",
    "\n",
    "# def train_random_forest(X_combined, y_combined):\n",
    "#     \"\"\"Trains a single Random Forest model on processed data with evaluation plots.\"\"\"\n",
    "#     print(\"\\nüîç Checking for missing values...\")\n",
    "#     imputer = SimpleImputer(strategy='mean')\n",
    "#     X_combined = pd.DataFrame(imputer.fit_transform(X_combined), columns=X_combined.columns)\n",
    "#     X_combined.dropna(inplace=True)\n",
    "#     y_combined.dropna(inplace=True)\n",
    "\n",
    "#     # Apply feature scaling\n",
    "#     scaler = StandardScaler()\n",
    "#     X_combined = pd.DataFrame(scaler.fit_transform(X_combined), columns=X_combined.columns)\n",
    "\n",
    "#     # Split dataset\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "#     print(f\"\\nüöÄ Training on merged dataset with {X_train.shape[0]} samples...\")\n",
    "\n",
    "#     # Train Random Forest model\n",
    "#     model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     # Predictions\n",
    "#     y_train_pred = model.predict(X_train)\n",
    "#     y_test_pred = model.predict(X_test)\n",
    "\n",
    "#     # Evaluate accuracy\n",
    "#     train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "#     test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#     print(f\"üìä Training Accuracy: {train_accuracy:.4f}\")\n",
    "#     print(f\"üéØ Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#     # Plot Accuracy Comparison\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "#     plt.bar([\"Train Accuracy\", \"Test Accuracy\"], [train_accuracy, test_accuracy], color=['blue', 'red'])\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "#     plt.title(\"Model Performance - Random Forest\")\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Generate Learning Curve\n",
    "#     train_sizes, train_scores, test_scores = learning_curve(\n",
    "#         model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1,\n",
    "#         train_sizes=np.linspace(0.1, 1.0, 5)\n",
    "#     )\n",
    "\n",
    "#     # Compute mean and standard deviation\n",
    "#     train_mean = np.mean(train_scores, axis=1)\n",
    "#     train_std = np.std(train_scores, axis=1)\n",
    "#     test_mean = np.mean(test_scores, axis=1)\n",
    "#     test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "#     # Plot Learning Curve\n",
    "#     plt.figure(figsize=(8, 5))\n",
    "#     plt.plot(train_sizes, train_mean, 'o-', color=\"blue\", label=\"Training Accuracy\")\n",
    "#     plt.plot(train_sizes, test_mean, 'o-', color=\"red\", label=\"Validation Accuracy\")\n",
    "#     plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color=\"blue\")\n",
    "#     plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color=\"red\")\n",
    "#     plt.xlabel(\"Training Set Size\")\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "#     plt.title(\"Learning Curve - Random Forest\")\n",
    "#     plt.legend(loc=\"best\")\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "# def process_data(cleaned_dataframes, target_columns):\n",
    "#     \"\"\"\n",
    "#     From each cleaned df:\n",
    "#       ‚Ä¢ extract y = df[target_columns]\n",
    "#       ‚Ä¢ extract X = df.drop(columns=target_columns)\n",
    "#     then stack them all into one big X, y pair.\n",
    "#     \"\"\"\n",
    "#     X_parts = []\n",
    "#     y_parts = []\n",
    "\n",
    "#     for df_name, df in cleaned_dataframes.items():\n",
    "#         # check that all target columns are present\n",
    "#         missing = set(target_columns) - set(df.columns)\n",
    "#         if missing:\n",
    "#             print(f\"‚ö†Ô∏è  Skipping {df_name}: missing targets {missing}\")\n",
    "#             continue\n",
    "\n",
    "#         # isolate X, y\n",
    "#         y = df[target_columns]\n",
    "#         X = df.drop(columns=target_columns)\n",
    "\n",
    "#         print(f\"‚úÖ  {df_name}: X shape {X.shape}, y shape {y.shape}\")\n",
    "#         X_parts.append(X)\n",
    "#         y_parts.append(y)\n",
    "\n",
    "#     if not X_parts:\n",
    "#         raise ValueError(\"No dataframes contained all target columns!\")\n",
    "\n",
    "#     # concat all the pieces\n",
    "#     X_combined = pd.concat(X_parts, axis=0, ignore_index=True)\n",
    "#     y_combined = pd.concat(y_parts, axis=0, ignore_index=True)\n",
    "\n",
    "#     print(f\"\\nüìä Combined dataset: X {X_combined.shape}, y {y_combined.shape}\")\n",
    "#     return X_combined, y_combined\n",
    "\n",
    "# # Run training function\n",
    "# X_combined, y_combined = process_data(final_cleaned_dataframes, TARGET_COLUMNS)\n",
    "\n",
    "# # train & evaluate\n",
    "# train_random_forest(X_combined, y_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46676ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import joblib\n",
    "# from sklearn.model_selection import train_test_split, learning_curve, cross_val_score\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# # Create necessary directories\n",
    "# os.makedirs('data/processed_data', exist_ok=True)\n",
    "\n",
    "# # Define multiple ML models\n",
    "# models = {\n",
    "#     \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "#     \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "#     \"KNN\": KNeighborsClassifier(n_neighbors=3),\n",
    "#     \"Decision Tree\": DecisionTreeClassifier(random_state=42, max_depth=5),\n",
    "#     \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "#     \"AdaBoost\": AdaBoostClassifier(n_estimators=50, random_state=42),\n",
    "#     \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=50, random_state=42),\n",
    "# }\n",
    "\n",
    "# failed_models = []\n",
    "\n",
    "# def process_data(final_cleaned_dataframes, TARGET_COLUMNS):\n",
    "#     \"\"\"Detects missing values, merges all datasets, resets index, and saves processed data.\"\"\"\n",
    "#     all_X = []\n",
    "#     all_y = []\n",
    "\n",
    "#     print(\"\\n‚úÖ Checking available dataframes and target columns...\")\n",
    "#     print(\"Dataframes found:\", list(final_cleaned_dataframes.keys()))\n",
    "#     print(\"Target columns expected:\", TARGET_COLUMNS)\n",
    "\n",
    "#     for df_name, df in final_cleaned_dataframes.items():\n",
    "#         existing_target_columns = [col for col in df.columns if any(target in col for target in TARGET_COLUMNS)]\n",
    "\n",
    "#         if not existing_target_columns:\n",
    "#             print(f\"‚ö†Ô∏è {df_name}: No matching target columns found.\")\n",
    "#             continue\n",
    "\n",
    "#         print(f\"\\nüîç Processing {df_name} - Found target columns: {existing_target_columns}\")\n",
    "\n",
    "#         for target_column in existing_target_columns:\n",
    "#             print(f\"üìå Processing data for target: {target_column}\")\n",
    "\n",
    "#             X = df.drop(columns=existing_target_columns)\n",
    "#             y = df[target_column]\n",
    "\n",
    "#             if y.nunique() == 1:\n",
    "#                 print(f\"‚ö†Ô∏è Skipping {df_name}_{target_column}: Only one class present.\")\n",
    "#                 continue\n",
    "\n",
    "#             X = X.reset_index(drop=True)\n",
    "#             y = y.reset_index(drop=True)\n",
    "\n",
    "#             all_X.append(X)\n",
    "#             all_y.append(y)\n",
    "\n",
    "#     print(f\"\\n‚úÖ Total datasets processed: {len(all_X)}\")\n",
    "\n",
    "#     if not all_X or not all_y:\n",
    "#         msg = \"üö® No objects to concatenate. Check TARGET_COLUMNS or ensure target values vary.\"\n",
    "#         print(msg)\n",
    "#         raise ValueError(msg)\n",
    "\n",
    "#     X_combined = pd.concat(all_X, axis=0).reset_index(drop=True)\n",
    "#     y_combined = pd.concat(all_y, axis=0).reset_index(drop=True)\n",
    "\n",
    "#     print(f\"\\n‚úÖ Final merged dataset shape: {X_combined.shape}, {y_combined.shape}\")\n",
    "\n",
    "#     return X_combined, y_combined\n",
    "\n",
    "# def train_models(X_combined, y_combined):\n",
    "#     \"\"\"Trains multiple ML models & evaluates performance.\"\"\"\n",
    "#     print(\"\\nüîç Checking for NaN values...\")\n",
    "#     imputer = SimpleImputer(strategy='mean')\n",
    "#     X_combined = pd.DataFrame(imputer.fit_transform(X_combined), columns=X_combined.columns)\n",
    "#     X_combined.dropna(inplace=True)\n",
    "#     y_combined.dropna(inplace=True)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "#     model_results = {}\n",
    "\n",
    "#     plt.figure(figsize=(8, 5))\n",
    "\n",
    "#     for name, model in models.items():\n",
    "#         print(f\"\\nüöÄ Training {name}...\")\n",
    "#         try:\n",
    "#             cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "#             model.fit(X_train, y_train)\n",
    "#             y_pred = model.predict(X_test)\n",
    "\n",
    "#             test_accuracy = accuracy_score(y_test, y_pred)\n",
    "#             model_results[name] = test_accuracy\n",
    "\n",
    "#             print(f\"‚úÖ {name}: Test Accuracy = {test_accuracy:.4f}\")\n",
    "\n",
    "#             # Learning Curve\n",
    "#             train_sizes, train_scores, test_scores = learning_curve(\n",
    "#                 model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1,\n",
    "#                 train_sizes=np.linspace(0.1, 1.0, 5)\n",
    "#             )\n",
    "\n",
    "#             test_mean = np.mean(test_scores, axis=1)\n",
    "#             test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "#             plt.plot(train_sizes, test_mean, marker='o', label=f\"{name} (Acc: {test_mean[-1]:.2f})\")\n",
    "#             plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ö†Ô∏è Error training {name}: {e}\")\n",
    "#             failed_models.append(name)\n",
    "\n",
    "#     plt.xlabel(\"Training Set Size\")\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "#     plt.title(\"Learning Curve - All Models\")\n",
    "#     plt.legend(loc=\"best\")\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Rank models\n",
    "#     print(\"\\nüìä Model Rankings by Test Accuracy:\")\n",
    "#     ranked_models = sorted(model_results.items(), key=lambda x: x[1], reverse=True)\n",
    "#     ranking_df = pd.DataFrame(ranked_models, columns=[\"Model\", \"Test Accuracy\"])\n",
    "#     print(ranking_df.to_string(index=False))\n",
    "\n",
    "#     # Save the top models\n",
    "#     best_models = ranked_models[:2]\n",
    "#     combined_X_train, combined_y_train = X_combined, y_combined\n",
    "\n",
    "#     for name, acc in best_models:\n",
    "#         model = models[name]\n",
    "#         model.fit(combined_X_train, combined_y_train)\n",
    "#         joblib.dump(model, f'models/machine_learning/{name.replace(\" \", \"_\")}_combined.pkl')\n",
    "\n",
    "#     print(\"\\nüöÄ Model evaluation, ranking, and saving completed!\")\n",
    "#     print(f\"‚ö†Ô∏è Models that failed: {failed_models}\")\n",
    "\n",
    "# # Run the pipeline\n",
    "# X_combined, y_combined = process_data(final_cleaned_dataframes, TARGET_COLUMNS)\n",
    "# train_models(X_combined, y_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452215ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, learning_curve, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('data/processed_data', exist_ok=True)\n",
    "\n",
    "# Define ML models (Updated Logistic Regression)\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(solver=\"saga\", max_iter=5000, random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=3),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42, max_depth=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=50, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=50, random_state=42),\n",
    "}\n",
    "\n",
    "failed_models = []\n",
    "\n",
    "def process_data(final_cleaned_dataframes, TARGET_COLUMNS):\n",
    "    \"\"\"Detects missing values, merges all datasets, resets index, and saves processed data.\"\"\"\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "\n",
    "    print(\"\\n‚úÖ Checking available dataframes and target columns...\")\n",
    "    print(\"Dataframes found:\", list(final_cleaned_dataframes.keys()))\n",
    "    print(\"Target columns expected:\", TARGET_COLUMNS)\n",
    "\n",
    "    for df_name, df in final_cleaned_dataframes.items():\n",
    "        existing_target_columns = [col for col in df.columns if any(target in col for target in TARGET_COLUMNS)]\n",
    "\n",
    "        if not existing_target_columns:\n",
    "            print(f\"‚ö†Ô∏è {df_name}: No matching target columns found.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüîç Processing {df_name} - Found target columns: {existing_target_columns}\")\n",
    "\n",
    "        for target_column in existing_target_columns:\n",
    "            print(f\"üìå Processing data for target: {target_column}\")\n",
    "\n",
    "            X = df.drop(columns=existing_target_columns)\n",
    "            y = df[target_column]\n",
    "\n",
    "            if y.nunique() == 1:\n",
    "                print(f\"‚ö†Ô∏è Skipping {df_name}_{target_column}: Only one class present.\")\n",
    "                continue\n",
    "\n",
    "            X = X.reset_index(drop=True)\n",
    "            y = y.reset_index(drop=True)\n",
    "\n",
    "            all_X.append(X)\n",
    "            all_y.append(y)\n",
    "\n",
    "    print(f\"\\n‚úÖ Total datasets processed: {len(all_X)}\")\n",
    "\n",
    "    if not all_X or not all_y:\n",
    "        msg = \"üö® No objects to concatenate. Check TARGET_COLUMNS or ensure target values vary.\"\n",
    "        print(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    X_combined = pd.concat(all_X, axis=0).reset_index(drop=True)\n",
    "    y_combined = pd.concat(all_y, axis=0).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n‚úÖ Final merged dataset shape: {X_combined.shape}, {y_combined.shape}\")\n",
    "\n",
    "    return X_combined, y_combined\n",
    "\n",
    "def train_models(X_combined, y_combined):\n",
    "    \"\"\"Trains multiple ML models & evaluates performance.\"\"\"\n",
    "    print(\"\\nüîç Checking for NaN values...\")\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_combined = pd.DataFrame(imputer.fit_transform(X_combined), columns=X_combined.columns)\n",
    "    X_combined.dropna(inplace=True)\n",
    "    y_combined.dropna(inplace=True)\n",
    "\n",
    "    # Apply Standard Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_combined = pd.DataFrame(scaler.fit_transform(X_combined), columns=X_combined.columns)\n",
    "\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "    model_results = {}\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüöÄ Training {name}...\")\n",
    "        try:\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            test_accuracy = accuracy_score(y_test, y_pred)\n",
    "            model_results[name] = test_accuracy\n",
    "\n",
    "            print(f\"‚úÖ {name}: Test Accuracy = {test_accuracy:.4f}\")\n",
    "\n",
    "            # Learning Curve\n",
    "            train_sizes, train_scores, test_scores = learning_curve(\n",
    "                model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1,\n",
    "                train_sizes=np.linspace(0.1, 1.0, 5)\n",
    "            )\n",
    "\n",
    "            test_mean = np.mean(test_scores, axis=1)\n",
    "            test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "            plt.plot(train_sizes, test_mean, marker='o', label=f\"{name} (Acc: {test_mean[-1]:.2f})\")\n",
    "            plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error training {name}: {e}\")\n",
    "            failed_models.append(name)\n",
    "\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Learning Curve - All Models\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Rank models\n",
    "    print(\"\\nüìä Model Rankings by Test Accuracy:\")\n",
    "    ranked_models = sorted(model_results.items(), key=lambda x: x[1], reverse=True)\n",
    "    ranking_df = pd.DataFrame(ranked_models, columns=[\"Model\", \"Test Accuracy\"])\n",
    "    print(ranking_df.to_string(index=False))\n",
    "\n",
    "    # Save the top models\n",
    "    best_models = ranked_models[:2]\n",
    "    combined_X_train, combined_y_train = X_combined, y_combined\n",
    "\n",
    "    for name, acc in best_models:\n",
    "        model = models[name]\n",
    "        model.fit(combined_X_train, combined_y_train)\n",
    "        joblib.dump(model, os.path.join(ML_MODELS_DIR, f'{name.replace(\" \", \"_\")}_combined.pkl'))\n",
    "\n",
    "    print(\"\\nüöÄ Model evaluation, ranking, and saving completed!\")\n",
    "    print(f\"‚ö†Ô∏è Models that failed: {failed_models}\")\n",
    "\n",
    "# Run the pipeline\n",
    "X_combined, y_combined = process_data(final_cleaned_dataframes, TARGET_COLUMNS)\n",
    "train_models(X_combined, y_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8139f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install xgboost\n",
    "# %pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce378f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Unique values in y_train:\", np.unique(y_train))\n",
    "# print(\"Unique values in y_test:\", np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, learning_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define models and search spaces for Bayesian tuning\n",
    "param_spaces = {\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': Integer(100, 1000),\n",
    "        'max_depth': Integer(3, 30),\n",
    "        'min_samples_split': Integer(2, 15),\n",
    "        'max_features': Categorical(['sqrt', 'log2', None])\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        'C': Real(0.01, 10, prior='log-uniform'),\n",
    "        'solver': Categorical(['liblinear', 'lbfgs', 'saga'])\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        'C': Real(0.1, 10, prior='log-uniform'),\n",
    "        'gamma': Real(0.01, 1, prior='log-uniform'),\n",
    "        'kernel': Categorical(['linear', 'rbf'])\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        'n_neighbors': Integer(3, 15),\n",
    "        'weights': Categorical(['uniform', 'distance']),\n",
    "        'metric': Categorical(['euclidean', 'manhattan', 'minkowski'])\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        'max_depth': Integer(3, 20),\n",
    "        'min_samples_split': Integer(2, 10)\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        'n_estimators': Integer(50, 500),\n",
    "        'learning_rate': Real(0.01, 1, prior='log-uniform')\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        'n_estimators': Integer(50, 500),\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'max_depth': Integer(3, 15)\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=5000, random_state=42),\n",
    "    \"SVM\": SVC(probability=True, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "failed_models = []\n",
    "\n",
    "def optimize_model(model, param_space, X_train, y_train):\n",
    "    \"\"\"Bayesian optimization of hyperparameters.\"\"\"\n",
    "    opt = BayesSearchCV(\n",
    "        model,\n",
    "        param_space,\n",
    "        n_iter=50,  # Number of optimization steps\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    opt.fit(X_train, y_train)\n",
    "    return opt.best_estimator_, opt.best_score_\n",
    "\n",
    "def plot_learning_curve(model, X_train, y_train, dataset_name, model_name):\n",
    "    \"\"\"Plots learning curves for all models in one graph.\"\"\"\n",
    "    try:\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            model, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1,\n",
    "            train_sizes=np.linspace(0.2, 0.8, 4)\n",
    "        )\n",
    "\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        test_mean = np.mean(test_scores, axis=1)\n",
    "        test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "        plt.plot(train_sizes, test_mean, marker='o', label=f\"{model_name} (Acc: {test_mean[-1]:.2f})\")\n",
    "        plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error generating learning curve for {model_name}: {e}\")\n",
    "        failed_models.append(model_name)\n",
    "\n",
    "def tune_all_models(X_train, y_train, dataset_name):\n",
    "    \"\"\"Runs hyperparameter optimization, saves models, and ranks them.\"\"\"\n",
    "    best_models = {}\n",
    "\n",
    "    plt.figure(figsize=(10, 6))  # Single plot for all learning curves\n",
    "\n",
    "    for model_name, param_space in param_spaces.items():\n",
    "        print(f\"\\nüîç Optimizing {model_name} for {dataset_name}...\")\n",
    "        best_model, best_score = optimize_model(models[model_name], param_space, X_train, y_train)\n",
    "        best_models[model_name] = (best_model, best_score)\n",
    "\n",
    "        print(f\"‚úÖ Best {model_name}: Test Accuracy = {best_score:.4f}\")\n",
    "        joblib.dump(best_model, os.path.join(ML_MODELS_DIR, f'{model_name.replace(\" \", \"_\")}_optimized.pkl'))\n",
    "\n",
    "\n",
    "        # Plot learning curves for all models in the same graph\n",
    "        plot_learning_curve(best_model, X_train, y_train, dataset_name, model_name)\n",
    "\n",
    "    # Rank models\n",
    "    ranked_models = sorted(best_models.items(), key=lambda x: x[1][1], reverse=True)\n",
    "    print(f\"\\nüìä Ranking for {dataset_name}:\")\n",
    "    ranking_df = pd.DataFrame({\n",
    "        \"Model\": [model for model, _ in ranked_models],\n",
    "        \"Test Accuracy\": [metrics[1] for _, metrics in ranked_models]\n",
    "    })\n",
    "    print(ranking_df.to_string(index=False))\n",
    "\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"Learning Curve - Optimized Models ({dataset_name})\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.savefig(f'plots/{dataset_name}_optimized_learning_curves.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return ranked_models\n",
    "\n",
    "# Example Data (Replace with Real `train_test_data`)\n",
    "if \"train_test_data\" not in globals():\n",
    "    from sklearn.datasets import load_iris\n",
    "    iris = load_iris()\n",
    "    X, y = pd.DataFrame(iris.data, columns=iris.feature_names), pd.Series(iris.target)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    train_test_data = {\"iris_dataset\": (X_train, X_test, y_train, y_test)}\n",
    "\n",
    "# Run model tuning\n",
    "for dataset_name, (X_train, X_test, y_train, y_test) in train_test_data.items():\n",
    "    ranked_models = tune_all_models(X_train, y_train, dataset_name)\n",
    "\n",
    "print(\"\\nüöÄ Hyperparameter tuning, ranking, and saving completed!\")\n",
    "print(f\"‚ö†Ô∏è Models that failed: {failed_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a81fe7",
   "metadata": {},
   "source": [
    "## Deep-Learning Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224dfec4",
   "metadata": {},
   "source": [
    "<!-- ## Deep Learning -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Available columns in dataset: {list(df.columns)}\")\n",
    "print(f\"TARGET_COLUMNS: {TARGET_COLUMNS}\")\n",
    "\n",
    "existing_target_columns = [col for col in df.columns if col.strip().lower() in [t.lower() for t in TARGET_COLUMNS]]\n",
    "print(f\"Unique values in target column: {df[existing_target_columns[0]].nunique()}\" if existing_target_columns else \"No target column found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Dictionaries to store encoders\n",
    "feature_encoders = {}  # Stores encoders for feature columns\n",
    "target_encoders = {}   # Stores encoders for target columns\n",
    "\n",
    "USE_ONE_HOT = False\n",
    "\n",
    "# Iterate over the clean dataframes\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    print(f\"\\nüîÑ Encoding categorical features for {df_name}...\")\n",
    "\n",
    "    # Identify categorical columns in the DataFrame\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "    # Split columns between target and features based on TARGET_COLUMNS\n",
    "    target_cols = [col for col in categorical_cols if col in TARGET_COLUMNS]\n",
    "    feature_cols = [col for col in categorical_cols if col not in TARGET_COLUMNS]\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Encode target columns (for classification tasks you may later decide\n",
    "    # to one-hot encode these as needed for your deep learning model)\n",
    "    # ---------------------------------------------\n",
    "    for col in target_cols:\n",
    "        encoder = LabelEncoder()\n",
    "        df[col] = encoder.fit_transform(df[col].astype(str))\n",
    "        target_encoders[f\"{df_name}_{col}\"] = encoder\n",
    "        print(f\"‚úÖ Stored Target Encoder for {df_name} - {col}\")\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Encode feature columns\n",
    "    # For deep learning, integer encoding is generally preferred so that\n",
    "    # embeddings can be used (especially if the cardinality isn‚Äôt extremely low).\n",
    "    # ---------------------------------------------\n",
    "    for col in feature_cols:\n",
    "        encoder = LabelEncoder()\n",
    "        df[col] = encoder.fit_transform(df[col].astype(str))\n",
    "        feature_encoders[f\"{df_name}_{col}\"] = encoder\n",
    "        print(f\"‚úÖ Stored Feature Encoder for {df_name} - {col}\")\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Optionally apply One-Hot Encoding to features if desired.\n",
    "    # For deep learning models using embeddings, you would typically keep the integer encoding.\n",
    "    # ---------------------------------------------\n",
    "    if USE_ONE_HOT:\n",
    "        one_hot_cols = feature_cols  # you can modify this list if you want one-hot on a subset\n",
    "        if one_hot_cols:\n",
    "            encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "            encoded_values = encoder.fit_transform(df[one_hot_cols])\n",
    "            encoded_df = pd.DataFrame(\n",
    "                encoded_values,\n",
    "                index=df.index,\n",
    "                columns=encoder.get_feature_names_out(one_hot_cols)\n",
    "            )\n",
    "            # Drop the original one-hot columns and add the encoded columns\n",
    "            df.drop(columns=one_hot_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "            print(f\"‚úÖ Applied One-Hot Encoding for columns: {one_hot_cols}\")\n",
    "\n",
    "    # Save the updated DataFrame back\n",
    "    final_cleaned_dataframes[df_name] = df\n",
    "    print(f\"‚úÖ Successfully encoded categorical features for {df_name}. New shape: {df.shape}\")\n",
    "\n",
    "print(\"üöÄ Final categorical encoding applied successfully across all datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e48475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Iterate over all cleaned dataframes and target columns\n",
    "for df_name, df in final_cleaned_dataframes.items():\n",
    "    print(f\"\\nüîÑ Processing dataframe: {df_name}\")\n",
    "\n",
    "    # Ensure TARGET_COLUMNS exists in the dataframe\n",
    "    missing_targets = set(TARGET_COLUMNS) - set(df.columns)\n",
    "    if missing_targets:\n",
    "        print(f\"‚ö†Ô∏è Missing target columns in {df_name}: {missing_targets}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    for target_col in TARGET_COLUMNS:\n",
    "        print(f\"üéØ Training model for target column: {target_col}\")\n",
    "\n",
    "        # Prepare features and target\n",
    "        X = df.drop(columns=TARGET_COLUMNS).fillna(0)\n",
    "        y = df[target_col].fillna(0)\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale the features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Build and compile the model\n",
    "        num_features = X_train_scaled.shape[1]\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(128, activation='relu', input_shape=(num_features,)),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        # Ensure labels are within the valid range\n",
    "        y_train = y_train.clip(0, num_classes - 1)\n",
    "        y_test = y_test.clip(0, num_classes - 1)\n",
    "\n",
    "        history = model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "        # Plot Training and Validation Accuracy\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Training Curve for {df_name} - {target_col}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0400675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs('models/deep_learning', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# Ensure train_test_data exists\n",
    "if \"train_test_data\" not in globals():\n",
    "    raise ValueError(\"üö® No train-test data found! Make sure previous models processed data correctly.\")\n",
    "\n",
    "# Load train-test splits from ML models\n",
    "dataset_name = list(train_test_data.keys())[0]  # Use first dataset\n",
    "X_train, X_test, y_train, y_test = train_test_data[dataset_name]\n",
    "\n",
    "# Function to process text features\n",
    "def process_text_features(X_train, X_test):\n",
    "    text_columns = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    print(f\"üìå Text columns detected: {text_columns}\")\n",
    "\n",
    "    if text_columns:\n",
    "        vectorizer = TfidfVectorizer(max_features=500)\n",
    "        X_train_text = pd.DataFrame(vectorizer.fit_transform(X_train[text_columns].fillna(\"\").apply(lambda x: \" \".join(x), axis=1)).toarray(), index=X_train.index)\n",
    "        X_test_text = pd.DataFrame(vectorizer.transform(X_test[text_columns].fillna(\"\").apply(lambda x: \" \".join(x), axis=1)).toarray(), index=X_test.index)\n",
    "    else:\n",
    "        X_train_text = pd.DataFrame(index=X_train.index)\n",
    "        X_test_text = pd.DataFrame(index=X_test.index)\n",
    "\n",
    "    return X_train_text, X_test_text\n",
    "\n",
    "# Function to scale numerical features\n",
    "def preprocess_numerical_features(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train.select_dtypes(exclude=[\"object\"])), columns=X_train.select_dtypes(exclude=[\"object\"]).columns, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test.select_dtypes(exclude=[\"object\"])), columns=X_test.select_dtypes(exclude=[\"object\"]).columns, index=X_test.index)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "# Process features\n",
    "X_train_text, X_test_text = process_text_features(X_train, X_test)\n",
    "X_train_scaled, X_test_scaled = preprocess_numerical_features(X_train, X_test)\n",
    "\n",
    "# Combine text and numerical features\n",
    "X_train_combined = np.hstack([X_train_scaled, X_train_text])\n",
    "X_test_combined = np.hstack([X_test_scaled, X_test_text])\n",
    "\n",
    "print(f\"‚úÖ Final X_train_combined shape: {X_train_combined.shape}\")\n",
    "print(f\"‚úÖ Final X_test_combined shape: {X_test_combined.shape}\")\n",
    "\n",
    "# Define deep learning models\n",
    "deep_models = {\n",
    "    \"DNN\": keras.Sequential([\n",
    "        keras.layers.Dense(128, activation='relu', input_shape=(X_train_combined.shape[1],)),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(len(set(y_train)), activation='softmax')\n",
    "    ]),\n",
    "    \"Wide & Deep\": keras.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=(X_train_combined.shape[1],)),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.4),\n",
    "        keras.layers.Dense(len(set(y_train)), activation='softmax')\n",
    "    ]),\n",
    "    \"TabNet\": keras.Sequential([\n",
    "        keras.layers.Dense(256, activation='relu', input_shape=(X_train_combined.shape[1],)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(len(set(y_train)), activation='softmax')\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Train models and evaluate performance\n",
    "model_results = {}\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for name, model in deep_models.items():\n",
    "    print(f\"\\nüöÄ Training {name}...\")\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train_combined, y_train, epochs=20, batch_size=32, validation_data=(X_test_combined, y_test), verbose=0)\n",
    "\n",
    "    final_val_accuracy = max(history.history['val_accuracy'])\n",
    "    model_results[name] = final_val_accuracy\n",
    "\n",
    "    print(f\"‚úÖ {name}: Best Validation Accuracy = {final_val_accuracy:.4f}\")\n",
    "\n",
    "    # Plot learning curves\n",
    "    plt.plot(history.history['val_accuracy'], marker='o', label=f\"{name} (Acc: {final_val_accuracy:.2f})\")\n",
    "\n",
    "    # Save trained models\n",
    "    model.save(os.path.join(DL_MODELS_DIR, f'{name}_best_model.keras'))\n",
    "best_model.model.save(os.path.join(DL_MODELS_DIR, 'best_model_tuned.keras'))\n",
    "\n",
    "# Ranking models based on validation accuracy\n",
    "ranked_models = sorted(model_results.items(), key=lambda x: x[1], reverse=True)\n",
    "ranking_df = pd.DataFrame(ranked_models, columns=[\"Model\", \"Validation Accuracy\"])\n",
    "print(f\"\\nüìä Model Ranking:\\n{ranking_df.to_string(index=False)}\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Deep Learning Model Comparison\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.savefig(f'plots/deep_learning_learning_curves.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüöÄ Deep Learning Model Training, Ranking, and Saving Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83901c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 14:37:27.466080: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-10 14:37:28.410740: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-06-10 14:37:28.410817: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-06-10 14:37:28.497549: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-10 14:37:30.252880: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-06-10 14:37:30.253162: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-06-10 14:37:30.253182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplots\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load train-test splits from ML models\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mtrain_test_data\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Use first dataset\u001b[39;00m\n\u001b[1;32m     18\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_data[dataset_name]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Process text features\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs('models/deep_learning', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# Load train-test splits from ML models\n",
    "dataset_name = list(train_test_data.keys())[0]  # Use first dataset\n",
    "X_train, X_test, y_train, y_test = train_test_data[dataset_name]\n",
    "\n",
    "# Process text features\n",
    "def process_text_features(X_train, X_test):\n",
    "    text_columns = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    print(f\"üìå Text columns detected: {text_columns}\")\n",
    "\n",
    "    if text_columns:\n",
    "        vectorizer = TfidfVectorizer(max_features=500)\n",
    "        X_train_text = pd.DataFrame(\n",
    "            vectorizer.fit_transform(\n",
    "                X_train[text_columns].fillna(\"\").apply(lambda x: \" \".join(x), axis=1)\n",
    "            ).toarray(),\n",
    "            index=X_train.index\n",
    "        )\n",
    "        X_test_text = pd.DataFrame(\n",
    "            vectorizer.transform(\n",
    "                X_test[text_columns].fillna(\"\").apply(lambda x: \" \".join(x), axis=1)\n",
    "            ).toarray(),\n",
    "            index=X_test.index\n",
    "        )\n",
    "    else:\n",
    "        X_train_text = pd.DataFrame(index=X_train.index)\n",
    "        X_test_text = pd.DataFrame(index=X_test.index)\n",
    "\n",
    "    return X_train_text, X_test_text\n",
    "\n",
    "# Scale numerical features\n",
    "def preprocess_numerical_features(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train.select_dtypes(exclude=[\"object\"])),\n",
    "        columns=X_train.select_dtypes(exclude=[\"object\"]).columns,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_test.select_dtypes(exclude=[\"object\"])),\n",
    "        columns=X_test.select_dtypes(exclude=[\"object\"]).columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "# Process categorical features\n",
    "def process_categorical_features(X_train, X_test):\n",
    "    categorical_columns = X_train.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\n",
    "    print(f\"üìå Categorical columns detected: {categorical_columns}\")\n",
    "\n",
    "    if categorical_columns:\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        X_train_encoded = pd.DataFrame(\n",
    "            encoder.fit_transform(X_train[categorical_columns]),\n",
    "            columns=encoder.get_feature_names_out(categorical_columns),\n",
    "            index=X_train.index\n",
    "        )\n",
    "        X_test_encoded = pd.DataFrame(\n",
    "            encoder.transform(X_test[categorical_columns]),\n",
    "            columns=encoder.get_feature_names_out(categorical_columns),\n",
    "            index=X_test.index\n",
    "        )\n",
    "    else:\n",
    "        X_train_encoded = pd.DataFrame(index=X_train.index)\n",
    "        X_test_encoded = pd.DataFrame(index=X_test.index)\n",
    "\n",
    "    return X_train_encoded, X_test_encoded\n",
    "\n",
    "# Prepare data\n",
    "X_train_text, X_test_text = process_text_features(X_train, X_test)\n",
    "X_train_scaled, X_test_scaled = preprocess_numerical_features(X_train, X_test)\n",
    "X_train_cat, X_test_cat = process_categorical_features(X_train, X_test)\n",
    "\n",
    "# Combine all features\n",
    "X_train_combined = pd.concat([X_train_scaled, X_train_cat, X_train_text], axis=1)\n",
    "X_test_combined = pd.concat([X_test_scaled, X_test_cat, X_test_text], axis=1)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_space = {\n",
    "    'units_1': Integer(32, 256),\n",
    "    'units_2': Integer(16, 128),\n",
    "    'dropout_1': Real(0.1, 0.5),\n",
    "    'dropout_2': Real(0.1, 0.5),\n",
    "    'learning_rate': Real(1e-4, 1e-2, prior='log-uniform'),\n",
    "    'batch_size': Integer(16, 128)\n",
    "}\n",
    "\n",
    "# Function to build model dynamically\n",
    "def build_model(units_1, units_2, dropout_1, dropout_2, learning_rate):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(units_1, activation='relu', input_shape=(X_train_combined.shape[1],)),\n",
    "        keras.layers.Dropout(dropout_1),\n",
    "        keras.layers.Dense(units_2, activation='relu'),\n",
    "        keras.layers.Dropout(dropout_2),\n",
    "        keras.layers.Dense(len(set(y_train)), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to optimize hyperparameters\n",
    "def tune_hyperparameters():\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model, epochs=20, verbose=0),\n",
    "        search_spaces=param_space,\n",
    "        n_iter=50,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    opt.fit(X_train_combined, y_train)\n",
    "    return opt.best_estimator_, opt.best_params_\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "best_model, best_params = tune_hyperparameters()\n",
    "\n",
    "print(f\"\\n‚úÖ Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Train final model\n",
    "history = best_model.fit(X_train_combined, y_train, epochs=50, batch_size=best_params['batch_size'], validation_data=(X_test_combined, y_test))\n",
    "\n",
    "# Plot Learning Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Deep Learning Training Curve - {dataset_name}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(f'plots/deep_learning_tuned_learning_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Save best model\n",
    "DL_MODELS_DIR = 'models/deep_learning'\n",
    "best_model.model.save(os.path.join(DL_MODELS_DIR, 'best_model_tuned.keras'))\n",
    "\n",
    "print(\"\\nüöÄ Deep Learning Model Training & Hyperparameter Tuning Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4212144",
   "metadata": {},
   "source": [
    "## Testing on maquettes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f783b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_excel_files(directory):\n",
    "    \"\"\"Loads all Excel files from the specified directory and displays their heads.\"\"\"\n",
    "    new_dataframes = {}\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".xlsx\") or file.endswith(\".xls\"):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df_name = os.path.splitext(file)[0]  # Use filename as key\n",
    "            new_dataframes[df_name] = pd.read_excel(file_path)\n",
    "            print(f\"\\n‚úÖ Loaded {file}\")\n",
    "            print(new_dataframes[df_name].head())  # üëÄ Show first few rows\n",
    "\n",
    "    return new_dataframes\n",
    "\n",
    "new_dataframes = load_excel_files(os.path.join(TESTING_DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baedc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load all ML models from ML_MODELS_DIR\n",
    "ml_models = {}\n",
    "for file in os.listdir(ML_MODELS_DIR):\n",
    "    if file.endswith(\".pkl\"):\n",
    "        model_name = os.path.splitext(file)[0]\n",
    "        model_path = os.path.join(ML_MODELS_DIR, file)\n",
    "        ml_models[model_name] = joblib.load(model_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded ML models: {list(ml_models.keys())}\")\n",
    "\n",
    "# Load all DL models from DL_MODELS_DIR\n",
    "dl_models = {}\n",
    "for file in os.listdir(DL_MODELS_DIR):\n",
    "    if file.endswith(\".keras\"):\n",
    "        model_name = os.path.splitext(file)[0]\n",
    "        model_path = os.path.join(DL_MODELS_DIR, file)\n",
    "        dl_models[model_name] = keras.models.load_model(model_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded DL models: {list(dl_models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf09dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in new_dataframes.items():\n",
    "\tprint(f\"Available columns in {df_name}:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfe7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_target_columns(df):\n",
    "    \"\"\"Automatically detects target columns based on keyword pairs.\"\"\"\n",
    "    target_patterns = [(\"011EC\", \"Lot\"), (\"012EC\", \"Ouvrage\"), (\"013EC\", \"Localisation\"), (\"014EC\", \"Mode Constructif\")]\n",
    "\n",
    "    detected_targets = [col for col in df.columns if any(k1 in col and k2 in col for k1, k2 in target_patterns)]\n",
    "\n",
    "    print(f\"‚úÖ Detected Target Columns: {detected_targets}\")\n",
    "    return detected_targets\n",
    "\n",
    "for df_name, df in new_dataframes.items():\n",
    "    print(f\"\\nüîç Processing DataFrame: {df_name}\")\n",
    "    detected_targets = detect_target_columns(df)\n",
    "    print(f\"üéØ Detected Target Columns in {df_name}: {detected_targets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd66eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def isolate_and_verify_target_columns(new_dataframes, target_columns):\n",
    "    \"\"\"Removes target columns and verifies successful isolation.\"\"\"\n",
    "    processed_dataframes = {}\n",
    "\n",
    "    for df_name, df in new_dataframes.items():\n",
    "        existing_targets = [col for col in target_columns if col in df.columns]\n",
    "\n",
    "        if not existing_targets:\n",
    "            print(f\"‚ö†Ô∏è No target columns found in {df_name}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüîç Isolating targets in {df_name}: {existing_targets}\")\n",
    "\n",
    "        # Save actual target values separately\n",
    "        actual_values = df[existing_targets].copy()\n",
    "\n",
    "        # Drop target columns from dataset\n",
    "        df_cleaned = df.drop(columns=existing_targets)\n",
    "\n",
    "        # Verify removal\n",
    "        assert all(col not in df_cleaned.columns for col in existing_targets), \"‚ùå Target columns were not properly removed!\"\n",
    "\n",
    "        print(f\"‚úÖ Target columns successfully removed from {df_name}\")\n",
    "        print(f\"üõ† Cleaned DataFrame Head for {df_name}:\\n\", df_cleaned.head())\n",
    "\n",
    "        processed_dataframes[df_name] = (df_cleaned, actual_values)\n",
    "\n",
    "    return processed_dataframes\n",
    "\n",
    "# Example usage:\n",
    "processed_dataframes = isolate_and_verify_target_columns(new_dataframes, detected_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54fe5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def align_features(X_test, trained_model):\n",
    "#     \"\"\"Ensures the test dataset has the same features as the trained model.\"\"\"\n",
    "#     expected_features = trained_model.feature_names_in_\n",
    "\n",
    "#     # Remove unexpected columns\n",
    "#     X_test_aligned = X_test[expected_features].copy()\n",
    "\n",
    "#     print(f\"‚úÖ Aligned dataset shape: {X_test_aligned.shape}\")\n",
    "#     return X_test_aligned\n",
    "\n",
    "# # Example usage:\n",
    "# # Ensure the file exists before loading\n",
    "# rf_model_path = os.path.join(ML_MODELS_DIR, \"Random_Forest_optimized.pkl\")\n",
    "# if not os.path.exists(rf_model_path):\n",
    "#     raise FileNotFoundError(f\"File not found: {rf_model_path}. Please ensure the model file exists in the specified directory.\")\n",
    "\n",
    "# rf_model = joblib.load(rf_model_path)\n",
    "\n",
    "# # Define or load X_test (example: using a DataFrame from the `cleaned_dataframes` dictionary)\n",
    "# X_test = cleaned_dataframes['maquette_23015.xlsx_Sols']  # Replace with the appropriate key\n",
    "\n",
    "# X_test_aligned = align_features(X_test, rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a85a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## showing the head of the dataframes we gonna use for predictions\n",
    "for df_name, (df_cleaned, actual_values) in processed_dataframes.items():\n",
    "    print(f\"\\nüõ† Cleaned DataFrame Head for {df_name}:\\n\", df_cleaned.head())\n",
    "    print(f\"üéØ Actual Values Head for {df_name}:\\n\", actual_values.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d612d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "## the model cant see the target columns, so we need to align the features\n",
    "def align_features(X_test, trained_model):\n",
    "    \"\"\"Ensures the test dataset has the same features as the trained model.\"\"\"\n",
    "    expected_features = trained_model.feature_names_in_\n",
    "\n",
    "    # Remove unexpected columns\n",
    "    X_test_aligned = X_test[expected_features].copy()\n",
    "\n",
    "    print(f\"‚úÖ Aligned dataset shape: {X_test_aligned.shape}\")\n",
    "    return X_test_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def convert_predictions_to_text(numeric_preds, target_map):\n",
    "    \"\"\"\n",
    "    Convert numeric predictions to text using target_map.\n",
    "    If numeric_preds is 1D, convert each element.\n",
    "    If numeric_preds is 2D, convert each element and return an array.\n",
    "    \"\"\"\n",
    "    if numeric_preds.ndim == 1:\n",
    "        return np.array([target_map.get(pred, str(pred)) for pred in numeric_preds])\n",
    "    else:\n",
    "        # Use np.vectorize to apply the conversion elementwise.\n",
    "        return np.vectorize(lambda x: target_map.get(x, str(x)))(numeric_preds)\n",
    "\n",
    "def predict_and_export(mapped_dataframes, ml_models, target_patterns, target_map, output_dir):\n",
    "    \"\"\"\n",
    "    Performs predictions on each Excel-derived DataFrame in mapped_dataframes.\n",
    "    - Excludes columns that match any of the target pattern tuples.\n",
    "    - Aligns features as expected by the model (adding missing ones with value 0).\n",
    "    - Converts numeric predictions to text using target_map.\n",
    "    - Exports the resulting DataFrame (with predictions appended) to Excel.\n",
    "\n",
    "    The export file is saved in output_dir with the modified file name:\n",
    "    \"{original_dataframe_name}_{model_name}.xlsx\"\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    for df_name, X_test in mapped_dataframes.items():\n",
    "        print(f\"\\nüöÄ Predicting targets for {df_name}...\")\n",
    "\n",
    "        # Exclude any column whose name contains both parts of any target pattern.\n",
    "        # The comparison uses lower-case to be case-insensitive.\n",
    "        excluded_columns = [\n",
    "            col for col in X_test.columns\n",
    "            if any(k1.lower() in col.lower() and k2.lower() in col.lower() for k1, k2 in target_patterns)\n",
    "        ]\n",
    "        X_test_cleaned = X_test.drop(columns=excluded_columns, errors='ignore')\n",
    "        print(f\"‚úÖ Excluded columns from prediction: {excluded_columns}\")\n",
    "\n",
    "        # For each model, perform prediction and export the results.\n",
    "        for model_name, model in ml_models.items():\n",
    "            # Work on a copy so that adding missing columns doesn't persist for later models.\n",
    "            X_aligned = X_test_cleaned.copy()\n",
    "\n",
    "            # Add any missing features the model expects (with a default value of 0)\n",
    "            missing_features = set(model.feature_names_in_) - set(X_aligned.columns)\n",
    "            for feature in missing_features:\n",
    "                X_aligned[feature] = 0\n",
    "\n",
    "            # Ensure the feature order matches what the model expects.\n",
    "            X_aligned = X_aligned.loc[:, model.feature_names_in_]\n",
    "\n",
    "            try:\n",
    "                # Make the predictions (assuming the model outputs numeric labels)\n",
    "                numeric_preds = model.predict(X_aligned)\n",
    "\n",
    "                # Convert numeric predictions into text using the provided mapping.\n",
    "                text_preds = convert_predictions_to_text(numeric_preds, target_map)\n",
    "\n",
    "                # Prepare a DataFrame of predictions.\n",
    "                # If predictions are multidimensional, assume each column corresponds to one target.\n",
    "                if text_preds.ndim > 1:\n",
    "                    # Create column names based on your target patterns.\n",
    "                    pred_columns = [f\"{k1}_{k2}\" for k1, k2 in target_patterns]\n",
    "                    preds_df = pd.DataFrame(text_preds, columns=pred_columns)\n",
    "                else:\n",
    "                    preds_df = pd.DataFrame(text_preds, columns=[\"Prediction\"])\n",
    "\n",
    "                # Combine the original test DataFrame (for context) with predictions.\n",
    "                export_df = X_test.copy()  # use the original DataFrame with all columns\n",
    "                # Append predictions as new columns.\n",
    "                export_df = pd.concat([export_df, preds_df], axis=1)\n",
    "\n",
    "                # Set the output file name.\n",
    "                # For example: if df_name is \"maquette_23015\" and model_name is \"Random_Forest_optimized\",\n",
    "                # the file name becomes \"maquette_23015_Random_Forest_optimized.xlsx\"\n",
    "                output_file = os.path.join(PREDICTED_DATA_DIR, f\"{df_name}_{model_name}.xlsx\")\n",
    "                export_df.to_excel(output_file, index=False)\n",
    "                print(f\"‚úÖ Exported predictions for {df_name} using {model_name} to {output_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error predicting with {model_name} on {df_name}: {e}\")\n",
    "\n",
    "\n",
    "target_patterns = [\n",
    "    (\"011EC\", \"Lot\"),\n",
    "    (\"012EC\", \"Ouvrage\"),\n",
    "    (\"013EC\", \"Localisation\"),\n",
    "    (\"014EC\", \"Mode Constructif\")\n",
    "]\n",
    "\n",
    "# For instance, if your model returns 0 for \"Type A\" and 1 for \"Type B\":\n",
    "target_map = {0: \"Type A\", 1: \"Type B\"}\n",
    "\n",
    "# Directory for exporting prediction files:\n",
    "predicting_data_dir = \"predicting_data_dir\"\n",
    "\n",
    "# Now call the function.\n",
    "# (Ensure that `mapped_dataframes` and `ml_models` are defined in your environment.)\n",
    "predictions = predict_and_export(mapped_dataframes, ml_models, target_patterns, target_map, predicting_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b87c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import joblib\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# def predict_with_models(processed_dataframes, ml_model_paths, deep_model_paths):\n",
    "#     \"\"\"Uses ML & Deep Learning models to predict target values.\"\"\"\n",
    "#     predictions = {}\n",
    "\n",
    "#     for df_name, (X_test, actual_values) in processed_dataframes.items():\n",
    "#         print(f\"\\nüöÄ Predicting targets for {df_name}...\")\n",
    "\n",
    "#         # Load trained ML models and make predictions\n",
    "#         ml_preds = {}\n",
    "#         for model_name, model_path in ml_model_paths.items():\n",
    "#             model = joblib.load(model_path)\n",
    "#             ml_preds[model_name] = model.predict(X_test)\n",
    "\n",
    "#         # Load trained deep learning models and make predictions\n",
    "#         deep_preds = {}\n",
    "#         for model_name, model_path in deep_model_paths.items():\n",
    "#             deep_model = keras.models.load_model(model_path)\n",
    "#             deep_preds[model_name] = np.argmax(deep_model.predict(X_test), axis=1)\n",
    "\n",
    "#         predictions[df_name] = {\n",
    "#             \"ML\": ml_preds,\n",
    "#             \"Deep Learning\": deep_preds,\n",
    "#             \"Actual\": actual_values\n",
    "#         }\n",
    "\n",
    "#     return predictions\n",
    "\n",
    "# # Example usage:\n",
    "# ml_model_paths = {\n",
    "#     \"Random Forest\": \"models/machine_learning/Random_Forest_optimized.pkl\",\n",
    "#     \"Gradient Boosting\": \"models/machine_learning/Gradient_Boosting_optimized.pkl\"\n",
    "# }\n",
    "\n",
    "# deep_model_paths = {\n",
    "#     \"Best Deep Model\": \"models/deep_learning/best_model_tuned.keras\"\n",
    "# }\n",
    "\n",
    "# predictions = predict_with_models(processed_dataframes, ml_model_paths, deep_model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680417a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predictions Structure: {predictions.keys()}\")\n",
    "\n",
    "for df_name, results in predictions.items():\n",
    "    print(f\"\\nüîé Checking {df_name}: {results.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions, new_dataframes, detected_targets):\n",
    "    \"\"\"Compares predicted vs. actual target values and calculates accuracy.\"\"\"\n",
    "    evaluation_results = {}\n",
    "\n",
    "    for df_name, results in predictions.items():\n",
    "        print(f\"\\nüìä Evaluating predictions for {df_name}...\")\n",
    "\n",
    "        # Retrieve the actual values from the original dataframes\n",
    "        actual_values = new_dataframes[df_name][detected_targets].copy()\n",
    "\n",
    "        eval_metrics = {}\n",
    "\n",
    "        # Evaluate ML models\n",
    "        for model_name, y_pred in results[\"ML\"].items():\n",
    "            accuracy = accuracy_score(actual_values.values.flatten(), y_pred)\n",
    "            print(f\"‚úÖ {model_name} Accuracy: {accuracy:.4f}\")\n",
    "            eval_metrics[model_name] = accuracy\n",
    "\n",
    "        # Evaluate Deep Learning models\n",
    "        for model_name, y_pred in results[\"Deep Learning\"].items():\n",
    "            accuracy = accuracy_score(actual_values.values.flatten(), y_pred)\n",
    "            print(f\"‚úÖ {model_name} Accuracy: {accuracy:.4f}\")\n",
    "            eval_metrics[model_name] = accuracy\n",
    "\n",
    "        evaluation_results[df_name] = eval_metrics\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_predictions(predictions, new_dataframes, detected_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ca3f4",
   "metadata": {},
   "source": [
    "<!-- üîπ BERT or GPT ‚Üí If text has complex relationships\n",
    "üîπ LSTM or GRU ‚Üí If text has sequential meaning\n",
    "üîπ CNN for NLP ‚Üí If local word patterns matte\n",
    "\n",
    "\n",
    "üöÄ Machine Learning Models\n",
    "If your data is structured (numerical/tabular), traditional ML methods may work well: ‚úÖ Decision Trees & Random Forest ‚Üí Good for structured data, feature importance analysis\n",
    "‚úÖ Gradient Boosting (XGBoost, LightGBM, CatBoost) ‚Üí Powerful for tabular data with boosting techniques\n",
    "‚úÖ Support Vector Machines (SVM) ‚Üí Great for classification problems\n",
    "‚úÖ K-Nearest Neighbors (KNN) ‚Üí Simple but useful for certain cases\n",
    "‚úÖ Logistic Regression ‚Üí Best for binary classification\n",
    "üî• Deep Learning Architectures\n",
    "If you have images, text, or highly complex patterns, DL might be a better choice: ‚úÖ Convolutional Neural Networks (CNNs) ‚Üí Best for image processing\n",
    "‚úÖ Recurrent Neural Networks (RNNs) & LSTMs ‚Üí Designed for sequential data (like time series or language models)\n",
    "‚úÖ Transformers (BERT, GPT) ‚Üí Cutting-edge for NLP and deep sequence understanding\n",
    "‚úÖ Autoencoders & GANs ‚Üí Used for generative tasks or anomaly detection\n",
    " -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BImpredict2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
