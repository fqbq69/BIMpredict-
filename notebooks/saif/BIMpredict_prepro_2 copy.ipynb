{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1fae56",
   "metadata": {},
   "source": [
    "# Bim_Predict NoteBook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08c1ec",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227613f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: ../../data/raw_data\n",
      "Directory already exists: ../../data/processed_data\n",
      "Directory already exists: ../../data/predicting_data\n",
      "Directory already exists: ../../models\n",
      "Directory already exists: ../../models/SK/machine_learning\n",
      "Directory already exists: ../../models/SK/deep_learning\n",
      "Directory already exists: ../../models/SK/other\n",
      "Directory already exists: ../../python_modules\n",
      "Directory already exists: ../../plots\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define project folder paths\n",
    "# Data directories\n",
    "BASE_DIR = \"../../\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "PREDICTED_DATA_DIR = os.path.join(DATA_DIR, \"predicting_data\")\n",
    "TESTING_DATA_DIR = os.path.join(DATA_DIR, \"testing_data\")\n",
    "\n",
    "# Model directories\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ML_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/machine_learning\")\n",
    "DL_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/deep_learning\")\n",
    "OTHER_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/other\")\n",
    "\n",
    "# Python modules and plots directories\n",
    "PYTHON_MODULES_DIR = os.path.join(BASE_DIR, \"python_modules\")\n",
    "PLOTS_DIR = os.path.join(BASE_DIR, \"plots\")\n",
    "\n",
    "# List of directories to create\n",
    "directories = [\n",
    "    RAW_DATA_DIR, PROCESSED_DATA_DIR, PREDICTED_DATA_DIR,\n",
    "    MODELS_DIR, ML_MODELS_DIR, DL_MODELS_DIR, OTHER_MODELS_DIR,\n",
    "    PYTHON_MODULES_DIR, PLOTS_DIR\n",
    "]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e384cf",
   "metadata": {},
   "source": [
    "<!-- ### Paths Creating && Data Importing -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4763e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../data/raw_data/maquette_23017.xlsx\n",
      "Loading: ../../data/raw_data/maquette_23016.xlsx\n",
      "Loading: ../../data/raw_data/maquette_23002.xlsx\n",
      "Loading: ../../data/raw_data/maquette_23007.xlsx\n",
      "Loading: ../../data/raw_data/RawData-Cibles.xlsx\n",
      "Loading: ../../data/raw_data/maquette_23001.xlsx\n",
      "\n",
      "Total files processed: 23\n",
      "Loaded DataFrame: maquette_23017.xlsx_Murs, Shape: (215, 149)\n",
      "Loaded DataFrame: maquette_23017.xlsx_Sols, Shape: (29, 140)\n",
      "Loaded DataFrame: maquette_23017.xlsx_Poutres, Shape: (152, 136)\n",
      "Loaded DataFrame: maquette_23017.xlsx_Poteaux, Shape: (72, 111)\n",
      "Loaded DataFrame: maquette_23016.xlsx_Murs, Shape: (1589, 146)\n",
      "Loaded DataFrame: maquette_23016.xlsx_Sols, Shape: (45, 142)\n",
      "Loaded DataFrame: maquette_23016.xlsx_Poutres, Shape: (778, 136)\n",
      "Loaded DataFrame: maquette_23016.xlsx_Poteaux, Shape: (215, 110)\n",
      "Loaded DataFrame: maquette_23002.xlsx_Murs, Shape: (345, 94)\n",
      "Loaded DataFrame: maquette_23002.xlsx_Sols, Shape: (32, 91)\n",
      "Loaded DataFrame: maquette_23002.xlsx_Poutres, Shape: (96, 89)\n",
      "Loaded DataFrame: maquette_23007.xlsx_Murs, Shape: (203, 91)\n",
      "Loaded DataFrame: maquette_23007.xlsx_Sols, Shape: (41, 82)\n",
      "Loaded DataFrame: maquette_23007.xlsx_Poutres, Shape: (287, 91)\n",
      "Loaded DataFrame: maquette_23007.xlsx_Poteaux, Shape: (115, 83)\n",
      "Loaded DataFrame: RawData-Cibles.xlsx_Murs, Shape: (312, 96)\n",
      "Loaded DataFrame: RawData-Cibles.xlsx_Sols, Shape: (107, 94)\n",
      "Loaded DataFrame: RawData-Cibles.xlsx_Poutres, Shape: (246, 100)\n",
      "Loaded DataFrame: RawData-Cibles.xlsx_Poteaux, Shape: (68, 87)\n",
      "Loaded DataFrame: maquette_23001.xlsx_Murs, Shape: (312, 96)\n",
      "Loaded DataFrame: maquette_23001.xlsx_Sols, Shape: (107, 94)\n",
      "Loaded DataFrame: maquette_23001.xlsx_Poutres, Shape: (246, 100)\n",
      "Loaded DataFrame: maquette_23001.xlsx_Poteaux, Shape: (68, 87)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# List all Excel files in RAW_DATA_DIR\n",
    "excel_files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith(\".xlsx\") or f.endswith(\".xls\")]\n",
    "\n",
    "# Dictionary to store DataFrames for each file and sheet\n",
    "dataframes = {}\n",
    "\n",
    "# Process each Excel file\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(RAW_DATA_DIR, file)\n",
    "    print(f\"Loading: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # Load Excel file\n",
    "        excel_data = pd.ExcelFile(file_path)\n",
    "\n",
    "        # Load all sheets dynamically\n",
    "        for sheet_name in excel_data.sheet_names:\n",
    "            df = excel_data.parse(sheet_name)\n",
    "\n",
    "            # Save DataFrame with a unique identifier\n",
    "            dataframes[f\"{file}_{sheet_name}\"] = df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# Display summary of loaded data\n",
    "print(f\"\\nTotal files processed: {len(dataframes)}\")\n",
    "for key, df in dataframes.items():\n",
    "    print(f\"Loaded DataFrame: {key}, Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1514a",
   "metadata": {},
   "source": [
    "<!-- ### Data Cleaning && PreProcessing -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc959f",
   "metadata": {},
   "source": [
    "## PreProcessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b36ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define required columns dynamically\n",
    "# required_columns = {\n",
    "#     \"Murs\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"Hauteur\",\n",
    "#              \"Epaisseur\", \"AI\", \"AS\", \"Sols en intersection\", \"Sols coupés (u)\", \"Sols coupés (Ids)\",\n",
    "#              \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Sol au-dessus\", \"Sol au-dessous\", \"Fenêtres\", \"Portes\",\n",
    "#              \"Ouvertures\", \"Murs imbriqués\", \"Mur multicouche\", \"Mur empilé\", \"Profil modifié\", \"Extension inférieure\",\n",
    "#              \"Extension supérieure\", \"Partie inférieure attachée\", \"Partie supérieure attachée\", \"Décalage supérieur\",\n",
    "#              \"Décalage inférieur\", \"Matériau structurel\"],\n",
    "\n",
    "#     \"Sols\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"Murs en intersection\",\n",
    "#              \"Murs coupés (u)\", \"Murs coupés (Ids)\", \"Murs coupants (u)\", \"Murs coupants (Ids)\", \"Poutres en intersection\",\n",
    "#              \"Poutres coupés (u)\", \"Poutres coupés (Ids)\", \"Poutres coupants (u)\", \"Poutres coupants (Ids)\",\n",
    "#              \"Poteaux en intersection\", \"Poteaux coupés (u)\", \"Poteaux coupés (Ids)\", \"Poteaux coupants (u)\",\n",
    "#              \"Poteaux coupants (Ids)\", \"Ouvertures\", \"Sol multicouche\", \"Profil modifié\", \"Décalage par rapport au niveau\",\n",
    "#              \"Epaisseur\", \"Lié au volume\", \"Etude de l'élévation à la base\", \"Etude de l'élévation en haut\",\n",
    "#              \"Epaisseur du porteur\", \"Elévation au niveau du noyau inférieur\", \"Elévation au niveau du noyau supérieur\",\n",
    "#              \"Elévation en haut\", \"Elévation à la base\", \"Matériau structurel\"],\n",
    "\n",
    "#     \"Poutres\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"AI\", \"AS\",\n",
    "#                 \"Hauteur totale\", \"Hauteur\", \"Sols en intersection\", \"Sols coupés (u)\", \"Sols coupés (Ids)\",\n",
    "#                 \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Sol au-dessus\", \"Sol au-dessous\", \"Poteaux en intersection\",\n",
    "#                 \"Poteaux coupés (u)\", \"Poteaux coupés (Ids)\", \"Poteaux coupants (u)\", \"Poteaux coupants (Ids)\",\n",
    "#                 \"Etat de la jonction\", \"Valeur de décalage Z\", \"Justification Z\", \"Valeur de décalage Y\", \"Justification Y\",\n",
    "#                 \"Justification YZ\", \"Matériau structurel\", \"Elévation du niveau de référence\", \"Elévation en haut\",\n",
    "#                 \"Rotation de la section\", \"Orientation\", \"Décalage du niveau d'arrivée\", \"Décalage du niveau de départ\",\n",
    "#                 \"Elévation à la base\", \"Longueur de coupe\", \"Longueur\", \"hauteur_section\", \"largeur_section\"],\n",
    "\n",
    "#     \"Poteaux\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"AI\", \"AS\",\n",
    "#                 \"Hauteur\", \"Longueur\", \"Partie inférieure attachée\", \"Partie supérieure attachée\", \"Sols en intersection\",\n",
    "#                 \"Sols coupés (u)\", \"Sols coupés (Ids)\", \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Poutres en intersection\",\n",
    "#                 \"Poutres coupés (u)\", \"Poutres coupés (Ids)\", \"Poutres coupants (u)\", \"Poutres coupants (Ids)\",\n",
    "#                 \"Matériau structurel\", \"Décalage supérieur\", \"Décalage inférieur\", \"Diamètre poteau\", \"h\", \"b\",\n",
    "#                 \"hauteur_section\", \"largeur_section\"]\n",
    "# }\n",
    "\n",
    "# # Initialize a dictionary to store filtered dataframes\n",
    "# cleaned_dataframes = {}\n",
    "\n",
    "# for df_name, df in dataframes.items():\n",
    "#     print(f\"\\n🟢 Original shape of {df_name}: {df.shape}\")\n",
    "\n",
    "#     # Automatically detect the correct category for filtering\n",
    "#     for category, columns in required_columns.items():\n",
    "#         if category.lower() in df_name.lower():  # Match dynamically\n",
    "#             try:\n",
    "#                 filtered_df = df[columns]  # Keep only the required columns\n",
    "#             except KeyError as e:\n",
    "#                 missing_columns = set(columns) - set(df.columns)\n",
    "#                 print(f\"⚠️ Missing columns in {df_name}: {missing_columns}. Skipping this dataframe.\")\n",
    "#                 continue\n",
    "#             cleaned_dataframes[df_name] = filtered_df\n",
    "#             print(f\"✅ Shape after filtering {df_name}: {filtered_df.shape}\")\n",
    "#             break  # Stop looping once the correct match is found\n",
    "#     else:\n",
    "#         print(f\"⚠️ No matching category for {df_name}, skipping filtering.\")\n",
    "\n",
    "# # Add prefixes to column names based on the dataframe category and update index\n",
    "# for name, df in cleaned_dataframes.items():\n",
    "#     if \"murs\" in name.lower():\n",
    "#         prefix = \"murs_\"\n",
    "#     elif \"sols\" in name.lower():\n",
    "#         prefix = \"sols_\"\n",
    "#     elif \"poutres\" in name.lower():\n",
    "#         prefix = \"poutres_\"\n",
    "#     elif \"poteaux\" in name.lower():\n",
    "#         prefix = \"poteaux_\"\n",
    "#     else:\n",
    "#         prefix = \"\"\n",
    "\n",
    "#     # Rename columns with the prefix\n",
    "#     df.rename(columns=lambda col: f\"{prefix}{col}\" if col.lower() != \"id\" else f\"{prefix}id\", inplace=True)\n",
    "\n",
    "#     # Drop the existing index and set the prefixed ID column as the new index\n",
    "#     id_column = f\"{prefix}id\"\n",
    "#     if id_column in df.columns:\n",
    "#         df.set_index(id_column, inplace=True)\n",
    "#         print(f\"✅ Set '{id_column}' as index for {name}.\")\n",
    "#     else:\n",
    "#         print(f\"⚠️ '{id_column}' column not found in {name}, skipping index setting.\")\n",
    "\n",
    "    # Update the cleaned_dataframes dictionary\n",
    "    # cleaned_dataframes[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d5cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n📊 Cleaned DataFrames:\")\n",
    "# for df_name, df in cleaned_dataframes.items():\n",
    "#     print(f\" - {df_name}: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8b2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove duplicates and 100% missing columns (with exceptions)\n",
    "# def process_dataframe(df_name, exception_keywords):\n",
    "#     # Access the dataframe from cleaned_dataframes\n",
    "#     df = cleaned_dataframes[df_name].copy()\n",
    "\n",
    "#     # Remove duplicate rows\n",
    "#     initial_shape = df.shape\n",
    "#     df.drop_duplicates(inplace=True)\n",
    "#     duplicates_removed = initial_shape[0] - df.shape[0]\n",
    "#     if duplicates_removed > 0:\n",
    "#         print(f\"🟢 Removed {duplicates_removed} duplicate rows from {df_name}.\")\n",
    "#     else:\n",
    "#         print(f\"✅ No duplicate rows found in {df_name}.\")\n",
    "\n",
    "#     # Identify fully missing columns\n",
    "#     missing_cols = df.columns[df.isnull().mean() == 1]\n",
    "#     # Keep columns that contain an exception keyword or are in target_columns intact\n",
    "#     cols_to_drop = [\n",
    "#         col for col in missing_cols\n",
    "#         if not any(keyword in col.lower() for keyword in exception_keywords)\n",
    "#     ]\n",
    "\n",
    "#     if cols_to_drop:\n",
    "#         print(f\"🟠 Dropping fully missing columns from {df_name}: {cols_to_drop}\")\n",
    "#         df.drop(columns=cols_to_drop, inplace=True)\n",
    "#     else:\n",
    "#         print(f\"✅ No columns dropped from {df_name}; all fully missing columns are exceptions.\")\n",
    "\n",
    "#     # Fill missing values in columns containing \"coupé\" or \"coupants\"\n",
    "#     columns_to_fill = [col for col in df.columns if \"coupé\" in col.lower() or \"coupants\" in col.lower()]\n",
    "#     if columns_to_fill:\n",
    "#         print(f\"🔵 Filling missing values with 0 for columns in {df_name}: {columns_to_fill}\")\n",
    "#         df[columns_to_fill] = df[columns_to_fill].fillna(0)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # Apply processing on all dataframes in cleaned_dataframes\n",
    "# exception_keywords = [\"coupés\", \"coupants\"]\n",
    "\n",
    "# processed_dataframes = {}\n",
    "# for df_name in cleaned_dataframes.keys():\n",
    "#     print(f\"\\n🔍 Processing dataframe: {df_name}\")\n",
    "#     processed_dataframes[df_name] = process_dataframe(df_name, exception_keywords)\n",
    "#     final_shape = processed_dataframes[df_name].shape\n",
    "#     print(f\"✅ Final shape of {df_name}: {final_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b37a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify target columns dynamically across all DataFrames\n",
    "# TARGET_COLUMNS = ['011EC_Lot', '012EC_Ouvrage', '013EC_Localisation', '014EC_Mode Constructif']\n",
    "\n",
    "# # Check and add missing target columns\n",
    "# for df_name, df in processed_dataframes.items():\n",
    "#     print(f\"\\nProcessing dataframe: {df_name}\")\n",
    "#     initial_shape = df.shape  # Store the initial shape of the dataframe\n",
    "\n",
    "#     for target in TARGET_COLUMNS:\n",
    "#         if target in df.columns:\n",
    "#             print(f\"✅ Target column '{target}' found in dataframe '{df_name}'.\")\n",
    "\n",
    "#             # Check for missing data in the target column\n",
    "#             missing_count = df[target].isnull().sum()\n",
    "#             total_count = len(df)\n",
    "#             missing_percentage = (missing_count / total_count) * 100\n",
    "#             if missing_count > 0:\n",
    "#                 print(f\"⚠️ Target column '{target}' has {missing_count} missing values ({missing_percentage:.2f}%).\")\n",
    "\n",
    "#                 # Drop rows if missing data is less than 10%\n",
    "#                 if missing_percentage < 10:\n",
    "#                     df = df[df[target].notnull()]\n",
    "#                     print(f\"✅ Dropped rows with missing values in '{target}' (less than 10%).\")\n",
    "#             else:\n",
    "#                 print(f\"✅ Target column '{target}' has no missing values.\")\n",
    "#         else:\n",
    "#             print(f\"⚠️ Target column '{target}' does not exist in dataframe '{df_name}'. Adding it...\")\n",
    "#             # Add the missing target column with default values (e.g., NaN)\n",
    "#             df[target] = float('nan')\n",
    "#             print(f\"✅ Added missing target column '{target}' to dataframe '{df_name}'.\")\n",
    "\n",
    "#     final_shape = df.shape  # Store the final shape of the dataframe\n",
    "#     if initial_shape != final_shape:\n",
    "#         print(f\"📊 Shape before: {initial_shape}, Shape after: {final_shape}\")\n",
    "\n",
    "#     # Update the cleaned_dataframes dictionary\n",
    "#     cleaned_dataframes[df_name] = processed_dataframes[df_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb298214",
   "metadata": {},
   "source": [
    "<!-- ### Exploratory Data Analysis (EDA) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9971ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure missing values are filled in the processed datasets unless in TARGET_COLUMNS\n",
    "# for df_name, df in cleaned_dataframes.items():\n",
    "#     print(f\"\\n🟢 Filling missing values for {df_name}...\")\n",
    "\n",
    "#     # Display shape before filling missing values\n",
    "#     initial_shape = df.shape\n",
    "#     print(f\"📌 Initial shape before filling NaN: {initial_shape}\")\n",
    "\n",
    "#     # Fill missing values with 0 for non-target columns\n",
    "#     non_target_columns = [col for col in df.columns if col not in TARGET_COLUMNS]\n",
    "#     df[non_target_columns] = df[non_target_columns].fillna(0)\n",
    "\n",
    "#     # Store updated dataframe back\n",
    "#     cleaned_dataframes[df_name] = df\n",
    "\n",
    "#     # Display shape after processing\n",
    "#     final_shape = df.shape\n",
    "#     print(f\"✅ Final shape after filling NaN: {final_shape}\")\n",
    "\n",
    "# print(\"🚀 Missing values successfully handled across all datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f2a66",
   "metadata": {},
   "source": [
    "## EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d1b70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Function to remove low-variance & highly correlated features\n",
    "# def optimize_feature_selection(df, variance_threshold=0.02, correlation_threshold=0.98):\n",
    "#     print(f\"\\n🔍 Processing {df.shape[0]} rows & {df.shape[1]} columns\")\n",
    "\n",
    "#     # Step 1: Remove Low-Variance Features\n",
    "#     selector = VarianceThreshold(variance_threshold)\n",
    "#     numeric_df = df.select_dtypes(include=[\"number\"])  # Focus only on numerical columns\n",
    "#     selector.fit(numeric_df)\n",
    "\n",
    "#     low_variance_cols = numeric_df.columns[~selector.get_support()]\n",
    "#     keep_cols = [col for col in low_variance_cols if any(keyword in col.lower() for keyword in [\"coupés\", \"coupants\"])]\n",
    "#     drop_cols = [col for col in low_variance_cols if col not in keep_cols and col not in TARGET_COLUMNS]\n",
    "\n",
    "#     df.drop(columns=drop_cols, inplace=True)\n",
    "#     print(f\"⚠️ Dropped {len(drop_cols)} low-variance columns (excluding 'coupés' and target columns): {drop_cols}\")\n",
    "\n",
    "#     # Step 2: Remove Highly Correlated Features\n",
    "#     numeric_df = df.select_dtypes(include=[\"number\"])\n",
    "#     correlation_matrix = numeric_df.corr().abs()\n",
    "#     upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "#     correlated_features = [\n",
    "#         col for col in upper_triangle.columns\n",
    "#         if any(upper_triangle[col] > correlation_threshold) and col not in TARGET_COLUMNS\n",
    "#     ]\n",
    "\n",
    "#     df.drop(columns=correlated_features, inplace=True)\n",
    "#     print(f\"⚠️ Dropped {len(correlated_features)} highly correlated columns (excluding target columns): {correlated_features}\")\n",
    "\n",
    "#     print(f\"✅ Final shape after filtering: {df.shape}\")\n",
    "#     return df\n",
    "\n",
    "# # Apply optimized feature selection to all datasets\n",
    "# final_cleaned_dataframes = {name: optimize_feature_selection(df) for name, df in cleaned_dataframes.items()}\n",
    "\n",
    "# print(\"🚀 Optimized feature selection completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8548dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display basic statistics for all cleaned sheets\n",
    "# for df_name, df in final_cleaned_dataframes.items():\n",
    "#     print(f\"\\nSummary statistics for {df_name}:\")\n",
    "\n",
    "#     print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabfb1e",
   "metadata": {},
   "source": [
    "<!-- ### Feature Selection -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a62fbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify target columns dynamically across all DataFrames\n",
    "# target_columns_found = set()\n",
    "# for df_name, df in final_cleaned_dataframes.items():\n",
    "#     found_targets = [\n",
    "#         col for col in df.columns\n",
    "#         if any(target.lower() in col.lower() for target in TARGET_COLUMNS)\n",
    "#     ]\n",
    "#     target_columns_found.update(found_targets)\n",
    "\n",
    "# print(f\"\\nTarget columns detected across datasets: {target_columns_found}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34f0966",
   "metadata": {},
   "source": [
    "<!-- ## Training and testing  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a81fe7",
   "metadata": {},
   "source": [
    "## Deep-Learning Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f20fb331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 13:37:55.784140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-10 13:37:56.087159: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-06-10 13:37:56.087198: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-06-10 13:37:56.147514: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-10 13:37:58.166644: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-06-10 13:37:58.166838: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-06-10 13:37:58.166854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.preprocessing    import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute           import SimpleImputer\n",
    "from sklearn.compose          import ColumnTransformer\n",
    "from sklearn.pipeline         import Pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cd52536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1. DATA CLEANING ─────────────────────────────────────────────────────────\n",
    "def filter_required_columns(dataframes, required_columns):\n",
    "    \"\"\"\n",
    "    For each df in `dataframes`, pick the matching sheet key and\n",
    "    keep only its required columns. Returns a dict of filtered dfs.\n",
    "    \"\"\"\n",
    "    cleaned = {}\n",
    "    for name, df in dataframes.items():\n",
    "        for sheet, cols in required_columns.items():\n",
    "            if sheet.lower() in name.lower():\n",
    "                missing = set(cols) - set(df.columns)\n",
    "                if missing:\n",
    "                    print(f\"⚠️ Missing in {name}: {missing}. Skipping.\")\n",
    "                else:\n",
    "                    cleaned[name] = df[cols].copy()\n",
    "                    print(f\"✅ {name}: filtered to {df[cols].shape}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"⚠️ No match for {name}, skipped\")\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def drop_duplicates_and_missing(df, exception_keywords):\n",
    "    \"\"\"\n",
    "    - Drop duplicate rows\n",
    "    - Drop any column that’s 100% NaN unless it contains an exception keyword\n",
    "    - Fill any “coupé”/“coupants” column’s NaNs with 0\n",
    "    \"\"\"\n",
    "    before = df.shape[0]\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"🗑️  Dups removed: {before - df.shape[0]}\")\n",
    "\n",
    "    fully_missing = [c for c in df.columns if df[c].isna().all()]\n",
    "    to_drop = [c for c in fully_missing\n",
    "               if not any(kw.lower() in c.lower() for kw in exception_keywords)]\n",
    "    df = df.drop(columns=to_drop)\n",
    "    if to_drop: print(f\"🗑️  Dropped 100% NaN cols: {to_drop}\")\n",
    "\n",
    "    fill_cols = [c for c in df.columns if \"coupé\" in c.lower() or \"coupant\" in c.lower()]\n",
    "    df[fill_cols] = df[fill_cols].fillna(0)\n",
    "    if fill_cols: print(f\"ℹ️  Filled coupé cols with 0: {fill_cols}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def ensure_targets(df, target_columns, drop_threshold=0.10):\n",
    "    \"\"\"\n",
    "    - Ensure each target exists (if not, create it with NaN)\n",
    "    - If existing but < drop_threshold% missing → drop those rows\n",
    "    - Leave > threshold as-is (you may choose to impute later)\n",
    "    \"\"\"\n",
    "    for t in target_columns:\n",
    "        if t not in df.columns:\n",
    "            df[t] = np.nan\n",
    "            print(f\"➕ Added missing target {t}\")\n",
    "        miss_pct = df[t].isna().mean()\n",
    "        if 0 < miss_pct < drop_threshold:\n",
    "            df = df[df[t].notna()]\n",
    "            print(f\"🗑️ Dropped rows where {t} was NaN ({miss_pct:.1%})\")\n",
    "        else:\n",
    "            print(f\"ℹ️  {t}: {miss_pct:.1%} missing\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_non_targets(df, target_columns):\n",
    "    \"\"\"\n",
    "    Fill all non‐target columns’ NaNs with 0\n",
    "    \"\"\"\n",
    "    non_targets = [c for c in df.columns if c not in target_columns]\n",
    "    df[non_targets] = df[non_targets].fillna(0)\n",
    "    print(f\"✅ Filled NaN→0 on non‐targets ({len(non_targets)} cols)\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_all_dataframes(\n",
    "    raw_dfs,\n",
    "    required_columns,\n",
    "    exception_keywords,\n",
    "    target_columns\n",
    "):\n",
    "    # 1) filter\n",
    "    filt = filter_required_columns(raw_dfs, required_columns)\n",
    "\n",
    "    # 2) per‐df cleaning\n",
    "    cleaned = {}\n",
    "    for name, df in filt.items():\n",
    "        print(f\"\\n🔍 Cleaning {name}\")\n",
    "        df2 = drop_duplicates_and_missing(df, exception_keywords)\n",
    "        df3 = ensure_targets(df2, target_columns)\n",
    "        df4 = fill_non_targets(df3,   target_columns)\n",
    "        cleaned[name] = df4\n",
    "        print(f\"🎯 Final {name} shape: {df4.shape}\")\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18579273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2. PREPROCESS & SPLIT ────────────────────────────────────────────────────\n",
    "def build_preprocessor(X_df):\n",
    "    \"\"\"\n",
    "    - numeric: mean‐impute + StandardScaler\n",
    "    - categorical: mode‐impute + OneHotEncoder (ensuring all values are strings)\n",
    "    \"\"\"\n",
    "    num_cols = X_df.select_dtypes([\"int64\", \"float64\"]).columns\n",
    "    cat_cols = X_df.select_dtypes([\"object\", \"category\"]).columns\n",
    "\n",
    "    # Convert categorical columns to string to avoid mixed types\n",
    "    X_df[cat_cols] = X_df[cat_cols].astype(str)\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\")),\n",
    "    ])\n",
    "    transformer = ColumnTransformer([\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols),\n",
    "    ])\n",
    "    return transformer\n",
    "\n",
    "\n",
    "def prepare_dataset(\n",
    "    df,\n",
    "    target_columns,\n",
    "    test_size=0.3,\n",
    "    val_size=0.5,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    - Splits df into X, y\n",
    "    - LabelEncodes each target\n",
    "    - Preprocesses X\n",
    "    - Splits into train/val/test\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=target_columns)\n",
    "    y = df[target_columns].astype(str)\n",
    "    # encode Y\n",
    "    le_dict, y_enc = {}, []\n",
    "    for col in y:\n",
    "        le = LabelEncoder().fit(y[col])\n",
    "        y_enc.append(le.transform(y[col]))\n",
    "        le_dict[col] = le\n",
    "    y_enc = np.vstack(y_enc).T\n",
    "\n",
    "    # preprocess X\n",
    "    pre = build_preprocessor(X)\n",
    "    X_all = pre.fit_transform(X)\n",
    "\n",
    "    # splits\n",
    "    strat = y_enc[:,0]\n",
    "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
    "        X_all, y_enc, test_size=test_size,\n",
    "        stratify=strat, random_state=random_state\n",
    "    )\n",
    "    X_val, X_te, y_val, y_te = train_test_split(\n",
    "        X_tmp, y_tmp, test_size=val_size,\n",
    "        stratify=y_tmp[:,0], random_state=random_state\n",
    "    )\n",
    "    return (X_tr,y_tr), (X_val,y_val), (X_te,y_te), pre, le_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2eba69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 3. MODEL BUILDING & TRAINING ────────────────────────────────────────────\n",
    "\n",
    "def build_mlp(input_dim, output_dims, hp):\n",
    "    inp = layers.Input((input_dim,))\n",
    "    x = inp\n",
    "    for i in range(hp[\"num_layers\"]):\n",
    "        x = layers.Dense(hp[\"units\"], activation=\"relu\")(x)\n",
    "        if hp[\"dropout\"]>0:\n",
    "            x = layers.Dropout(hp[\"dropout\"])(x)\n",
    "\n",
    "    outputs, losses, metrics = [], {}, {}\n",
    "    for i, d in enumerate(output_dims):\n",
    "        name = f\"out{i}\"\n",
    "        o = layers.Dense(d, activation=\"softmax\", name=name)(x)\n",
    "        outputs.append(o)\n",
    "        losses[name]   = \"sparse_categorical_crossentropy\"\n",
    "        metrics[name]  = [\"accuracy\"]\n",
    "\n",
    "    m = Model(inp, outputs)\n",
    "    m.compile(\n",
    "        optimizer=optimizers.Adam(hp[\"lr\"]),\n",
    "        loss=losses,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return m\n",
    "\n",
    "\n",
    "def train_models(\n",
    "    X_tr, y_tr,\n",
    "    X_val, y_val,\n",
    "    output_dims,\n",
    "    hyperparams,\n",
    "    epochs=30,\n",
    "    batch_size=32\n",
    "):\n",
    "    y_tr_dict  = {f\"out{i}\": y_tr[:,i] for i in range(y_tr.shape[1])}\n",
    "    y_val_dict = {f\"out{i}\": y_val[:,i] for i in range(y_val.shape[1])}\n",
    "\n",
    "    models, histories, scores = [], [], []\n",
    "    for hp in hyperparams:\n",
    "        tf.keras.backend.clear_session()\n",
    "        m = build_mlp(X_tr.shape[1], output_dims, hp)\n",
    "        h = m.fit(\n",
    "            X_tr, y_tr_dict,\n",
    "            validation_data=(X_val, y_val_dict),\n",
    "            epochs=epochs, batch_size=batch_size, verbose=0\n",
    "        )\n",
    "        ev = m.evaluate(X_val, y_val_dict, verbose=0)\n",
    "        models.append(m)\n",
    "        histories.append((hp, h))\n",
    "        scores.append((hp, ev))\n",
    "        print(f\"🏷 hp={hp} → val_loss={ev[0]:.4f}\")\n",
    "    return models, histories, scores\n",
    "\n",
    "\n",
    "def plot_learning_curves(histories, title):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    for hp,h in histories:\n",
    "        lbl = f\"{hp['units']}u×{hp['num_layers']}L dr={hp['dropout']} lr={hp['lr']}\"\n",
    "        plt.plot(h.history[\"val_loss\"], label=lbl)\n",
    "    plt.title(title); plt.xlabel(\"epoch\"); plt.ylabel(\"val_loss\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "def summarize_scores(scores):\n",
    "    rows = []\n",
    "    for hp, ev in scores:\n",
    "        row = {**hp, \"total_val_loss\": ev[0]}\n",
    "        idx = 1\n",
    "        head = 0\n",
    "        while idx<len(ev):\n",
    "            row[f\"o{head}_loss\"] = ev[idx]\n",
    "            row[f\"o{head}_acc\"]  = ev[idx+1]\n",
    "            idx+=2; head+=1\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows).sort_values(\"total_val_loss\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def save_top3(models, scores, prefix):\n",
    "    losses = np.array([s[1][0] for s in scores])\n",
    "    best   = np.argsort(losses)[:3]\n",
    "    for rank,i in enumerate(best,1):\n",
    "        fn = f\"{prefix}_top{rank}.h5\"\n",
    "        models[i].save(fn)\n",
    "        print(f\"💾 Saved {fn}\")\n",
    "\n",
    "\n",
    "def test_saved_models(\n",
    "    model_paths, preprocessor, label_encoders,\n",
    "    seen_excel_paths, sheet_key,\n",
    "    required_columns, target_columns\n",
    "):\n",
    "    for mp in model_paths:\n",
    "        print(f\"\\n🔎 Testing {mp}\")\n",
    "        m = tf.keras.models.load_model(mp)\n",
    "        for xp in seen_excel_paths:\n",
    "            df = pd.read_excel(xp, sheet_name=sheet_key)\n",
    "            df = df[required_columns[sheet_key]]\n",
    "            Xn = preprocessor.transform(df.drop(columns=target_columns))\n",
    "            preds = m.predict(Xn)\n",
    "            out = df.copy()\n",
    "            for i,col in enumerate(target_columns):\n",
    "                le = label_encoders[col]\n",
    "                out[f\"pred_{col}\"] = le.inverse_transform(preds[i].argmax(axis=1))\n",
    "            print(f\"— on {os.path.basename(xp)}:\")\n",
    "            print(out.head())\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../data/raw_data/maquette_23017.xlsx\n",
      "Loaded sheet: Murs from maquette_23017.xlsx, Shape: (215, 149)\n",
      "Loaded sheet: Sols from maquette_23017.xlsx, Shape: (29, 140)\n",
      "Loaded sheet: Poutres from maquette_23017.xlsx, Shape: (152, 136)\n",
      "Loaded sheet: Poteaux from maquette_23017.xlsx, Shape: (72, 111)\n",
      "Loading: ../../data/raw_data/maquette_23016.xlsx\n",
      "Loaded sheet: Murs from maquette_23016.xlsx, Shape: (1589, 146)\n",
      "Loaded sheet: Sols from maquette_23016.xlsx, Shape: (45, 142)\n",
      "Loaded sheet: Poutres from maquette_23016.xlsx, Shape: (778, 136)\n",
      "Loaded sheet: Poteaux from maquette_23016.xlsx, Shape: (215, 110)\n",
      "Loading: ../../data/raw_data/maquette_23002.xlsx\n",
      "Loaded sheet: Murs from maquette_23002.xlsx, Shape: (345, 94)\n",
      "Loaded sheet: Sols from maquette_23002.xlsx, Shape: (32, 91)\n",
      "Loaded sheet: Poutres from maquette_23002.xlsx, Shape: (96, 89)\n",
      "Loading: ../../data/raw_data/maquette_23007.xlsx\n",
      "Loaded sheet: Murs from maquette_23007.xlsx, Shape: (203, 91)\n",
      "Loaded sheet: Sols from maquette_23007.xlsx, Shape: (41, 82)\n",
      "Loaded sheet: Poutres from maquette_23007.xlsx, Shape: (287, 91)\n",
      "Loaded sheet: Poteaux from maquette_23007.xlsx, Shape: (115, 83)\n",
      "Loading: ../../data/raw_data/RawData-Cibles.xlsx\n",
      "Loaded sheet: Murs from RawData-Cibles.xlsx, Shape: (312, 96)\n",
      "Loaded sheet: Sols from RawData-Cibles.xlsx, Shape: (107, 94)\n",
      "Loaded sheet: Poutres from RawData-Cibles.xlsx, Shape: (246, 100)\n",
      "Loaded sheet: Poteaux from RawData-Cibles.xlsx, Shape: (68, 87)\n",
      "Loading: ../../data/raw_data/maquette_23001.xlsx\n",
      "Loaded sheet: Murs from maquette_23001.xlsx, Shape: (312, 96)\n",
      "Loaded sheet: Sols from maquette_23001.xlsx, Shape: (107, 94)\n",
      "Loaded sheet: Poutres from maquette_23001.xlsx, Shape: (246, 100)\n",
      "Loaded sheet: Poteaux from maquette_23001.xlsx, Shape: (68, 87)\n",
      "\n",
      "🔍 Cleaning maquette_23017.xlsx_Murs\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Nom de la famille', 'Nom du type', 'Identifiant', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', 'Composition', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', 'IFCExportAs', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (114 cols)\n",
      "🎯 Final maquette_23017.xlsx_Murs shape: (215, 118)\n",
      "\n",
      "🔍 Cleaning maquette_23017.xlsx_Sols\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Identifiant', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '052EC_Jour Coffrage', '053EC_Jour Ferraillage', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', 'Composition', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', 'IFCExportAs', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Fabricant', 'URL', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type']\n",
      "ℹ️  Filled coupé cols with 0: ['Murs coupés (u)', 'Murs coupés (Ids)', 'Murs coupants (u)', 'Murs coupants (Ids)', 'Poutres coupés (u)', 'Poutres coupés (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)', 'Poteaux coupés (u)', 'Poteaux coupés (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (105 cols)\n",
      "🎯 Final maquette_23017.xlsx_Sols shape: (29, 109)\n",
      "\n",
      "🔍 Cleaning maquette_23017.xlsx_Poutres\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Niveau', 'Nom de la famille', 'Nom du type', 'Identifiant', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', 'Composition', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', 'IFCExportAs', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (96 cols)\n",
      "🎯 Final maquette_23017.xlsx_Poutres shape: (152, 100)\n",
      "\n",
      "🔍 Cleaning maquette_23017.xlsx_Poteaux\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', 'Composition', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', 'IFCExportAs', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', 'Titre OmniClass', 'Numéro OmniClass', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (72 cols)\n",
      "🎯 Final maquette_23017.xlsx_Poteaux shape: (72, 76)\n",
      "\n",
      "🔍 Cleaning maquette_23016.xlsx_Murs\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', '001EC_Grue', '003EC_Zone', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue détail faible', 'Marque de type', \"Protection contre l'incendie\"]\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "🗑️ Dropped rows where 011EC_Lot was NaN (0.1%)\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (109 cols)\n",
      "🎯 Final maquette_23016.xlsx_Murs shape: (1587, 113)\n",
      "\n",
      "🔍 Cleaning maquette_23016.xlsx_Sols\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Niveau plancher fini', '001EC_Grue', '003EC_Zone', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '052EC_Jour Coffrage', '053EC_Jour Ferraillage', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue détail faible', 'Marque de type']\n",
      "ℹ️  Filled coupé cols with 0: ['Murs coupés (u)', 'Murs coupés (Ids)', 'Murs coupants (u)', 'Murs coupants (Ids)', 'Poutres coupés (u)', 'Poutres coupés (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)', 'Poteaux coupés (u)', 'Poteaux coupés (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (103 cols)\n",
      "🎯 Final maquette_23016.xlsx_Sols shape: (45, 107)\n",
      "\n",
      "🔍 Cleaning maquette_23016.xlsx_Poutres\n",
      "🗑️  Dups removed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38590/4147864326.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[non_targets] = df[non_targets].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Niveau', 'Nom de la famille', 'Nom du type', '001EC_Grue', '003EC_Zone', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', 'Titre OmniClass', 'Numéro OmniClass', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (95 cols)\n",
      "🎯 Final maquette_23016.xlsx_Poutres shape: (778, 99)\n",
      "\n",
      "🔍 Cleaning maquette_23016.xlsx_Poteaux\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', \"Marque d'emplacement du poteau\", 'Nom de la famille', 'Nom du type', '001EC_Grue', '003EC_Zone', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', 'Titre OmniClass', 'Numéro OmniClass', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 30.7% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (70 cols)\n",
      "🎯 Final maquette_23016.xlsx_Poteaux shape: (215, 74)\n",
      "\n",
      "🔍 Cleaning maquette_23002.xlsx_Murs\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Identifiant', 'Batiment', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue détail faible', 'Marque de type', \"Protection contre l'incendie\"]\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (70 cols)\n",
      "🎯 Final maquette_23002.xlsx_Murs shape: (345, 74)\n",
      "\n",
      "🔍 Cleaning maquette_23002.xlsx_Sols\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Identifiant', 'Batiment', 'Niveau fini', 'Niveau Brut', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue détail faible', 'Marque de type', 'épaisseur']\n",
      "ℹ️  Filled coupé cols with 0: ['Murs coupés (u)', 'Murs coupés (Ids)', 'Murs coupants (u)', 'Murs coupants (Ids)', 'Poutres coupés (u)', 'Poutres coupés (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)', 'Poteaux coupés (u)', 'Poteaux coupés (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (65 cols)\n",
      "🎯 Final maquette_23002.xlsx_Sols shape: (32, 69)\n",
      "\n",
      "🔍 Cleaning maquette_23002.xlsx_Poutres\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Niveau', 'Nom de la famille', 'Nom du type', 'Identifiant', 'Batiment', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poteaux coupés (u)', 'Poteauc coupés (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (63 cols)\n",
      "🎯 Final maquette_23002.xlsx_Poutres shape: (96, 67)\n",
      "\n",
      "🔍 Cleaning maquette_23007.xlsx_Murs\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Nom de la famille', 'Nom du type', 'Identifiant', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue détail faible', 'Marque de type', \"Protection contre l'incendie\", 'ID MONTAGE', 'Désignation système', 'Réf DT']\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (67 cols)\n",
      "🎯 Final maquette_23007.xlsx_Murs shape: (203, 71)\n",
      "\n",
      "🔍 Cleaning maquette_23007.xlsx_Sols\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Nom de la famille', 'Nom du type', 'Identifiant', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue détail faible', 'Marque de type']\n",
      "ℹ️  Filled coupé cols with 0: ['Murs coupés (u)', 'Murs coupés (Ids)', 'Murs coupants (u)', 'Murs coupants (Ids)', 'Poutres coupés (u)', 'Poutres coupés (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)', 'Poteaux coupés (u)', 'Poteaux coupés (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "🗑️ Dropped rows where 011EC_Lot was NaN (9.8%)\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38590/4147864326.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[non_targets] = df[non_targets].fillna(0)\n",
      "/tmp/ipykernel_38590/4147864326.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[non_targets] = df[non_targets].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filled NaN→0 on non‐targets (61 cols)\n",
      "🎯 Final maquette_23007.xlsx_Sols shape: (37, 65)\n",
      "\n",
      "🔍 Cleaning maquette_23007.xlsx_Poutres\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Niveau', 'Nom de la famille', 'Nom du type', 'Identifiant', 'NOEMI', 'MILLS_View', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poteaux coupés (u)', 'Poteauc coupés (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "🗑️ Dropped rows where 011EC_Lot was NaN (4.5%)\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (65 cols)\n",
      "🎯 Final maquette_23007.xlsx_Poutres shape: (274, 69)\n",
      "\n",
      "🔍 Cleaning maquette_23007.xlsx_Poteaux\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', \"Marque d'emplacement du poteau\", 'Nom de la famille', 'Nom du type', 'Identifiant', 'NOEMI', 'MILLS_View', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type', 'Motif vue détail faible']\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poutres coupés (u)', 'Poutres coupés (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (57 cols)\n",
      "🎯 Final maquette_23007.xlsx_Poteaux shape: (115, 61)\n",
      "\n",
      "🔍 Cleaning RawData-Cibles.xlsx_Murs\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Batiment', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue détail faible', 'Marque de type', \"Protection contre l'incendie\"]\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (74 cols)\n",
      "🎯 Final RawData-Cibles.xlsx_Murs shape: (312, 78)\n",
      "\n",
      "🔍 Cleaning RawData-Cibles.xlsx_Sols\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Nom de la famille', 'Nom du type', 'Batiment', 'Niveau fini', 'Niveau Brut', 'NIVEAU_STRUCTURE', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Type IfcGUID', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue détail faible', 'Marque de type', 'épaisseur']\n",
      "ℹ️  Filled coupé cols with 0: ['Murs coupés (u)', 'Murs coupés (Ids)', 'Murs coupants (u)', 'Murs coupants (Ids)', 'Poutres coupés (u)', 'Poutres coupés (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)', 'Poteaux coupés (u)', 'Poteaux coupés (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "🗑️ Dropped rows where 012EC_Ouvrage was NaN (0.9%)\n",
      "🗑️ Dropped rows where 013EC_Localisation was NaN (2.8%)\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (69 cols)\n",
      "🎯 Final RawData-Cibles.xlsx_Sols shape: (103, 73)\n",
      "\n",
      "🔍 Cleaning RawData-Cibles.xlsx_Poutres\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Niveau', 'Nom de la famille', 'Nom du type', 'Batiment', 'NIVEAU_STRUCTURE', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', \"Description de l'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poteaux coupés (u)', 'Poteaux coupés (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "🗑️ Dropped rows where 011EC_Lot was NaN (1.6%)\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (76 cols)\n",
      "🎯 Final RawData-Cibles.xlsx_Poutres shape: (242, 80)\n",
      "\n",
      "🔍 Cleaning RawData-Cibles.xlsx_Poteaux\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Batiment', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', 'Titre OmniClass', 'Numéro OmniClass', \"Description de l'assemblage\", 'Marque de type']\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poutres coupés (u)', 'Poutres coupés (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38590/4147864326.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[non_targets] = df[non_targets].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filled NaN→0 on non‐targets (63 cols)\n",
      "🎯 Final RawData-Cibles.xlsx_Poteaux shape: (68, 67)\n",
      "\n",
      "🔍 Cleaning maquette_23001.xlsx_Murs\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Batiment', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue détail faible', 'Marque de type', \"Protection contre l'incendie\"]\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (74 cols)\n",
      "🎯 Final maquette_23001.xlsx_Murs shape: (312, 78)\n",
      "\n",
      "🔍 Cleaning maquette_23001.xlsx_Sols\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Nom de la famille', 'Nom du type', 'Batiment', 'Niveau fini', 'Niveau Brut', 'NIVEAU_STRUCTURE', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Type IfcGUID', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue détail faible', 'Marque de type', 'épaisseur']\n",
      "ℹ️  Filled coupé cols with 0: ['Murs coupés (u)', 'Murs coupés (Ids)', 'Murs coupants (u)', 'Murs coupants (Ids)', 'Poutres coupés (u)', 'Poutres coupés (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)', 'Poteaux coupés (u)', 'Poteaux coupés (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "🗑️ Dropped rows where 012EC_Ouvrage was NaN (0.9%)\n",
      "🗑️ Dropped rows where 013EC_Localisation was NaN (2.8%)\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (69 cols)\n",
      "🎯 Final maquette_23001.xlsx_Sols shape: (103, 73)\n",
      "\n",
      "🔍 Cleaning maquette_23001.xlsx_Poutres\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Niveau', 'Nom de la famille', 'Nom du type', 'Batiment', 'NIVEAU_STRUCTURE', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', \"Description de l'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poteaux coupés (u)', 'Poteauc coupés (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "🗑️ Dropped rows where 011EC_Lot was NaN (1.6%)\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38590/4147864326.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[non_targets] = df[non_targets].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filled NaN→0 on non‐targets (76 cols)\n",
      "🎯 Final maquette_23001.xlsx_Poutres shape: (242, 80)\n",
      "\n",
      "🔍 Cleaning maquette_23001.xlsx_Poteaux\n",
      "🗑️  Dups removed: 0\n",
      "🗑️  Dropped 100% NaN cols: [\"Type prédéfini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Batiment', \"Note d'identification\", \"Type: Type prédéfini d'IFC\", 'Exporter le type au format IFC sous', 'Modèle', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', 'Titre OmniClass', 'Numéro OmniClass', \"Description de l'assemblage\", 'Marque de type']\n",
      "ℹ️  Filled coupé cols with 0: ['Sols coupés (u)', 'Sols coupés (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poutres coupés (u)', 'Poutres coupés (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)']\n",
      "ℹ️  011EC_Lot: 0.0% missing\n",
      "ℹ️  012EC_Ouvrage: 0.0% missing\n",
      "ℹ️  013EC_Localisation: 0.0% missing\n",
      "ℹ️  014EC_Mode Constructif: 0.0% missing\n",
      "✅ Filled NaN→0 on non‐targets (63 cols)\n",
      "🎯 Final maquette_23001.xlsx_Poteaux shape: (68, 67)\n",
      "\n",
      "\n",
      "===== PROCESSING maquette_23017.xlsx_Murs =====\n",
      "⚠️ Skipping maquette_23017.xlsx_Murs due to insufficient valid classes in 011EC_Lot.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['011EC_Lot', '012EC_Ouvrage', '013EC_Localisation', '014EC_Mode Constructif'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltered \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Removed classes with fewer than 2 samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# preprocess & split\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m (X_tr, y_tr), (X_val, y_val), (X_te, y_te), preproc, le_dict \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTARGET_COLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m out_dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(le_dict[c]\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m TARGET_COLS]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(df, target_columns, test_size, val_size, random_state)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_dataset\u001b[39m(\n\u001b[1;32m     29\u001b[0m     df,\n\u001b[1;32m     30\u001b[0m     target_columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     34\u001b[0m ):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    - Splits df into X, y\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    - LabelEncodes each target\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    - Preprocesses X\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    - Splits into train/val/test\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     y \u001b[38;5;241m=\u001b[39m df[target_columns]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# encode Y\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/BImpredict2/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/BImpredict2/lib/python3.10/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/BImpredict2/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/BImpredict2/lib/python3.10/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/.pyenv/versions/BImpredict2/lib/python3.10/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/BImpredict2/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['011EC_Lot', '012EC_Ouvrage', '013EC_Localisation', '014EC_Mode Constructif'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# ─── 4. MAIN EXECUTION ────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) load your raw Excel sheets however you like:\n",
    "    # Load all Excel files in RAW_DATA_DIR\n",
    "    raw_dfs = {}\n",
    "    for file in excel_files:\n",
    "        file_path = os.path.join(RAW_DATA_DIR, file)\n",
    "        print(f\"Loading: {file_path}\")\n",
    "        try:\n",
    "            excel_data = pd.ExcelFile(file_path)\n",
    "            for sheet_name in excel_data.sheet_names:\n",
    "                df_name = f\"{file}_{sheet_name}\"\n",
    "                raw_dfs[df_name] = excel_data.parse(sheet_name)\n",
    "                print(f\"Loaded sheet: {sheet_name} from {file}, Shape: {raw_dfs[df_name].shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "    # 2) your parameters\n",
    "    EXC_KEYWORDS      = [\"coupés\", \"coupants\"]\n",
    "    TARGET_COLS       = [\"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\"]\n",
    "    SEEN_FILES        = [\"seen1.xlsx\", \"seen2.xlsx\"]\n",
    "    HYPERPARAM_GRID   = [\n",
    "        {\"num_layers\": 1, \"units\": 64, \"dropout\": 0.0, \"lr\": 1e-3},\n",
    "        {\"num_layers\": 2, \"units\": 64, \"dropout\": 0.2, \"lr\": 1e-3},\n",
    "        {\"num_layers\": 2, \"units\": 128, \"dropout\": 0.3, \"lr\": 5e-4},\n",
    "        {\"num_layers\": 3, \"units\": 256, \"dropout\": 0.4, \"lr\": 1e-4},\n",
    "    ]\n",
    "\n",
    "    # 3) CLEAN\n",
    "    cleaned = {}\n",
    "    for name, df in raw_dfs.items():\n",
    "        print(f\"\\n🔍 Cleaning {name}\")\n",
    "        df2 = drop_duplicates_and_missing(df, EXC_KEYWORDS)\n",
    "        df3 = ensure_targets(df2, TARGET_COLS)\n",
    "        df4 = fill_non_targets(df3, TARGET_COLS)\n",
    "        cleaned[name] = df4\n",
    "        print(f\"🎯 Final {name} shape: {df4.shape}\")\n",
    "\n",
    "    # 4) FOR EACH SHEET → preprocess, train, eval, save, test\n",
    "    for name, df in cleaned.items():\n",
    "        print(f\"\\n\\n===== PROCESSING {name} =====\")\n",
    "\n",
    "        # Ensure all classes in the target columns have at least two samples\n",
    "        for target in TARGET_COLS:\n",
    "            class_counts = df[target].value_counts()\n",
    "            valid_classes = class_counts[class_counts >= 2].index\n",
    "            if len(valid_classes) < 2:\n",
    "                print(f\"⚠️ Skipping {name} due to insufficient valid classes in {target}.\")\n",
    "                df = pd.DataFrame()  # Clear the dataframe to avoid further processing\n",
    "                break\n",
    "            df = df[df[target].isin(valid_classes)]\n",
    "            print(f\"Filtered {target}: Removed classes with fewer than 2 samples.\")\n",
    "\n",
    "        # preprocess & split\n",
    "        (X_tr, y_tr), (X_val, y_val), (X_te, y_te), preproc, le_dict = prepare_dataset(\n",
    "            df, TARGET_COLS, test_size=0.3, val_size=0.5, random_state=42\n",
    "        )\n",
    "        out_dims = [len(le_dict[c].classes_) for c in TARGET_COLS]\n",
    "\n",
    "        # train\n",
    "        models, hists, scores = train_models(\n",
    "            X_tr, y_tr, X_val, y_val,\n",
    "            output_dims=out_dims,\n",
    "            hyperparams=HYPERPARAM_GRID,\n",
    "            epochs=40\n",
    "        )\n",
    "\n",
    "        # visualize & table\n",
    "        plot_learning_curves(hists, title=f\"{name} val_loss\")\n",
    "        df_scores = summarize_scores(scores)\n",
    "        print(df_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Save & test\n",
    "        prefix = sheet.lower()\n",
    "        save_top3(models, scores, os.path.join(DL_MODELS_DIR, prefix))\n",
    "        best_models = [os.path.join(DL_MODELS_DIR, f\"{prefix}_top{i}.h5\") for i in (1, 2, 3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # List all Excel files in TESTING_DATA_DIR\n",
    "        testing_excel_files = [f for f in os.listdir(TESTING_DATA_DIR) if f.endswith(\".xlsx\") or f.endswith(\".xls\")]\n",
    "\n",
    "        # Test saved models and save predictions\n",
    "        for model_path in best_models:\n",
    "            print(f\"\\n🔎 Testing model: {model_path}\")\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "            for test_file in testing_excel_files:\n",
    "                test_file_path = os.path.join(TESTING_DATA_DIR, test_file)\n",
    "                print(f\"Processing test file: {test_file_path}\")\n",
    "\n",
    "                try:\n",
    "                    # Load the Excel file\n",
    "                    test_excel_data = pd.ExcelFile(test_file_path)\n",
    "\n",
    "                    # Process each sheet in the Excel file\n",
    "                    for sheet_name in test_excel_data.sheet_names:\n",
    "                        if sheet_name.lower() == sheet.lower():  # Match the category dynamically\n",
    "                            test_df = test_excel_data.parse(sheet_name)\n",
    "\n",
    "                            # Filter required columns\n",
    "                            test_df = test_df[required_columns[sheet]]\n",
    "\n",
    "                            # Preprocess the test data\n",
    "                            X_test = preproc.transform(test_df.drop(columns=TARGET_COLUMNS))\n",
    "\n",
    "                            # Predict using the model\n",
    "                            predictions = model.predict(X_test)\n",
    "\n",
    "                            # Add predictions to the DataFrame\n",
    "                            for i, target in enumerate(TARGET_COLUMNS):\n",
    "                                test_df[f\"pred_{target}\"] = le_dict[target].inverse_transform(predictions[i].argmax(axis=1))\n",
    "\n",
    "                            # Save the predictions to PREDICTED_DATA_DIR\n",
    "                            predicted_file_path = os.path.join(PREDICTED_DATA_DIR, f\"{test_file}_{sheet_name}_predictions.xlsx\")\n",
    "                            test_df.to_excel(predicted_file_path, index=False)\n",
    "                            print(f\"✅ Predictions saved to: {predicted_file_path}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error processing {test_file_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BImpredict2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
