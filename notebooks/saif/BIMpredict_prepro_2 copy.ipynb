{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1fae56",
   "metadata": {},
   "source": [
    "# Bim_Predict NoteBook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08c1ec",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227613f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: ../../data/raw_data\n",
      "Directory already exists: ../../data/processed_data\n",
      "Directory already exists: ../../data/predicting_data\n",
      "Directory already exists: ../../models\n",
      "Directory already exists: ../../models/SK/machine_learning\n",
      "Directory already exists: ../../models/SK/deep_learning\n",
      "Directory already exists: ../../models/SK/other\n",
      "Directory already exists: ../../python_modules\n",
      "Directory already exists: ../../plots\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define project folder paths\n",
    "# Data directories\n",
    "BASE_DIR = \"../../\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "PREDICTED_DATA_DIR = os.path.join(DATA_DIR, \"predicting_data\")\n",
    "TESTING_DATA_DIR = os.path.join(DATA_DIR, \"testing_data\")\n",
    "\n",
    "# Model directories\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ML_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/machine_learning\")\n",
    "DL_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/deep_learning\")\n",
    "OTHER_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/other\")\n",
    "\n",
    "# Python modules and plots directories\n",
    "PYTHON_MODULES_DIR = os.path.join(BASE_DIR, \"python_modules\")\n",
    "PLOTS_DIR = os.path.join(BASE_DIR, \"plots\")\n",
    "\n",
    "# List of directories to create\n",
    "directories = [\n",
    "    RAW_DATA_DIR, PROCESSED_DATA_DIR, PREDICTED_DATA_DIR,\n",
    "    MODELS_DIR, ML_MODELS_DIR, DL_MODELS_DIR, OTHER_MODELS_DIR,\n",
    "    PYTHON_MODULES_DIR, PLOTS_DIR\n",
    "]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e384cf",
   "metadata": {},
   "source": [
    "<!-- ### Paths Creating && Data Importing -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4763e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../data/raw_data/maquette_23017.xlsx\n",
      "Loading: ../../data/raw_data/maquette_23016.xlsx\n",
      "Loading: ../../data/raw_data/maquette_23002.xlsx\n",
      "Loading: ../../data/raw_data/maquette_23007.xlsx\n",
      "Loading: ../../data/raw_data/RawData-Cibles.xlsx\n",
      "Loading: ../../data/raw_data/maquette_23001.xlsx\n",
      "\n",
      "Total files processed: 23\n",
      "Loaded DataFrame: maquette_23017.xlsx_Murs, Shape: (215, 149)\n",
      "Loaded DataFrame: maquette_23017.xlsx_Sols, Shape: (29, 140)\n",
      "Loaded DataFrame: maquette_23017.xlsx_Poutres, Shape: (152, 136)\n",
      "Loaded DataFrame: maquette_23017.xlsx_Poteaux, Shape: (72, 111)\n",
      "Loaded DataFrame: maquette_23016.xlsx_Murs, Shape: (1589, 146)\n",
      "Loaded DataFrame: maquette_23016.xlsx_Sols, Shape: (45, 142)\n",
      "Loaded DataFrame: maquette_23016.xlsx_Poutres, Shape: (778, 136)\n",
      "Loaded DataFrame: maquette_23016.xlsx_Poteaux, Shape: (215, 110)\n",
      "Loaded DataFrame: maquette_23002.xlsx_Murs, Shape: (345, 94)\n",
      "Loaded DataFrame: maquette_23002.xlsx_Sols, Shape: (32, 91)\n",
      "Loaded DataFrame: maquette_23002.xlsx_Poutres, Shape: (96, 89)\n",
      "Loaded DataFrame: maquette_23007.xlsx_Murs, Shape: (203, 91)\n",
      "Loaded DataFrame: maquette_23007.xlsx_Sols, Shape: (41, 82)\n",
      "Loaded DataFrame: maquette_23007.xlsx_Poutres, Shape: (287, 91)\n",
      "Loaded DataFrame: maquette_23007.xlsx_Poteaux, Shape: (115, 83)\n",
      "Loaded DataFrame: RawData-Cibles.xlsx_Murs, Shape: (312, 96)\n",
      "Loaded DataFrame: RawData-Cibles.xlsx_Sols, Shape: (107, 94)\n",
      "Loaded DataFrame: RawData-Cibles.xlsx_Poutres, Shape: (246, 100)\n",
      "Loaded DataFrame: RawData-Cibles.xlsx_Poteaux, Shape: (68, 87)\n",
      "Loaded DataFrame: maquette_23001.xlsx_Murs, Shape: (312, 96)\n",
      "Loaded DataFrame: maquette_23001.xlsx_Sols, Shape: (107, 94)\n",
      "Loaded DataFrame: maquette_23001.xlsx_Poutres, Shape: (246, 100)\n",
      "Loaded DataFrame: maquette_23001.xlsx_Poteaux, Shape: (68, 87)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# List all Excel files in RAW_DATA_DIR\n",
    "excel_files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith(\".xlsx\") or f.endswith(\".xls\")]\n",
    "\n",
    "# Dictionary to store DataFrames for each file and sheet\n",
    "dataframes = {}\n",
    "\n",
    "# Process each Excel file\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(RAW_DATA_DIR, file)\n",
    "    print(f\"Loading: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # Load Excel file\n",
    "        excel_data = pd.ExcelFile(file_path)\n",
    "\n",
    "        # Load all sheets dynamically\n",
    "        for sheet_name in excel_data.sheet_names:\n",
    "            df = excel_data.parse(sheet_name)\n",
    "\n",
    "            # Save DataFrame with a unique identifier\n",
    "            dataframes[f\"{file}_{sheet_name}\"] = df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# Display summary of loaded data\n",
    "print(f\"\\nTotal files processed: {len(dataframes)}\")\n",
    "for key, df in dataframes.items():\n",
    "    print(f\"Loaded DataFrame: {key}, Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1514a",
   "metadata": {},
   "source": [
    "<!-- ### Data Cleaning && PreProcessing -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc959f",
   "metadata": {},
   "source": [
    "## PreProcessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b36ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define required columns dynamically\n",
    "# required_columns = {\n",
    "#     \"Murs\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"Hauteur\",\n",
    "#              \"Epaisseur\", \"AI\", \"AS\", \"Sols en intersection\", \"Sols coup√©s (u)\", \"Sols coup√©s (Ids)\",\n",
    "#              \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Sol au-dessus\", \"Sol au-dessous\", \"Fen√™tres\", \"Portes\",\n",
    "#              \"Ouvertures\", \"Murs imbriqu√©s\", \"Mur multicouche\", \"Mur empil√©\", \"Profil modifi√©\", \"Extension inf√©rieure\",\n",
    "#              \"Extension sup√©rieure\", \"Partie inf√©rieure attach√©e\", \"Partie sup√©rieure attach√©e\", \"D√©calage sup√©rieur\",\n",
    "#              \"D√©calage inf√©rieur\", \"Mat√©riau structurel\"],\n",
    "\n",
    "#     \"Sols\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"Murs en intersection\",\n",
    "#              \"Murs coup√©s (u)\", \"Murs coup√©s (Ids)\", \"Murs coupants (u)\", \"Murs coupants (Ids)\", \"Poutres en intersection\",\n",
    "#              \"Poutres coup√©s (u)\", \"Poutres coup√©s (Ids)\", \"Poutres coupants (u)\", \"Poutres coupants (Ids)\",\n",
    "#              \"Poteaux en intersection\", \"Poteaux coup√©s (u)\", \"Poteaux coup√©s (Ids)\", \"Poteaux coupants (u)\",\n",
    "#              \"Poteaux coupants (Ids)\", \"Ouvertures\", \"Sol multicouche\", \"Profil modifi√©\", \"D√©calage par rapport au niveau\",\n",
    "#              \"Epaisseur\", \"Li√© au volume\", \"Etude de l'√©l√©vation √† la base\", \"Etude de l'√©l√©vation en haut\",\n",
    "#              \"Epaisseur du porteur\", \"El√©vation au niveau du noyau inf√©rieur\", \"El√©vation au niveau du noyau sup√©rieur\",\n",
    "#              \"El√©vation en haut\", \"El√©vation √† la base\", \"Mat√©riau structurel\"],\n",
    "\n",
    "#     \"Poutres\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"AI\", \"AS\",\n",
    "#                 \"Hauteur totale\", \"Hauteur\", \"Sols en intersection\", \"Sols coup√©s (u)\", \"Sols coup√©s (Ids)\",\n",
    "#                 \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Sol au-dessus\", \"Sol au-dessous\", \"Poteaux en intersection\",\n",
    "#                 \"Poteaux coup√©s (u)\", \"Poteaux coup√©s (Ids)\", \"Poteaux coupants (u)\", \"Poteaux coupants (Ids)\",\n",
    "#                 \"Etat de la jonction\", \"Valeur de d√©calage Z\", \"Justification Z\", \"Valeur de d√©calage Y\", \"Justification Y\",\n",
    "#                 \"Justification YZ\", \"Mat√©riau structurel\", \"El√©vation du niveau de r√©f√©rence\", \"El√©vation en haut\",\n",
    "#                 \"Rotation de la section\", \"Orientation\", \"D√©calage du niveau d'arriv√©e\", \"D√©calage du niveau de d√©part\",\n",
    "#                 \"El√©vation √† la base\", \"Longueur de coupe\", \"Longueur\", \"hauteur_section\", \"largeur_section\"],\n",
    "\n",
    "#     \"Poteaux\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"AI\", \"AS\",\n",
    "#                 \"Hauteur\", \"Longueur\", \"Partie inf√©rieure attach√©e\", \"Partie sup√©rieure attach√©e\", \"Sols en intersection\",\n",
    "#                 \"Sols coup√©s (u)\", \"Sols coup√©s (Ids)\", \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Poutres en intersection\",\n",
    "#                 \"Poutres coup√©s (u)\", \"Poutres coup√©s (Ids)\", \"Poutres coupants (u)\", \"Poutres coupants (Ids)\",\n",
    "#                 \"Mat√©riau structurel\", \"D√©calage sup√©rieur\", \"D√©calage inf√©rieur\", \"Diam√®tre poteau\", \"h\", \"b\",\n",
    "#                 \"hauteur_section\", \"largeur_section\"]\n",
    "# }\n",
    "\n",
    "# # Initialize a dictionary to store filtered dataframes\n",
    "# cleaned_dataframes = {}\n",
    "\n",
    "# for df_name, df in dataframes.items():\n",
    "#     print(f\"\\nüü¢ Original shape of {df_name}: {df.shape}\")\n",
    "\n",
    "#     # Automatically detect the correct category for filtering\n",
    "#     for category, columns in required_columns.items():\n",
    "#         if category.lower() in df_name.lower():  # Match dynamically\n",
    "#             try:\n",
    "#                 filtered_df = df[columns]  # Keep only the required columns\n",
    "#             except KeyError as e:\n",
    "#                 missing_columns = set(columns) - set(df.columns)\n",
    "#                 print(f\"‚ö†Ô∏è Missing columns in {df_name}: {missing_columns}. Skipping this dataframe.\")\n",
    "#                 continue\n",
    "#             cleaned_dataframes[df_name] = filtered_df\n",
    "#             print(f\"‚úÖ Shape after filtering {df_name}: {filtered_df.shape}\")\n",
    "#             break  # Stop looping once the correct match is found\n",
    "#     else:\n",
    "#         print(f\"‚ö†Ô∏è No matching category for {df_name}, skipping filtering.\")\n",
    "\n",
    "# # Add prefixes to column names based on the dataframe category and update index\n",
    "# for name, df in cleaned_dataframes.items():\n",
    "#     if \"murs\" in name.lower():\n",
    "#         prefix = \"murs_\"\n",
    "#     elif \"sols\" in name.lower():\n",
    "#         prefix = \"sols_\"\n",
    "#     elif \"poutres\" in name.lower():\n",
    "#         prefix = \"poutres_\"\n",
    "#     elif \"poteaux\" in name.lower():\n",
    "#         prefix = \"poteaux_\"\n",
    "#     else:\n",
    "#         prefix = \"\"\n",
    "\n",
    "#     # Rename columns with the prefix\n",
    "#     df.rename(columns=lambda col: f\"{prefix}{col}\" if col.lower() != \"id\" else f\"{prefix}id\", inplace=True)\n",
    "\n",
    "#     # Drop the existing index and set the prefixed ID column as the new index\n",
    "#     id_column = f\"{prefix}id\"\n",
    "#     if id_column in df.columns:\n",
    "#         df.set_index(id_column, inplace=True)\n",
    "#         print(f\"‚úÖ Set '{id_column}' as index for {name}.\")\n",
    "#     else:\n",
    "#         print(f\"‚ö†Ô∏è '{id_column}' column not found in {name}, skipping index setting.\")\n",
    "\n",
    "    # Update the cleaned_dataframes dictionary\n",
    "    # cleaned_dataframes[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d5cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nüìä Cleaned DataFrames:\")\n",
    "# for df_name, df in cleaned_dataframes.items():\n",
    "#     print(f\" - {df_name}: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8b2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove duplicates and 100% missing columns (with exceptions)\n",
    "# def process_dataframe(df_name, exception_keywords):\n",
    "#     # Access the dataframe from cleaned_dataframes\n",
    "#     df = cleaned_dataframes[df_name].copy()\n",
    "\n",
    "#     # Remove duplicate rows\n",
    "#     initial_shape = df.shape\n",
    "#     df.drop_duplicates(inplace=True)\n",
    "#     duplicates_removed = initial_shape[0] - df.shape[0]\n",
    "#     if duplicates_removed > 0:\n",
    "#         print(f\"üü¢ Removed {duplicates_removed} duplicate rows from {df_name}.\")\n",
    "#     else:\n",
    "#         print(f\"‚úÖ No duplicate rows found in {df_name}.\")\n",
    "\n",
    "#     # Identify fully missing columns\n",
    "#     missing_cols = df.columns[df.isnull().mean() == 1]\n",
    "#     # Keep columns that contain an exception keyword or are in target_columns intact\n",
    "#     cols_to_drop = [\n",
    "#         col for col in missing_cols\n",
    "#         if not any(keyword in col.lower() for keyword in exception_keywords)\n",
    "#     ]\n",
    "\n",
    "#     if cols_to_drop:\n",
    "#         print(f\"üü† Dropping fully missing columns from {df_name}: {cols_to_drop}\")\n",
    "#         df.drop(columns=cols_to_drop, inplace=True)\n",
    "#     else:\n",
    "#         print(f\"‚úÖ No columns dropped from {df_name}; all fully missing columns are exceptions.\")\n",
    "\n",
    "#     # Fill missing values in columns containing \"coup√©\" or \"coupants\"\n",
    "#     columns_to_fill = [col for col in df.columns if \"coup√©\" in col.lower() or \"coupants\" in col.lower()]\n",
    "#     if columns_to_fill:\n",
    "#         print(f\"üîµ Filling missing values with 0 for columns in {df_name}: {columns_to_fill}\")\n",
    "#         df[columns_to_fill] = df[columns_to_fill].fillna(0)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # Apply processing on all dataframes in cleaned_dataframes\n",
    "# exception_keywords = [\"coup√©s\", \"coupants\"]\n",
    "\n",
    "# processed_dataframes = {}\n",
    "# for df_name in cleaned_dataframes.keys():\n",
    "#     print(f\"\\nüîç Processing dataframe: {df_name}\")\n",
    "#     processed_dataframes[df_name] = process_dataframe(df_name, exception_keywords)\n",
    "#     final_shape = processed_dataframes[df_name].shape\n",
    "#     print(f\"‚úÖ Final shape of {df_name}: {final_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b37a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify target columns dynamically across all DataFrames\n",
    "# TARGET_COLUMNS = ['011EC_Lot', '012EC_Ouvrage', '013EC_Localisation', '014EC_Mode Constructif']\n",
    "\n",
    "# # Check and add missing target columns\n",
    "# for df_name, df in processed_dataframes.items():\n",
    "#     print(f\"\\nProcessing dataframe: {df_name}\")\n",
    "#     initial_shape = df.shape  # Store the initial shape of the dataframe\n",
    "\n",
    "#     for target in TARGET_COLUMNS:\n",
    "#         if target in df.columns:\n",
    "#             print(f\"‚úÖ Target column '{target}' found in dataframe '{df_name}'.\")\n",
    "\n",
    "#             # Check for missing data in the target column\n",
    "#             missing_count = df[target].isnull().sum()\n",
    "#             total_count = len(df)\n",
    "#             missing_percentage = (missing_count / total_count) * 100\n",
    "#             if missing_count > 0:\n",
    "#                 print(f\"‚ö†Ô∏è Target column '{target}' has {missing_count} missing values ({missing_percentage:.2f}%).\")\n",
    "\n",
    "#                 # Drop rows if missing data is less than 10%\n",
    "#                 if missing_percentage < 10:\n",
    "#                     df = df[df[target].notnull()]\n",
    "#                     print(f\"‚úÖ Dropped rows with missing values in '{target}' (less than 10%).\")\n",
    "#             else:\n",
    "#                 print(f\"‚úÖ Target column '{target}' has no missing values.\")\n",
    "#         else:\n",
    "#             print(f\"‚ö†Ô∏è Target column '{target}' does not exist in dataframe '{df_name}'. Adding it...\")\n",
    "#             # Add the missing target column with default values (e.g., NaN)\n",
    "#             df[target] = float('nan')\n",
    "#             print(f\"‚úÖ Added missing target column '{target}' to dataframe '{df_name}'.\")\n",
    "\n",
    "#     final_shape = df.shape  # Store the final shape of the dataframe\n",
    "#     if initial_shape != final_shape:\n",
    "#         print(f\"üìä Shape before: {initial_shape}, Shape after: {final_shape}\")\n",
    "\n",
    "#     # Update the cleaned_dataframes dictionary\n",
    "#     cleaned_dataframes[df_name] = processed_dataframes[df_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb298214",
   "metadata": {},
   "source": [
    "<!-- ### Exploratory Data Analysis (EDA) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9971ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure missing values are filled in the processed datasets unless in TARGET_COLUMNS\n",
    "# for df_name, df in cleaned_dataframes.items():\n",
    "#     print(f\"\\nüü¢ Filling missing values for {df_name}...\")\n",
    "\n",
    "#     # Display shape before filling missing values\n",
    "#     initial_shape = df.shape\n",
    "#     print(f\"üìå Initial shape before filling NaN: {initial_shape}\")\n",
    "\n",
    "#     # Fill missing values with 0 for non-target columns\n",
    "#     non_target_columns = [col for col in df.columns if col not in TARGET_COLUMNS]\n",
    "#     df[non_target_columns] = df[non_target_columns].fillna(0)\n",
    "\n",
    "#     # Store updated dataframe back\n",
    "#     cleaned_dataframes[df_name] = df\n",
    "\n",
    "#     # Display shape after processing\n",
    "#     final_shape = df.shape\n",
    "#     print(f\"‚úÖ Final shape after filling NaN: {final_shape}\")\n",
    "\n",
    "# print(\"üöÄ Missing values successfully handled across all datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f2a66",
   "metadata": {},
   "source": [
    "## EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d1b70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Function to remove low-variance & highly correlated features\n",
    "# def optimize_feature_selection(df, variance_threshold=0.02, correlation_threshold=0.98):\n",
    "#     print(f\"\\nüîç Processing {df.shape[0]} rows & {df.shape[1]} columns\")\n",
    "\n",
    "#     # Step 1: Remove Low-Variance Features\n",
    "#     selector = VarianceThreshold(variance_threshold)\n",
    "#     numeric_df = df.select_dtypes(include=[\"number\"])  # Focus only on numerical columns\n",
    "#     selector.fit(numeric_df)\n",
    "\n",
    "#     low_variance_cols = numeric_df.columns[~selector.get_support()]\n",
    "#     keep_cols = [col for col in low_variance_cols if any(keyword in col.lower() for keyword in [\"coup√©s\", \"coupants\"])]\n",
    "#     drop_cols = [col for col in low_variance_cols if col not in keep_cols and col not in TARGET_COLUMNS]\n",
    "\n",
    "#     df.drop(columns=drop_cols, inplace=True)\n",
    "#     print(f\"‚ö†Ô∏è Dropped {len(drop_cols)} low-variance columns (excluding 'coup√©s' and target columns): {drop_cols}\")\n",
    "\n",
    "#     # Step 2: Remove Highly Correlated Features\n",
    "#     numeric_df = df.select_dtypes(include=[\"number\"])\n",
    "#     correlation_matrix = numeric_df.corr().abs()\n",
    "#     upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "#     correlated_features = [\n",
    "#         col for col in upper_triangle.columns\n",
    "#         if any(upper_triangle[col] > correlation_threshold) and col not in TARGET_COLUMNS\n",
    "#     ]\n",
    "\n",
    "#     df.drop(columns=correlated_features, inplace=True)\n",
    "#     print(f\"‚ö†Ô∏è Dropped {len(correlated_features)} highly correlated columns (excluding target columns): {correlated_features}\")\n",
    "\n",
    "#     print(f\"‚úÖ Final shape after filtering: {df.shape}\")\n",
    "#     return df\n",
    "\n",
    "# # Apply optimized feature selection to all datasets\n",
    "# final_cleaned_dataframes = {name: optimize_feature_selection(df) for name, df in cleaned_dataframes.items()}\n",
    "\n",
    "# print(\"üöÄ Optimized feature selection completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8548dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display basic statistics for all cleaned sheets\n",
    "# for df_name, df in final_cleaned_dataframes.items():\n",
    "#     print(f\"\\nSummary statistics for {df_name}:\")\n",
    "\n",
    "#     print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabfb1e",
   "metadata": {},
   "source": [
    "<!-- ### Feature Selection -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a62fbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify target columns dynamically across all DataFrames\n",
    "# target_columns_found = set()\n",
    "# for df_name, df in final_cleaned_dataframes.items():\n",
    "#     found_targets = [\n",
    "#         col for col in df.columns\n",
    "#         if any(target.lower() in col.lower() for target in TARGET_COLUMNS)\n",
    "#     ]\n",
    "#     target_columns_found.update(found_targets)\n",
    "\n",
    "# print(f\"\\nTarget columns detected across datasets: {target_columns_found}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34f0966",
   "metadata": {},
   "source": [
    "<!-- ## Training and testing  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a81fe7",
   "metadata": {},
   "source": [
    "## Deep-Learning Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f20fb331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 13:37:55.784140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-10 13:37:56.087159: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-06-10 13:37:56.087198: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-06-10 13:37:56.147514: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-10 13:37:58.166644: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-06-10 13:37:58.166838: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-06-10 13:37:58.166854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.preprocessing    import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute           import SimpleImputer\n",
    "from sklearn.compose          import ColumnTransformer\n",
    "from sklearn.pipeline         import Pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cd52536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ 1. DATA CLEANING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def filter_required_columns(dataframes, required_columns):\n",
    "    \"\"\"\n",
    "    For each df in `dataframes`, pick the matching sheet key and\n",
    "    keep only its required columns. Returns a dict of filtered dfs.\n",
    "    \"\"\"\n",
    "    cleaned = {}\n",
    "    for name, df in dataframes.items():\n",
    "        for sheet, cols in required_columns.items():\n",
    "            if sheet.lower() in name.lower():\n",
    "                missing = set(cols) - set(df.columns)\n",
    "                if missing:\n",
    "                    print(f\"‚ö†Ô∏è Missing in {name}: {missing}. Skipping.\")\n",
    "                else:\n",
    "                    cleaned[name] = df[cols].copy()\n",
    "                    print(f\"‚úÖ {name}: filtered to {df[cols].shape}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No match for {name}, skipped\")\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def drop_duplicates_and_missing(df, exception_keywords):\n",
    "    \"\"\"\n",
    "    - Drop duplicate rows\n",
    "    - Drop any column that‚Äôs 100% NaN unless it contains an exception keyword\n",
    "    - Fill any ‚Äúcoup√©‚Äù/‚Äúcoupants‚Äù column‚Äôs NaNs with 0\n",
    "    \"\"\"\n",
    "    before = df.shape[0]\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"üóëÔ∏è  Dups removed: {before - df.shape[0]}\")\n",
    "\n",
    "    fully_missing = [c for c in df.columns if df[c].isna().all()]\n",
    "    to_drop = [c for c in fully_missing\n",
    "               if not any(kw.lower() in c.lower() for kw in exception_keywords)]\n",
    "    df = df.drop(columns=to_drop)\n",
    "    if to_drop: print(f\"üóëÔ∏è  Dropped 100% NaN cols: {to_drop}\")\n",
    "\n",
    "    fill_cols = [c for c in df.columns if \"coup√©\" in c.lower() or \"coupant\" in c.lower()]\n",
    "    df[fill_cols] = df[fill_cols].fillna(0)\n",
    "    if fill_cols: print(f\"‚ÑπÔ∏è  Filled coup√© cols with 0: {fill_cols}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def ensure_targets(df, target_columns, drop_threshold=0.10):\n",
    "    \"\"\"\n",
    "    - Ensure each target exists (if not, create it with NaN)\n",
    "    - If existing but < drop_threshold% missing ‚Üí drop those rows\n",
    "    - Leave > threshold as-is (you may choose to impute later)\n",
    "    \"\"\"\n",
    "    for t in target_columns:\n",
    "        if t not in df.columns:\n",
    "            df[t] = np.nan\n",
    "            print(f\"‚ûï Added missing target {t}\")\n",
    "        miss_pct = df[t].isna().mean()\n",
    "        if 0 < miss_pct < drop_threshold:\n",
    "            df = df[df[t].notna()]\n",
    "            print(f\"üóëÔ∏è Dropped rows where {t} was NaN ({miss_pct:.1%})\")\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è  {t}: {miss_pct:.1%} missing\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_non_targets(df, target_columns):\n",
    "    \"\"\"\n",
    "    Fill all non‚Äêtarget columns‚Äô NaNs with 0\n",
    "    \"\"\"\n",
    "    non_targets = [c for c in df.columns if c not in target_columns]\n",
    "    df[non_targets] = df[non_targets].fillna(0)\n",
    "    print(f\"‚úÖ Filled NaN‚Üí0 on non‚Äêtargets ({len(non_targets)} cols)\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_all_dataframes(\n",
    "    raw_dfs,\n",
    "    required_columns,\n",
    "    exception_keywords,\n",
    "    target_columns\n",
    "):\n",
    "    # 1) filter\n",
    "    filt = filter_required_columns(raw_dfs, required_columns)\n",
    "\n",
    "    # 2) per‚Äêdf cleaning\n",
    "    cleaned = {}\n",
    "    for name, df in filt.items():\n",
    "        print(f\"\\nüîç Cleaning {name}\")\n",
    "        df2 = drop_duplicates_and_missing(df, exception_keywords)\n",
    "        df3 = ensure_targets(df2, target_columns)\n",
    "        df4 = fill_non_targets(df3,   target_columns)\n",
    "        cleaned[name] = df4\n",
    "        print(f\"üéØ Final {name} shape: {df4.shape}\")\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18579273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ 2. PREPROCESS & SPLIT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def build_preprocessor(X_df):\n",
    "    \"\"\"\n",
    "    - numeric: mean‚Äêimpute + StandardScaler\n",
    "    - categorical: mode‚Äêimpute + OneHotEncoder (ensuring all values are strings)\n",
    "    \"\"\"\n",
    "    num_cols = X_df.select_dtypes([\"int64\", \"float64\"]).columns\n",
    "    cat_cols = X_df.select_dtypes([\"object\", \"category\"]).columns\n",
    "\n",
    "    # Convert categorical columns to string to avoid mixed types\n",
    "    X_df[cat_cols] = X_df[cat_cols].astype(str)\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\")),\n",
    "    ])\n",
    "    transformer = ColumnTransformer([\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols),\n",
    "    ])\n",
    "    return transformer\n",
    "\n",
    "\n",
    "def prepare_dataset(\n",
    "    df,\n",
    "    target_columns,\n",
    "    test_size=0.3,\n",
    "    val_size=0.5,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    - Splits df into X, y\n",
    "    - LabelEncodes each target\n",
    "    - Preprocesses X\n",
    "    - Splits into train/val/test\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=target_columns)\n",
    "    y = df[target_columns].astype(str)\n",
    "    # encode Y\n",
    "    le_dict, y_enc = {}, []\n",
    "    for col in y:\n",
    "        le = LabelEncoder().fit(y[col])\n",
    "        y_enc.append(le.transform(y[col]))\n",
    "        le_dict[col] = le\n",
    "    y_enc = np.vstack(y_enc).T\n",
    "\n",
    "    # preprocess X\n",
    "    pre = build_preprocessor(X)\n",
    "    X_all = pre.fit_transform(X)\n",
    "\n",
    "    # splits\n",
    "    strat = y_enc[:,0]\n",
    "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
    "        X_all, y_enc, test_size=test_size,\n",
    "        stratify=strat, random_state=random_state\n",
    "    )\n",
    "    X_val, X_te, y_val, y_te = train_test_split(\n",
    "        X_tmp, y_tmp, test_size=val_size,\n",
    "        stratify=y_tmp[:,0], random_state=random_state\n",
    "    )\n",
    "    return (X_tr,y_tr), (X_val,y_val), (X_te,y_te), pre, le_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2eba69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ 3. MODEL BUILDING & TRAINING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def build_mlp(input_dim, output_dims, hp):\n",
    "    inp = layers.Input((input_dim,))\n",
    "    x = inp\n",
    "    for i in range(hp[\"num_layers\"]):\n",
    "        x = layers.Dense(hp[\"units\"], activation=\"relu\")(x)\n",
    "        if hp[\"dropout\"]>0:\n",
    "            x = layers.Dropout(hp[\"dropout\"])(x)\n",
    "\n",
    "    outputs, losses, metrics = [], {}, {}\n",
    "    for i, d in enumerate(output_dims):\n",
    "        name = f\"out{i}\"\n",
    "        o = layers.Dense(d, activation=\"softmax\", name=name)(x)\n",
    "        outputs.append(o)\n",
    "        losses[name]   = \"sparse_categorical_crossentropy\"\n",
    "        metrics[name]  = [\"accuracy\"]\n",
    "\n",
    "    m = Model(inp, outputs)\n",
    "    m.compile(\n",
    "        optimizer=optimizers.Adam(hp[\"lr\"]),\n",
    "        loss=losses,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return m\n",
    "\n",
    "\n",
    "def train_models(\n",
    "    X_tr, y_tr,\n",
    "    X_val, y_val,\n",
    "    output_dims,\n",
    "    hyperparams,\n",
    "    epochs=30,\n",
    "    batch_size=32\n",
    "):\n",
    "    y_tr_dict  = {f\"out{i}\": y_tr[:,i] for i in range(y_tr.shape[1])}\n",
    "    y_val_dict = {f\"out{i}\": y_val[:,i] for i in range(y_val.shape[1])}\n",
    "\n",
    "    models, histories, scores = [], [], []\n",
    "    for hp in hyperparams:\n",
    "        tf.keras.backend.clear_session()\n",
    "        m = build_mlp(X_tr.shape[1], output_dims, hp)\n",
    "        h = m.fit(\n",
    "            X_tr, y_tr_dict,\n",
    "            validation_data=(X_val, y_val_dict),\n",
    "            epochs=epochs, batch_size=batch_size, verbose=0\n",
    "        )\n",
    "        ev = m.evaluate(X_val, y_val_dict, verbose=0)\n",
    "        models.append(m)\n",
    "        histories.append((hp, h))\n",
    "        scores.append((hp, ev))\n",
    "        print(f\"üè∑ hp={hp} ‚Üí val_loss={ev[0]:.4f}\")\n",
    "    return models, histories, scores\n",
    "\n",
    "\n",
    "def plot_learning_curves(histories, title):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    for hp,h in histories:\n",
    "        lbl = f\"{hp['units']}u√ó{hp['num_layers']}L dr={hp['dropout']} lr={hp['lr']}\"\n",
    "        plt.plot(h.history[\"val_loss\"], label=lbl)\n",
    "    plt.title(title); plt.xlabel(\"epoch\"); plt.ylabel(\"val_loss\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "def summarize_scores(scores):\n",
    "    rows = []\n",
    "    for hp, ev in scores:\n",
    "        row = {**hp, \"total_val_loss\": ev[0]}\n",
    "        idx = 1\n",
    "        head = 0\n",
    "        while idx<len(ev):\n",
    "            row[f\"o{head}_loss\"] = ev[idx]\n",
    "            row[f\"o{head}_acc\"]  = ev[idx+1]\n",
    "            idx+=2; head+=1\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows).sort_values(\"total_val_loss\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def save_top3(models, scores, prefix):\n",
    "    losses = np.array([s[1][0] for s in scores])\n",
    "    best   = np.argsort(losses)[:3]\n",
    "    for rank,i in enumerate(best,1):\n",
    "        fn = f\"{prefix}_top{rank}.h5\"\n",
    "        models[i].save(fn)\n",
    "        print(f\"üíæ Saved {fn}\")\n",
    "\n",
    "\n",
    "def test_saved_models(\n",
    "    model_paths, preprocessor, label_encoders,\n",
    "    seen_excel_paths, sheet_key,\n",
    "    required_columns, target_columns\n",
    "):\n",
    "    for mp in model_paths:\n",
    "        print(f\"\\nüîé Testing {mp}\")\n",
    "        m = tf.keras.models.load_model(mp)\n",
    "        for xp in seen_excel_paths:\n",
    "            df = pd.read_excel(xp, sheet_name=sheet_key)\n",
    "            df = df[required_columns[sheet_key]]\n",
    "            Xn = preprocessor.transform(df.drop(columns=target_columns))\n",
    "            preds = m.predict(Xn)\n",
    "            out = df.copy()\n",
    "            for i,col in enumerate(target_columns):\n",
    "                le = label_encoders[col]\n",
    "                out[f\"pred_{col}\"] = le.inverse_transform(preds[i].argmax(axis=1))\n",
    "            print(f\"‚Äî on {os.path.basename(xp)}:\")\n",
    "            print(out.head())\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../data/raw_data/maquette_23017.xlsx\n",
      "Loaded sheet: Murs from maquette_23017.xlsx, Shape: (215, 149)\n",
      "Loaded sheet: Sols from maquette_23017.xlsx, Shape: (29, 140)\n",
      "Loaded sheet: Poutres from maquette_23017.xlsx, Shape: (152, 136)\n",
      "Loaded sheet: Poteaux from maquette_23017.xlsx, Shape: (72, 111)\n",
      "Loading: ../../data/raw_data/maquette_23016.xlsx\n",
      "Loaded sheet: Murs from maquette_23016.xlsx, Shape: (1589, 146)\n",
      "Loaded sheet: Sols from maquette_23016.xlsx, Shape: (45, 142)\n",
      "Loaded sheet: Poutres from maquette_23016.xlsx, Shape: (778, 136)\n",
      "Loaded sheet: Poteaux from maquette_23016.xlsx, Shape: (215, 110)\n",
      "Loading: ../../data/raw_data/maquette_23002.xlsx\n",
      "Loaded sheet: Murs from maquette_23002.xlsx, Shape: (345, 94)\n",
      "Loaded sheet: Sols from maquette_23002.xlsx, Shape: (32, 91)\n",
      "Loaded sheet: Poutres from maquette_23002.xlsx, Shape: (96, 89)\n",
      "Loading: ../../data/raw_data/maquette_23007.xlsx\n",
      "Loaded sheet: Murs from maquette_23007.xlsx, Shape: (203, 91)\n",
      "Loaded sheet: Sols from maquette_23007.xlsx, Shape: (41, 82)\n",
      "Loaded sheet: Poutres from maquette_23007.xlsx, Shape: (287, 91)\n",
      "Loaded sheet: Poteaux from maquette_23007.xlsx, Shape: (115, 83)\n",
      "Loading: ../../data/raw_data/RawData-Cibles.xlsx\n",
      "Loaded sheet: Murs from RawData-Cibles.xlsx, Shape: (312, 96)\n",
      "Loaded sheet: Sols from RawData-Cibles.xlsx, Shape: (107, 94)\n",
      "Loaded sheet: Poutres from RawData-Cibles.xlsx, Shape: (246, 100)\n",
      "Loaded sheet: Poteaux from RawData-Cibles.xlsx, Shape: (68, 87)\n",
      "Loading: ../../data/raw_data/maquette_23001.xlsx\n",
      "Loaded sheet: Murs from maquette_23001.xlsx, Shape: (312, 96)\n",
      "Loaded sheet: Sols from maquette_23001.xlsx, Shape: (107, 94)\n",
      "Loaded sheet: Poutres from maquette_23001.xlsx, Shape: (246, 100)\n",
      "Loaded sheet: Poteaux from maquette_23001.xlsx, Shape: (68, 87)\n",
      "\n",
      "üîç Cleaning maquette_23017.xlsx_Murs\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Nom de la famille', 'Nom du type', 'Identifiant', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', 'Composition', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', 'IFCExportAs', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (114 cols)\n",
      "üéØ Final maquette_23017.xlsx_Murs shape: (215, 118)\n",
      "\n",
      "üîç Cleaning maquette_23017.xlsx_Sols\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Identifiant', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '052EC_Jour Coffrage', '053EC_Jour Ferraillage', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', 'Composition', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', 'IFCExportAs', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Fabricant', 'URL', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type']\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Murs coup√©s (u)', 'Murs coup√©s (Ids)', 'Murs coupants (u)', 'Murs coupants (Ids)', 'Poutres coup√©s (u)', 'Poutres coup√©s (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)', 'Poteaux coup√©s (u)', 'Poteaux coup√©s (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (105 cols)\n",
      "üéØ Final maquette_23017.xlsx_Sols shape: (29, 109)\n",
      "\n",
      "üîç Cleaning maquette_23017.xlsx_Poutres\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Niveau', 'Nom de la famille', 'Nom du type', 'Identifiant', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', 'Composition', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', 'IFCExportAs', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (96 cols)\n",
      "üéØ Final maquette_23017.xlsx_Poutres shape: (152, 100)\n",
      "\n",
      "üîç Cleaning maquette_23017.xlsx_Poteaux\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', 'Composition', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', 'IFCExportAs', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', 'Titre OmniClass', 'Num√©ro OmniClass', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (72 cols)\n",
      "üéØ Final maquette_23017.xlsx_Poteaux shape: (72, 76)\n",
      "\n",
      "üîç Cleaning maquette_23016.xlsx_Murs\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', '001EC_Grue', '003EC_Zone', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue d√©tail faible', 'Marque de type', \"Protection contre l'incendie\"]\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "üóëÔ∏è Dropped rows where 011EC_Lot was NaN (0.1%)\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (109 cols)\n",
      "üéØ Final maquette_23016.xlsx_Murs shape: (1587, 113)\n",
      "\n",
      "üîç Cleaning maquette_23016.xlsx_Sols\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Niveau plancher fini', '001EC_Grue', '003EC_Zone', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '052EC_Jour Coffrage', '053EC_Jour Ferraillage', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue d√©tail faible', 'Marque de type']\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Murs coup√©s (u)', 'Murs coup√©s (Ids)', 'Murs coupants (u)', 'Murs coupants (Ids)', 'Poutres coup√©s (u)', 'Poutres coup√©s (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)', 'Poteaux coup√©s (u)', 'Poteaux coup√©s (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (103 cols)\n",
      "üéØ Final maquette_23016.xlsx_Sols shape: (45, 107)\n",
      "\n",
      "üîç Cleaning maquette_23016.xlsx_Poutres\n",
      "üóëÔ∏è  Dups removed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38590/4147864326.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[non_targets] = df[non_targets].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Niveau', 'Nom de la famille', 'Nom du type', '001EC_Grue', '003EC_Zone', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', 'Titre OmniClass', 'Num√©ro OmniClass', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (95 cols)\n",
      "üéØ Final maquette_23016.xlsx_Poutres shape: (778, 99)\n",
      "\n",
      "üîç Cleaning maquette_23016.xlsx_Poteaux\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', \"Marque d'emplacement du poteau\", 'Nom de la famille', 'Nom du type', '001EC_Grue', '003EC_Zone', '021EC_Specificite', '041EC_Option Logetex 1', '042EC_Option Logetex 2', '043EC_Option Logetex 3', '051EC_Date Realisation', '054EC_Jour Coulage', '055EC_Jour Coulage Cumule', '084EC_Clef Planification', '085EC_Clef Grue', '024EC_Finition GO', '056EC_Heure Grue', '087EC_Clef AB', '088EC_Clef BMO', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', 'Titre OmniClass', 'Num√©ro OmniClass', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 30.7% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (70 cols)\n",
      "üéØ Final maquette_23016.xlsx_Poteaux shape: (215, 74)\n",
      "\n",
      "üîç Cleaning maquette_23002.xlsx_Murs\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Identifiant', 'Batiment', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue d√©tail faible', 'Marque de type', \"Protection contre l'incendie\"]\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (70 cols)\n",
      "üéØ Final maquette_23002.xlsx_Murs shape: (345, 74)\n",
      "\n",
      "üîç Cleaning maquette_23002.xlsx_Sols\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Identifiant', 'Batiment', 'Niveau fini', 'Niveau Brut', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue d√©tail faible', 'Marque de type', '√©paisseur']\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Murs coup√©s (u)', 'Murs coup√©s (Ids)', 'Murs coupants (u)', 'Murs coupants (Ids)', 'Poutres coup√©s (u)', 'Poutres coup√©s (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)', 'Poteaux coup√©s (u)', 'Poteaux coup√©s (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (65 cols)\n",
      "üéØ Final maquette_23002.xlsx_Sols shape: (32, 69)\n",
      "\n",
      "üîç Cleaning maquette_23002.xlsx_Poutres\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Niveau', 'Nom de la famille', 'Nom du type', 'Identifiant', 'Batiment', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poteaux coup√©s (u)', 'Poteauc coup√©s (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (63 cols)\n",
      "üéØ Final maquette_23002.xlsx_Poutres shape: (96, 67)\n",
      "\n",
      "üîç Cleaning maquette_23007.xlsx_Murs\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Nom de la famille', 'Nom du type', 'Identifiant', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue d√©tail faible', 'Marque de type', \"Protection contre l'incendie\", 'ID MONTAGE', 'D√©signation syst√®me', 'R√©f DT']\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (67 cols)\n",
      "üéØ Final maquette_23007.xlsx_Murs shape: (203, 71)\n",
      "\n",
      "üîç Cleaning maquette_23007.xlsx_Sols\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Nom de la famille', 'Nom du type', 'Identifiant', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue d√©tail faible', 'Marque de type']\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Murs coup√©s (u)', 'Murs coup√©s (Ids)', 'Murs coupants (u)', 'Murs coupants (Ids)', 'Poutres coup√©s (u)', 'Poutres coup√©s (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)', 'Poteaux coup√©s (u)', 'Poteaux coup√©s (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "üóëÔ∏è Dropped rows where 011EC_Lot was NaN (9.8%)\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38590/4147864326.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[non_targets] = df[non_targets].fillna(0)\n",
      "/tmp/ipykernel_38590/4147864326.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[non_targets] = df[non_targets].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (61 cols)\n",
      "üéØ Final maquette_23007.xlsx_Sols shape: (37, 65)\n",
      "\n",
      "üîç Cleaning maquette_23007.xlsx_Poutres\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Niveau', 'Nom de la famille', 'Nom du type', 'Identifiant', 'NOEMI', 'MILLS_View', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poteaux coup√©s (u)', 'Poteauc coup√©s (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "üóëÔ∏è Dropped rows where 011EC_Lot was NaN (4.5%)\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (65 cols)\n",
      "üéØ Final maquette_23007.xlsx_Poutres shape: (274, 69)\n",
      "\n",
      "üîç Cleaning maquette_23007.xlsx_Poteaux\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', \"Marque d'emplacement du poteau\", 'Nom de la famille', 'Nom du type', 'Identifiant', 'NOEMI', 'MILLS_View', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', \"Description de l'assemblage\", \"Code d'assemblage\", 'Marque de type', 'Motif vue d√©tail faible']\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poutres coup√©s (u)', 'Poutres coup√©s (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (57 cols)\n",
      "üéØ Final maquette_23007.xlsx_Poteaux shape: (115, 61)\n",
      "\n",
      "üîç Cleaning RawData-Cibles.xlsx_Murs\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Batiment', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue d√©tail faible', 'Marque de type', \"Protection contre l'incendie\"]\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (74 cols)\n",
      "üéØ Final RawData-Cibles.xlsx_Murs shape: (312, 78)\n",
      "\n",
      "üîç Cleaning RawData-Cibles.xlsx_Sols\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Nom de la famille', 'Nom du type', 'Batiment', 'Niveau fini', 'Niveau Brut', 'NIVEAU_STRUCTURE', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Type IfcGUID', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue d√©tail faible', 'Marque de type', '√©paisseur']\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Murs coup√©s (u)', 'Murs coup√©s (Ids)', 'Murs coupants (u)', 'Murs coupants (Ids)', 'Poutres coup√©s (u)', 'Poutres coup√©s (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)', 'Poteaux coup√©s (u)', 'Poteaux coup√©s (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "üóëÔ∏è Dropped rows where 012EC_Ouvrage was NaN (0.9%)\n",
      "üóëÔ∏è Dropped rows where 013EC_Localisation was NaN (2.8%)\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (69 cols)\n",
      "üéØ Final RawData-Cibles.xlsx_Sols shape: (103, 73)\n",
      "\n",
      "üîç Cleaning RawData-Cibles.xlsx_Poutres\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Niveau', 'Nom de la famille', 'Nom du type', 'Batiment', 'NIVEAU_STRUCTURE', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', \"Description de l'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poteaux coup√©s (u)', 'Poteaux coup√©s (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "üóëÔ∏è Dropped rows where 011EC_Lot was NaN (1.6%)\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (76 cols)\n",
      "üéØ Final RawData-Cibles.xlsx_Poutres shape: (242, 80)\n",
      "\n",
      "üîç Cleaning RawData-Cibles.xlsx_Poteaux\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Batiment', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', 'Titre OmniClass', 'Num√©ro OmniClass', \"Description de l'assemblage\", 'Marque de type']\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poutres coup√©s (u)', 'Poutres coup√©s (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38590/4147864326.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[non_targets] = df[non_targets].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (63 cols)\n",
      "üéØ Final RawData-Cibles.xlsx_Poteaux shape: (68, 67)\n",
      "\n",
      "üîç Cleaning maquette_23001.xlsx_Murs\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Batiment', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue d√©tail faible', 'Marque de type', \"Protection contre l'incendie\"]\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (74 cols)\n",
      "üéØ Final maquette_23001.xlsx_Murs shape: (312, 78)\n",
      "\n",
      "üîç Cleaning maquette_23001.xlsx_Sols\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Nom de la famille', 'Nom du type', 'Batiment', 'Niveau fini', 'Niveau Brut', 'NIVEAU_STRUCTURE', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Type IfcGUID', 'Fabricant', 'Commentaires du type', 'URL', 'Description', \"Description de l'assemblage\", \"Code d'assemblage\", 'Motif vue d√©tail faible', 'Marque de type', '√©paisseur']\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Murs coup√©s (u)', 'Murs coup√©s (Ids)', 'Murs coupants (u)', 'Murs coupants (Ids)', 'Poutres coup√©s (u)', 'Poutres coup√©s (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)', 'Poteaux coup√©s (u)', 'Poteaux coup√©s (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "üóëÔ∏è Dropped rows where 012EC_Ouvrage was NaN (0.9%)\n",
      "üóëÔ∏è Dropped rows where 013EC_Localisation was NaN (2.8%)\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (69 cols)\n",
      "üéØ Final maquette_23001.xlsx_Sols shape: (103, 73)\n",
      "\n",
      "üîç Cleaning maquette_23001.xlsx_Poutres\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Niveau', 'Nom de la famille', 'Nom du type', 'Batiment', 'NIVEAU_STRUCTURE', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', \"Description de l'assemblage\", 'Marque de type', \"Protection contre l'incendie\"]\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poteaux coup√©s (u)', 'Poteauc coup√©s (Ids)', 'Poteaux coupants (u)', 'Poteaux coupants (Ids)']\n",
      "üóëÔ∏è Dropped rows where 011EC_Lot was NaN (1.6%)\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38590/4147864326.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[non_targets] = df[non_targets].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (76 cols)\n",
      "üéØ Final maquette_23001.xlsx_Poutres shape: (242, 80)\n",
      "\n",
      "üîç Cleaning maquette_23001.xlsx_Poteaux\n",
      "üóëÔ∏è  Dups removed: 0\n",
      "üóëÔ∏è  Dropped 100% NaN cols: [\"Type pr√©d√©fini d'IFC\", 'Exporter au format IFC sous', 'Commentaires', 'Nom de la famille', 'Nom du type', 'Batiment', \"Note d'identification\", \"Type: Type pr√©d√©fini d'IFC\", 'Exporter le type au format IFC sous', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL', 'Description', 'Nom de code', 'Identifiant du nom de la coupe', 'Titre OmniClass', 'Num√©ro OmniClass', \"Description de l'assemblage\", 'Marque de type']\n",
      "‚ÑπÔ∏è  Filled coup√© cols with 0: ['Sols coup√©s (u)', 'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)', 'Poutres coup√©s (u)', 'Poutres coup√©s (Ids)', 'Poutres coupants (u)', 'Poutres coupants (Ids)']\n",
      "‚ÑπÔ∏è  011EC_Lot: 0.0% missing\n",
      "‚ÑπÔ∏è  012EC_Ouvrage: 0.0% missing\n",
      "‚ÑπÔ∏è  013EC_Localisation: 0.0% missing\n",
      "‚ÑπÔ∏è  014EC_Mode Constructif: 0.0% missing\n",
      "‚úÖ Filled NaN‚Üí0 on non‚Äêtargets (63 cols)\n",
      "üéØ Final maquette_23001.xlsx_Poteaux shape: (68, 67)\n",
      "\n",
      "\n",
      "===== PROCESSING maquette_23017.xlsx_Murs =====\n",
      "‚ö†Ô∏è Skipping maquette_23017.xlsx_Murs due to insufficient valid classes in 011EC_Lot.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['011EC_Lot', '012EC_Ouvrage', '013EC_Localisation', '014EC_Mode Constructif'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltered \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Removed classes with fewer than 2 samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# preprocess & split\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m (X_tr, y_tr), (X_val, y_val), (X_te, y_te), preproc, le_dict \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTARGET_COLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m out_dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(le_dict[c]\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m TARGET_COLS]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(df, target_columns, test_size, val_size, random_state)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_dataset\u001b[39m(\n\u001b[1;32m     29\u001b[0m     df,\n\u001b[1;32m     30\u001b[0m     target_columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     34\u001b[0m ):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    - Splits df into X, y\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    - LabelEncodes each target\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    - Preprocesses X\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    - Splits into train/val/test\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     y \u001b[38;5;241m=\u001b[39m df[target_columns]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# encode Y\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/BImpredict2/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/BImpredict2/lib/python3.10/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/BImpredict2/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/BImpredict2/lib/python3.10/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/.pyenv/versions/BImpredict2/lib/python3.10/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/BImpredict2/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['011EC_Lot', '012EC_Ouvrage', '013EC_Localisation', '014EC_Mode Constructif'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ 4. MAIN EXECUTION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) load your raw Excel sheets however you like:\n",
    "    # Load all Excel files in RAW_DATA_DIR\n",
    "    raw_dfs = {}\n",
    "    for file in excel_files:\n",
    "        file_path = os.path.join(RAW_DATA_DIR, file)\n",
    "        print(f\"Loading: {file_path}\")\n",
    "        try:\n",
    "            excel_data = pd.ExcelFile(file_path)\n",
    "            for sheet_name in excel_data.sheet_names:\n",
    "                df_name = f\"{file}_{sheet_name}\"\n",
    "                raw_dfs[df_name] = excel_data.parse(sheet_name)\n",
    "                print(f\"Loaded sheet: {sheet_name} from {file}, Shape: {raw_dfs[df_name].shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "    # 2) your parameters\n",
    "    EXC_KEYWORDS      = [\"coup√©s\", \"coupants\"]\n",
    "    TARGET_COLS       = [\"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\"]\n",
    "    SEEN_FILES        = [\"seen1.xlsx\", \"seen2.xlsx\"]\n",
    "    HYPERPARAM_GRID   = [\n",
    "        {\"num_layers\": 1, \"units\": 64, \"dropout\": 0.0, \"lr\": 1e-3},\n",
    "        {\"num_layers\": 2, \"units\": 64, \"dropout\": 0.2, \"lr\": 1e-3},\n",
    "        {\"num_layers\": 2, \"units\": 128, \"dropout\": 0.3, \"lr\": 5e-4},\n",
    "        {\"num_layers\": 3, \"units\": 256, \"dropout\": 0.4, \"lr\": 1e-4},\n",
    "    ]\n",
    "\n",
    "    # 3) CLEAN\n",
    "    cleaned = {}\n",
    "    for name, df in raw_dfs.items():\n",
    "        print(f\"\\nüîç Cleaning {name}\")\n",
    "        df2 = drop_duplicates_and_missing(df, EXC_KEYWORDS)\n",
    "        df3 = ensure_targets(df2, TARGET_COLS)\n",
    "        df4 = fill_non_targets(df3, TARGET_COLS)\n",
    "        cleaned[name] = df4\n",
    "        print(f\"üéØ Final {name} shape: {df4.shape}\")\n",
    "\n",
    "    # 4) FOR EACH SHEET ‚Üí preprocess, train, eval, save, test\n",
    "    for name, df in cleaned.items():\n",
    "        print(f\"\\n\\n===== PROCESSING {name} =====\")\n",
    "\n",
    "        # Ensure all classes in the target columns have at least two samples\n",
    "        for target in TARGET_COLS:\n",
    "            class_counts = df[target].value_counts()\n",
    "            valid_classes = class_counts[class_counts >= 2].index\n",
    "            if len(valid_classes) < 2:\n",
    "                print(f\"‚ö†Ô∏è Skipping {name} due to insufficient valid classes in {target}.\")\n",
    "                df = pd.DataFrame()  # Clear the dataframe to avoid further processing\n",
    "                break\n",
    "            df = df[df[target].isin(valid_classes)]\n",
    "            print(f\"Filtered {target}: Removed classes with fewer than 2 samples.\")\n",
    "\n",
    "        # preprocess & split\n",
    "        (X_tr, y_tr), (X_val, y_val), (X_te, y_te), preproc, le_dict = prepare_dataset(\n",
    "            df, TARGET_COLS, test_size=0.3, val_size=0.5, random_state=42\n",
    "        )\n",
    "        out_dims = [len(le_dict[c].classes_) for c in TARGET_COLS]\n",
    "\n",
    "        # train\n",
    "        models, hists, scores = train_models(\n",
    "            X_tr, y_tr, X_val, y_val,\n",
    "            output_dims=out_dims,\n",
    "            hyperparams=HYPERPARAM_GRID,\n",
    "            epochs=40\n",
    "        )\n",
    "\n",
    "        # visualize & table\n",
    "        plot_learning_curves(hists, title=f\"{name} val_loss\")\n",
    "        df_scores = summarize_scores(scores)\n",
    "        print(df_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Save & test\n",
    "        prefix = sheet.lower()\n",
    "        save_top3(models, scores, os.path.join(DL_MODELS_DIR, prefix))\n",
    "        best_models = [os.path.join(DL_MODELS_DIR, f\"{prefix}_top{i}.h5\") for i in (1, 2, 3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # List all Excel files in TESTING_DATA_DIR\n",
    "        testing_excel_files = [f for f in os.listdir(TESTING_DATA_DIR) if f.endswith(\".xlsx\") or f.endswith(\".xls\")]\n",
    "\n",
    "        # Test saved models and save predictions\n",
    "        for model_path in best_models:\n",
    "            print(f\"\\nüîé Testing model: {model_path}\")\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "            for test_file in testing_excel_files:\n",
    "                test_file_path = os.path.join(TESTING_DATA_DIR, test_file)\n",
    "                print(f\"Processing test file: {test_file_path}\")\n",
    "\n",
    "                try:\n",
    "                    # Load the Excel file\n",
    "                    test_excel_data = pd.ExcelFile(test_file_path)\n",
    "\n",
    "                    # Process each sheet in the Excel file\n",
    "                    for sheet_name in test_excel_data.sheet_names:\n",
    "                        if sheet_name.lower() == sheet.lower():  # Match the category dynamically\n",
    "                            test_df = test_excel_data.parse(sheet_name)\n",
    "\n",
    "                            # Filter required columns\n",
    "                            test_df = test_df[required_columns[sheet]]\n",
    "\n",
    "                            # Preprocess the test data\n",
    "                            X_test = preproc.transform(test_df.drop(columns=TARGET_COLUMNS))\n",
    "\n",
    "                            # Predict using the model\n",
    "                            predictions = model.predict(X_test)\n",
    "\n",
    "                            # Add predictions to the DataFrame\n",
    "                            for i, target in enumerate(TARGET_COLUMNS):\n",
    "                                test_df[f\"pred_{target}\"] = le_dict[target].inverse_transform(predictions[i].argmax(axis=1))\n",
    "\n",
    "                            # Save the predictions to PREDICTED_DATA_DIR\n",
    "                            predicted_file_path = os.path.join(PREDICTED_DATA_DIR, f\"{test_file}_{sheet_name}_predictions.xlsx\")\n",
    "                            test_df.to_excel(predicted_file_path, index=False)\n",
    "                            print(f\"‚úÖ Predictions saved to: {predicted_file_path}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error processing {test_file_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BImpredict2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
