{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1fae56",
   "metadata": {},
   "source": [
    "# Bim_Predict NoteBook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08c1ec",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227613f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: ../../data/raw_data\n",
      "Directory already exists: ../../data/processed_data\n",
      "Directory already exists: ../../data/predicting_data\n",
      "Directory already exists: ../../models\n",
      "Directory already exists: ../../models/SK/machine_learning\n",
      "Directory already exists: ../../models/SK/deep_learning\n",
      "Directory already exists: ../../models/SK/other\n",
      "Directory already exists: ../../python_modules\n",
      "Directory already exists: ../../plots\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define project folder paths\n",
    "# Data directories\n",
    "BASE_DIR = \"../../\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "PREDICTED_DATA_DIR = os.path.join(DATA_DIR, \"predicting_data\")\n",
    "TESTING_DATA_DIR = os.path.join(DATA_DIR, \"testing_data\")\n",
    "\n",
    "# Model directories\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ML_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/machine_learning\")\n",
    "DL_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/deep_learning\")\n",
    "OTHER_MODELS_DIR = os.path.join(MODELS_DIR, \"SK/other\")\n",
    "\n",
    "# Python modules and plots directories\n",
    "PYTHON_MODULES_DIR = os.path.join(BASE_DIR, \"python_modules\")\n",
    "PLOTS_DIR = os.path.join(BASE_DIR, \"plots\")\n",
    "\n",
    "# List of directories to create\n",
    "directories = [\n",
    "    RAW_DATA_DIR, PROCESSED_DATA_DIR, PREDICTED_DATA_DIR,\n",
    "    MODELS_DIR, ML_MODELS_DIR, DL_MODELS_DIR, OTHER_MODELS_DIR,\n",
    "    PYTHON_MODULES_DIR, PLOTS_DIR\n",
    "]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e384cf",
   "metadata": {},
   "source": [
    "<!-- ### Paths Creating && Data Importing -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4763e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../data/raw_data/maquette_23017.xlsx\n",
      "Loading: ../../data/raw_data/maquette_23016.xlsx\n",
      "Loading: ../../data/raw_data/maquette_23002.xlsx\n",
      "Loading: ../../data/raw_data/maquette_23007.xlsx\n",
      "Loading: ../../data/raw_data/RawData-Cibles.xlsx\n",
      "Loading: ../../data/raw_data/maquette_23001.xlsx\n",
      "\n",
      "Total files processed: 23\n",
      "Loaded DataFrame: maquette_23017.xlsx_Murs, Shape: (215, 149)\n",
      "Loaded DataFrame: maquette_23017.xlsx_Sols, Shape: (29, 140)\n",
      "Loaded DataFrame: maquette_23017.xlsx_Poutres, Shape: (152, 136)\n",
      "Loaded DataFrame: maquette_23017.xlsx_Poteaux, Shape: (72, 111)\n",
      "Loaded DataFrame: maquette_23016.xlsx_Murs, Shape: (1589, 146)\n",
      "Loaded DataFrame: maquette_23016.xlsx_Sols, Shape: (45, 142)\n",
      "Loaded DataFrame: maquette_23016.xlsx_Poutres, Shape: (778, 136)\n",
      "Loaded DataFrame: maquette_23016.xlsx_Poteaux, Shape: (215, 110)\n",
      "Loaded DataFrame: maquette_23002.xlsx_Murs, Shape: (345, 94)\n",
      "Loaded DataFrame: maquette_23002.xlsx_Sols, Shape: (32, 91)\n",
      "Loaded DataFrame: maquette_23002.xlsx_Poutres, Shape: (96, 89)\n",
      "Loaded DataFrame: maquette_23007.xlsx_Murs, Shape: (203, 91)\n",
      "Loaded DataFrame: maquette_23007.xlsx_Sols, Shape: (41, 82)\n",
      "Loaded DataFrame: maquette_23007.xlsx_Poutres, Shape: (287, 91)\n",
      "Loaded DataFrame: maquette_23007.xlsx_Poteaux, Shape: (115, 83)\n",
      "Loaded DataFrame: RawData-Cibles.xlsx_Murs, Shape: (312, 96)\n",
      "Loaded DataFrame: RawData-Cibles.xlsx_Sols, Shape: (107, 94)\n",
      "Loaded DataFrame: RawData-Cibles.xlsx_Poutres, Shape: (246, 100)\n",
      "Loaded DataFrame: RawData-Cibles.xlsx_Poteaux, Shape: (68, 87)\n",
      "Loaded DataFrame: maquette_23001.xlsx_Murs, Shape: (312, 96)\n",
      "Loaded DataFrame: maquette_23001.xlsx_Sols, Shape: (107, 94)\n",
      "Loaded DataFrame: maquette_23001.xlsx_Poutres, Shape: (246, 100)\n",
      "Loaded DataFrame: maquette_23001.xlsx_Poteaux, Shape: (68, 87)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# List all Excel files in RAW_DATA_DIR\n",
    "excel_files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith(\".xlsx\") or f.endswith(\".xls\")]\n",
    "\n",
    "# Dictionary to store DataFrames for each file and sheet\n",
    "dataframes = {}\n",
    "\n",
    "# Process each Excel file\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(RAW_DATA_DIR, file)\n",
    "    print(f\"Loading: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # Load Excel file\n",
    "        excel_data = pd.ExcelFile(file_path)\n",
    "\n",
    "        # Load all sheets dynamically\n",
    "        for sheet_name in excel_data.sheet_names:\n",
    "            df = excel_data.parse(sheet_name)\n",
    "\n",
    "            # Save DataFrame with a unique identifier\n",
    "            dataframes[f\"{file}_{sheet_name}\"] = df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# Display summary of loaded data\n",
    "print(f\"\\nTotal files processed: {len(dataframes)}\")\n",
    "for key, df in dataframes.items():\n",
    "    print(f\"Loaded DataFrame: {key}, Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1514a",
   "metadata": {},
   "source": [
    "<!-- ### Data Cleaning && PreProcessing -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc959f",
   "metadata": {},
   "source": [
    "## PreProcessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b36ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define required columns dynamically\n",
    "# required_columns = {\n",
    "#     \"Murs\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"Hauteur\",\n",
    "#              \"Epaisseur\", \"AI\", \"AS\", \"Sols en intersection\", \"Sols coupÃ©s (u)\", \"Sols coupÃ©s (Ids)\",\n",
    "#              \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Sol au-dessus\", \"Sol au-dessous\", \"FenÃªtres\", \"Portes\",\n",
    "#              \"Ouvertures\", \"Murs imbriquÃ©s\", \"Mur multicouche\", \"Mur empilÃ©\", \"Profil modifiÃ©\", \"Extension infÃ©rieure\",\n",
    "#              \"Extension supÃ©rieure\", \"Partie infÃ©rieure attachÃ©e\", \"Partie supÃ©rieure attachÃ©e\", \"DÃ©calage supÃ©rieur\",\n",
    "#              \"DÃ©calage infÃ©rieur\", \"MatÃ©riau structurel\"],\n",
    "\n",
    "#     \"Sols\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"Murs en intersection\",\n",
    "#              \"Murs coupÃ©s (u)\", \"Murs coupÃ©s (Ids)\", \"Murs coupants (u)\", \"Murs coupants (Ids)\", \"Poutres en intersection\",\n",
    "#              \"Poutres coupÃ©s (u)\", \"Poutres coupÃ©s (Ids)\", \"Poutres coupants (u)\", \"Poutres coupants (Ids)\",\n",
    "#              \"Poteaux en intersection\", \"Poteaux coupÃ©s (u)\", \"Poteaux coupÃ©s (Ids)\", \"Poteaux coupants (u)\",\n",
    "#              \"Poteaux coupants (Ids)\", \"Ouvertures\", \"Sol multicouche\", \"Profil modifiÃ©\", \"DÃ©calage par rapport au niveau\",\n",
    "#              \"Epaisseur\", \"LiÃ© au volume\", \"Etude de l'Ã©lÃ©vation Ã  la base\", \"Etude de l'Ã©lÃ©vation en haut\",\n",
    "#              \"Epaisseur du porteur\", \"ElÃ©vation au niveau du noyau infÃ©rieur\", \"ElÃ©vation au niveau du noyau supÃ©rieur\",\n",
    "#              \"ElÃ©vation en haut\", \"ElÃ©vation Ã  la base\", \"MatÃ©riau structurel\"],\n",
    "\n",
    "#     \"Poutres\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"AI\", \"AS\",\n",
    "#                 \"Hauteur totale\", \"Hauteur\", \"Sols en intersection\", \"Sols coupÃ©s (u)\", \"Sols coupÃ©s (Ids)\",\n",
    "#                 \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Sol au-dessus\", \"Sol au-dessous\", \"Poteaux en intersection\",\n",
    "#                 \"Poteaux coupÃ©s (u)\", \"Poteaux coupÃ©s (Ids)\", \"Poteaux coupants (u)\", \"Poteaux coupants (Ids)\",\n",
    "#                 \"Etat de la jonction\", \"Valeur de dÃ©calage Z\", \"Justification Z\", \"Valeur de dÃ©calage Y\", \"Justification Y\",\n",
    "#                 \"Justification YZ\", \"MatÃ©riau structurel\", \"ElÃ©vation du niveau de rÃ©fÃ©rence\", \"ElÃ©vation en haut\",\n",
    "#                 \"Rotation de la section\", \"Orientation\", \"DÃ©calage du niveau d'arrivÃ©e\", \"DÃ©calage du niveau de dÃ©part\",\n",
    "#                 \"ElÃ©vation Ã  la base\", \"Longueur de coupe\", \"Longueur\", \"hauteur_section\", \"largeur_section\"],\n",
    "\n",
    "#     \"Poteaux\": [\"Id\", \"011EC_Lot\", \"012EC_Ouvrage\", \"013EC_Localisation\", \"014EC_Mode Constructif\", \"AI\", \"AS\",\n",
    "#                 \"Hauteur\", \"Longueur\", \"Partie infÃ©rieure attachÃ©e\", \"Partie supÃ©rieure attachÃ©e\", \"Sols en intersection\",\n",
    "#                 \"Sols coupÃ©s (u)\", \"Sols coupÃ©s (Ids)\", \"Sols coupants (u)\", \"Sols coupants (Ids)\", \"Poutres en intersection\",\n",
    "#                 \"Poutres coupÃ©s (u)\", \"Poutres coupÃ©s (Ids)\", \"Poutres coupants (u)\", \"Poutres coupants (Ids)\",\n",
    "#                 \"MatÃ©riau structurel\", \"DÃ©calage supÃ©rieur\", \"DÃ©calage infÃ©rieur\", \"DiamÃ¨tre poteau\", \"h\", \"b\",\n",
    "#                 \"hauteur_section\", \"largeur_section\"]\n",
    "# }\n",
    "\n",
    "# # Initialize a dictionary to store filtered dataframes\n",
    "# cleaned_dataframes = {}\n",
    "\n",
    "# for df_name, df in dataframes.items():\n",
    "#     print(f\"\\nðŸŸ¢ Original shape of {df_name}: {df.shape}\")\n",
    "\n",
    "#     # Automatically detect the correct category for filtering\n",
    "#     for category, columns in required_columns.items():\n",
    "#         if category.lower() in df_name.lower():  # Match dynamically\n",
    "#             try:\n",
    "#                 filtered_df = df[columns]  # Keep only the required columns\n",
    "#             except KeyError as e:\n",
    "#                 missing_columns = set(columns) - set(df.columns)\n",
    "#                 print(f\"âš ï¸ Missing columns in {df_name}: {missing_columns}. Skipping this dataframe.\")\n",
    "#                 continue\n",
    "#             cleaned_dataframes[df_name] = filtered_df\n",
    "#             print(f\"âœ… Shape after filtering {df_name}: {filtered_df.shape}\")\n",
    "#             break  # Stop looping once the correct match is found\n",
    "#     else:\n",
    "#         print(f\"âš ï¸ No matching category for {df_name}, skipping filtering.\")\n",
    "\n",
    "# # Add prefixes to column names based on the dataframe category and update index\n",
    "# for name, df in cleaned_dataframes.items():\n",
    "#     if \"murs\" in name.lower():\n",
    "#         prefix = \"murs_\"\n",
    "#     elif \"sols\" in name.lower():\n",
    "#         prefix = \"sols_\"\n",
    "#     elif \"poutres\" in name.lower():\n",
    "#         prefix = \"poutres_\"\n",
    "#     elif \"poteaux\" in name.lower():\n",
    "#         prefix = \"poteaux_\"\n",
    "#     else:\n",
    "#         prefix = \"\"\n",
    "\n",
    "#     # Rename columns with the prefix\n",
    "#     df.rename(columns=lambda col: f\"{prefix}{col}\" if col.lower() != \"id\" else f\"{prefix}id\", inplace=True)\n",
    "\n",
    "#     # Drop the existing index and set the prefixed ID column as the new index\n",
    "#     id_column = f\"{prefix}id\"\n",
    "#     if id_column in df.columns:\n",
    "#         df.set_index(id_column, inplace=True)\n",
    "#         print(f\"âœ… Set '{id_column}' as index for {name}.\")\n",
    "#     else:\n",
    "#         print(f\"âš ï¸ '{id_column}' column not found in {name}, skipping index setting.\")\n",
    "\n",
    "    # Update the cleaned_dataframes dictionary\n",
    "    # cleaned_dataframes[df_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a81fe7",
   "metadata": {},
   "source": [
    "## Deep-Learning Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eabcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# # Define target columns\n",
    "# TARGET_COLUMNS = ['011EC_Lot', '012EC_Ouvrage', '013EC_Localisation', '014EC_Mode Constructif']\n",
    "\n",
    "# # Combine all dataframes into a single dataset\n",
    "# combined_df = pd.concat(dataframes.values(), ignore_index=True)\n",
    "\n",
    "# # Drop rows with missing target values\n",
    "# combined_df = combined_df.dropna(subset=TARGET_COLUMNS)\n",
    "\n",
    "# # Separate features and targets\n",
    "# X = combined_df.drop(columns=TARGET_COLUMNS)\n",
    "# y = combined_df[TARGET_COLUMNS]\n",
    "\n",
    "# # Encode categorical target columns\n",
    "# label_encoders = {}\n",
    "# for col in TARGET_COLUMNS:\n",
    "#     le = LabelEncoder()\n",
    "#     y[col] = le.fit_transform(y[col])\n",
    "#     label_encoders[col] = le\n",
    "\n",
    "# # Standardize numerical features\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X.select_dtypes(include=[np.number]))\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Build the deep learning model\n",
    "# model = Sequential([\n",
    "#     Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(len(TARGET_COLUMNS), activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# # Convert target columns to numpy arrays for compatibility with the model\n",
    "# y_train_arrays = [y_train[col].values for col in TARGET_COLUMNS]\n",
    "# y_test_arrays = [y_test[col].values for col in TARGET_COLUMNS]\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     X_train,\n",
    "#     y_train_arrays,\n",
    "#     epochs=50,\n",
    "#     batch_size=32,\n",
    "#     validation_split=0.2\n",
    "# )\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = model.evaluate(X_test, [y_test[col] for col in TARGET_COLUMNS])\n",
    "# print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ba59b",
   "metadata": {},
   "source": [
    "i want a machine learning model that is able to detetct relation between all the feateaurs in any dataframe and these columns : TARGET_COLUMNS = ['011EC_Lot', '012EC_Ouvrage', '013EC_Localisation', '014EC_Mode Constructif']\n",
    "the data is in french and might have integer and float and text maybe you could use a hybride or nlp or anything else but it should be able to predict and auto complet the missing values and even create the columns if not there ... if there are multiples functions seperat them in mutliple cells so if one of them break we are able to repair ... keep the code smart and refined and the show the model results and learning curves with the capacity to train on the data in raw_data and test its capacity at the end on the data in the testing data dir\n",
    "\n",
    "i want every thing from A to Z  & and make every function in a different cell \n",
    "\n",
    "from preprocceing to the models to hyper params and visualing the models and testing on the testing data in test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646eec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "TARGET_COLUMNS = ['011EC_Lot', '012EC_Ouvrage', '013EC_Localisation', '014EC_Mode Constructif']\n",
    "\n",
    "def ensure_target_columns(df, target_columns=TARGET_COLUMNS):\n",
    "    \"\"\"Ensure all target columns exist in the dataframe, create with NaN if missing.\"\"\"\n",
    "    for col in target_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "    return df\n",
    "\n",
    "def preprocess_dataframe(df, target_columns=TARGET_COLUMNS, fit_encoders=None):\n",
    "    \"\"\"\n",
    "    Preprocess a dataframe:\n",
    "    - Ensures target columns exist\n",
    "    - Imputes missing values\n",
    "    - Encodes categorical/text features\n",
    "    Returns: X, y, fitted_encoders\n",
    "    \"\"\"\n",
    "    df = ensure_target_columns(df, target_columns)\n",
    "    y = df[target_columns].copy()\n",
    "    X = df.drop(columns=target_columns, errors='ignore').copy()\n",
    "\n",
    "    # Separate types\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Impute numeric\n",
    "    num_imputer = SimpleImputer(strategy='mean')\n",
    "    X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
    "\n",
    "    # Impute categorical\n",
    "    cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "    X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
    "\n",
    "    # Encode categorical/text columns\n",
    "    encoders = fit_encoders or {}\n",
    "    for col in cat_cols:\n",
    "        # Use TF-IDF for long text, LabelEncoder for short categorical\n",
    "        if X[col].astype(str).str.len().mean() > 20:\n",
    "            # Text column\n",
    "            if col not in encoders:\n",
    "                encoders[col] = TfidfVectorizer(max_features=20)\n",
    "                tfidf = encoders[col].fit_transform(X[col])\n",
    "            else:\n",
    "                tfidf = encoders[col].transform(X[col])\n",
    "            tfidf_df = pd.DataFrame(tfidf.toarray(), columns=[f\"{col}_tfidf_{i}\" for i in range(tfidf.shape[1])])\n",
    "            X = X.drop(columns=[col])\n",
    "            X = pd.concat([X.reset_index(drop=True), tfidf_df], axis=1)\n",
    "        else:\n",
    "            # Categorical column\n",
    "            if col not in encoders:\n",
    "                encoders[col] = LabelEncoder()\n",
    "                X[col] = encoders[col].fit_transform(X[col])\n",
    "            else:\n",
    "                X[col] = encoders[col].transform(X[col])\n",
    "    return X, y, encoders\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BImpredict2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
