{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a3dbdd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Testing Raw Cible WB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b088204a",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed4dcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in /home/psyko/.pyenv/versions/3.10.6/envs/BImpredict2/lib/python3.10/site-packages (63.2.0)\n",
      "Requirement already satisfied: wheel in /home/psyko/.pyenv/versions/3.10.6/envs/BImpredict2/lib/python3.10/site-packages (0.45.1)\n",
      "Requirement already satisfied: cython in /home/psyko/.pyenv/versions/3.10.6/envs/BImpredict2/lib/python3.10/site-packages (3.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: statsmodels in /home/psyko/.pyenv/versions/3.10.6/envs/BImpredict2/lib/python3.10/site-packages (0.14.4)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/psyko/.pyenv/versions/3.10.6/envs/BImpredict2/lib/python3.10/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /home/psyko/.pyenv/versions/3.10.6/envs/BImpredict2/lib/python3.10/site-packages (from statsmodels) (1.26.4)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/psyko/.pyenv/versions/3.10.6/envs/BImpredict2/lib/python3.10/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /home/psyko/.pyenv/versions/3.10.6/envs/BImpredict2/lib/python3.10/site-packages (from statsmodels) (1.5.3)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /home/psyko/.pyenv/versions/3.10.6/envs/BImpredict2/lib/python3.10/site-packages (from statsmodels) (1.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/psyko/.pyenv/versions/3.10.6/envs/BImpredict2/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/psyko/.pyenv/versions/3.10.6/envs/BImpredict2/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/psyko/.pyenv/versions/3.10.6/envs/BImpredict2/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "### Dowload and install dependencies for the project\n",
    "# %matplotlib inline\n",
    "%pip install setuptools wheel cython\n",
    "%pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dfa8ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 15:05:08.101527: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-04 15:05:08.335623: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-06-04 15:05:08.335721: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-06-04 15:05:08.385876: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-04 15:05:09.751982: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-06-04 15:05:09.752233: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-06-04 15:05:09.752250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "### ====================\n",
    "### Importing libraries\n",
    "### ====================\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#import shap\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "### ====================\n",
    "### Set up visualization and warnings\n",
    "### ====================\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', None)\n",
    "# sns.set_palette('viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d491bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ====================\n",
    "### File paths\n",
    "###  ===================\n",
    "# Create directories for model saving\n",
    "models_dir = '../../models'\n",
    "for model_type in ['simple_models', 'ml_models', 'dl_models']:\n",
    "    models_dir = os.path.join(models_dir, model_type)\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Load Excel file\n",
    "maquettes_path = \"../../data/raw/\"\n",
    "maquettes= [\"RawData-Cibles.xlsx\"]\n",
    "for maquette in maquettes:\n",
    "    maquettes_path = os.path.join(maquettes_path, maquette)\n",
    "sheets = [\"Mur\", \"Sols\", \"Poutre\", \"Poteaux\"]  # Adjusted based on your description\n",
    "\n",
    "\n",
    "## Import Pythoon Modules\n",
    "import sys\n",
    "sys.path.append(\"../../module_python\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbf0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from the Excel file.\n",
      "Murs DataFrame Shape: (312, 96)\n",
      "Sols DataFrame Shape: (107, 94)\n",
      "Poutres DataFrame Shape: (246, 100)\n",
      "Poteaux DataFrame Shape: (68, 87)\n",
      "\n",
      "Murs DataFrame Preview:\n",
      "Index(['Id', '011EC_Lot', '012EC_Ouvrage', '013EC_Localisation',\n",
      "       '014EC_Mode Constructif', 'Nom', 'Hauteur', 'Epaisseur', 'AI', 'AS',\n",
      "       'Sols en intersection', 'Sols coup√©s (u)', 'Sols coup√©s (Ids)',\n",
      "       'Sols coupants (u)', 'Sols coupants (Ids)', 'Sol au-dessus',\n",
      "       'Sol en-dessous', 'Fen√™tres', 'Portes', 'Ouvertures', 'Murs imbriqu√©s',\n",
      "       'Mur multicouche', 'Mur empil√©', 'Profil modifi√©', 'Image', 'Cat√©gorie',\n",
      "       'Section', 'Type pr√©d√©fini d'IFC', 'Exporter au format IFC sous',\n",
      "       'Exporter au format IFC', 'IfcGUID', 'A une association',\n",
      "       'Enrobage d'armature - Autres faces',\n",
      "       'Enrobage d'armature - Face int√©rieure',\n",
      "       'Enrobage d'armature - Face ext√©rieure', 'Variantes',\n",
      "       'Extension inf√©rieure', 'Extension sup√©rieure', 'Volume', 'Surface',\n",
      "       'Phase de d√©molition', 'Phase de cr√©ation', 'Commentaires', 'Longueur',\n",
      "       'Famille et type', 'Famille', 'Type', 'Nom de la famille',\n",
      "       'Nom du type', 'ID du type', 'Li√© au volume', 'Structure',\n",
      "       'Identifiant', 'Ligne de justification', 'Utilisation structurelle',\n",
      "       'Partie inf√©rieure attach√©e', 'Partie sup√©rieure attach√©e',\n",
      "       'D√©calage sup√©rieur', 'D√©calage inf√©rieur', 'Contrainte inf√©rieure',\n",
      "       'Hauteur non contrainte', 'Contrainte sup√©rieure', 'Limite de pi√®ce',\n",
      "       'Nature_Ouvrage', 'Batiment', 'Mur arm√©', 'Affichage poteau',\n",
      "       'NIVEAU_STRUCTURE', 'Image du type', 'Note d'identification',\n",
      "       'Type: Type pr√©d√©fini d'IFC', 'Exporter le type au format IFC sous',\n",
      "       'Exporter le type au format IFC', 'Type IfcGUID', 'Mod√®le', 'Fabricant',\n",
      "       'Commentaires du type', 'URL', 'Description', 'Mat√©riau structurel',\n",
      "       'Rugosit√©', 'Coefficient d'absorbance', 'Masse thermique',\n",
      "       'R√©sistance thermique (R)', 'Coefficient de transfert thermique (U)',\n",
      "       'Description de l'assemblage', 'Code d'assemblage',\n",
      "       'Retournement aux insertions', 'Retournement aux extr√©mit√©s',\n",
      "       'Couleur vue d√©tail faible', 'Motif vue d√©tail faible',\n",
      "       'Marque de type', 'Protection contre l'incendie', 'Co√ªt', 'Fonction',\n",
      "       'Largeur'],\n",
      "      dtype='object')\n",
      "\n",
      "Sols DataFrame Preview:\n",
      "Index(['Id', '011EC_Lot', '012EC_Ouvrage', '013EC_Localisation',\n",
      "       '014EC_Mode Constructif', 'Nom', 'Murs en intersection',\n",
      "       'Murs coup√©s (u)', 'Murs coup√©s (Ids)', 'Murs coupants (u)',\n",
      "       'Murs coupants (Ids)', 'Poutres en intersection', 'Poutres coup√©s (u)',\n",
      "       'Poutres coup√©s (Ids)', 'Poutres coupants (u)',\n",
      "       'Poutres coupants (Ids)', 'Poteaux en intersection',\n",
      "       'Poteaux coup√©s (u)', 'Poteaux coup√©s (Ids)', 'Poteaux coupants (u)',\n",
      "       'Poteaux coupants (Ids)', 'Ouvertures', 'Sol multicouche',\n",
      "       'Profil modifi√©', 'Image', 'Cat√©gorie', 'Type pr√©d√©fini d'IFC',\n",
      "       'Exporter au format IFC sous', 'Exporter au format IFC', 'IfcGUID',\n",
      "       'A une association', 'Enrobage d'armature - Face inf√©rieure',\n",
      "       'Enrobage d'armature - Face sup√©rieure',\n",
      "       'Enrobage d'armature - Autres faces', 'Variantes', 'Volume', 'Surface',\n",
      "       'Phase de d√©molition', 'Phase de cr√©ation', 'Commentaires',\n",
      "       'Inclinaison', 'Niveau', 'Famille et type', 'Famille', 'Type',\n",
      "       'Nom de la famille', 'Nom du type', 'ID du type', 'Structure',\n",
      "       'P√©rim√®tre', 'D√©calage par rapport au niveau', 'Epaisseur',\n",
      "       'Li√© au volume', 'Etude de l'√©l√©vation √† la base',\n",
      "       'Etude de l'√©l√©vation en haut', 'Epaisseur du porteur',\n",
      "       'El√©vation au niveau du noyau inf√©rieur',\n",
      "       'El√©vation au niveau du noyau sup√©rieur', 'El√©vation en haut',\n",
      "       'El√©vation √† la base', 'Identifiant', 'Limite de pi√®ce',\n",
      "       'Nature_Ouvrage', 'Batiment', 'Niveau fini', 'Niveau Brut',\n",
      "       'NIVEAU_STRUCTURE', 'Image du type', 'Note d'identification',\n",
      "       'Type: Type pr√©d√©fini d'IFC', 'Exporter le type au format IFC sous',\n",
      "       'Exporter le type au format IFC', 'Type IfcGUID', 'Mod√®le', 'Fabricant',\n",
      "       'Commentaires du type', 'URL', 'Description', 'Mat√©riau structurel',\n",
      "       'Rugosit√©', 'Coefficient d'absorbance', 'Masse thermique',\n",
      "       'R√©sistance thermique (R)', 'Coefficient de transfert thermique (U)',\n",
      "       'Description de l'assemblage', 'Code d'assemblage',\n",
      "       'Couleur vue d√©tail faible', 'Motif vue d√©tail faible',\n",
      "       'Epaisseur par d√©faut', 'Marque de type', 'Co√ªt', 'Fonction',\n",
      "       '√©paisseur', 'Condition de bord incurv√©'],\n",
      "      dtype='object')\n",
      "\n",
      "Poutres DataFrame Preview:\n",
      "Index(['Id', '011EC_Lot', '012EC_Ouvrage', '013EC_Localisation',\n",
      "       '014EC_Mode Constructif', 'Nom', 'AI', 'AS', 'Hauteur totale',\n",
      "       'Hauteur', 'Sols en intersection', 'Sols coup√©s (u)',\n",
      "       'Sols coup√©s (Ids)', 'Sols coupants (u)', 'Sols coupants (Ids)',\n",
      "       'Sol au-dessus', 'Sol en-dessous', 'Poteaux en intersection',\n",
      "       'Poteaux coup√©s (u)', 'Poteaux coup√©s (Ids)', 'Poteaux coupants (u)',\n",
      "       'Poteaux coupants (Ids)', 'Image', 'Etat de la jonction',\n",
      "       'Valeur de d√©calage Z', 'Justification Z', 'Valeur de d√©calage Y',\n",
      "       'Justification Y', 'Justification YZ', 'Cat√©gorie',\n",
      "       'Type pr√©d√©fini d'IFC', 'Exporter au format IFC sous',\n",
      "       'Exporter au format IFC', 'IfcGUID', 'A une association',\n",
      "       'Enrobage d'armature - Face inf√©rieure',\n",
      "       'Enrobage d'armature - Face sup√©rieure',\n",
      "       'Enrobage d'armature - Autres faces', 'Variantes', 'Volume',\n",
      "       'Phase de d√©molition', 'Phase de cr√©ation', 'Commentaires',\n",
      "       'Mat√©riau structurel', 'ID h√¥te', 'Niveau', 'Famille et type',\n",
      "       'Famille', 'Type', 'Nom de la famille', 'Nom du type', 'ID du type',\n",
      "       'El√©vation du niveau de r√©f√©rence', 'El√©vation en haut',\n",
      "       'Rotation de la section', 'Orientation', 'D√©calage du niveau d'arriv√©e',\n",
      "       'D√©calage du niveau de d√©part', 'El√©vation √† la base',\n",
      "       'Longueur de coupe', 'Niveau de r√©f√©rence', 'Utilisation structurelle',\n",
      "       'Plan de construction', 'Longueur', 'Identifiant', 'Nature_Ouvrage',\n",
      "       'Batiment', 'Perimetre coffrage', 'Vtotal', 'hauteur_section',\n",
      "       'largeur_section', 'NIVEAU_STRUCTURE', 'Image du type',\n",
      "       'Note d'identification', 'Type: Type pr√©d√©fini d'IFC',\n",
      "       'Exporter le type au format IFC sous', 'Exporter le type au format IFC',\n",
      "       'Type IfcGUID', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL',\n",
      "       'Description', 'Nom de code', 'Identifiant du nom de la coupe',\n",
      "       'Forme de coupe', 'Titre OmniClass', 'Num√©ro OmniClass',\n",
      "       'Description de l'assemblage', 'Code d'assemblage', 'Marque de type',\n",
      "       'Protection contre l'incendie', 'Co√ªt', 'h', 'b', 'Type Ossature',\n",
      "       'Type d'attachement de fin', 'Type d'attachement de d√©but',\n",
      "       'LG_DECAISSE', 'ht-decaisse'],\n",
      "      dtype='object')\n",
      "\n",
      "Poteaux DataFrame Preview:\n",
      "Index(['Id', '011EC_Lot', '012EC_Ouvrage', '013EC_Localisation',\n",
      "       '014EC_Mode Constructif', 'Nom', 'AI', 'AS', 'Hauteur', 'Longueur',\n",
      "       'Partie inf√©rieure attach√©e', 'Partie sup√©rieure attach√©e',\n",
      "       'Sols en intersection', 'Sols coup√©s (u)', 'Sols coup√©s (Ids)',\n",
      "       'Sols coupants (u)', 'Sols coupants (Ids)', 'Poutres en intersection',\n",
      "       'Poutres coup√©s (u)', 'Poutres coup√©s (Ids)', 'Poutres coupants (u)',\n",
      "       'Poutres coupants (Ids)', 'Image', 'Style de poteau', 'Cat√©gorie',\n",
      "       'Type pr√©d√©fini d'IFC', 'Exporter au format IFC sous',\n",
      "       'Exporter au format IFC', 'IfcGUID', 'A une association',\n",
      "       'Enrobage d'armature - Face inf√©rieure',\n",
      "       'Enrobage d'armature - Face sup√©rieure',\n",
      "       'Enrobage d'armature - Autres faces', 'Variantes', 'Volume',\n",
      "       'Phase de d√©molition', 'Phase de cr√©ation', 'Commentaires',\n",
      "       'Mat√©riau structurel', 'Marque d'emplacement du poteau', 'ID h√¥te',\n",
      "       'D√©calage sup√©rieur', 'D√©calage inf√©rieur', 'Niveau sup√©rieur',\n",
      "       'Niveau de base', 'Niveau', 'Famille et type', 'Famille', 'Type',\n",
      "       'Nom de la famille', 'Nom du type', 'ID du type',\n",
      "       'Se d√©place avec les quadrillages', 'Identifiant', 'Limite de pi√®ce',\n",
      "       'Batiment', 'NIVEAU_STRUCTURE', 'Image du type',\n",
      "       'Note d'identification', 'Type: Type pr√©d√©fini d'IFC',\n",
      "       'Exporter le type au format IFC sous', 'Exporter le type au format IFC',\n",
      "       'Type IfcGUID', 'Mod√®le', 'Fabricant', 'Commentaires du type', 'URL',\n",
      "       'Description', 'Nom de code', 'Identifiant du nom de la coupe',\n",
      "       'Forme de coupe', 'Titre OmniClass', 'Num√©ro OmniClass',\n",
      "       'Description de l'assemblage', 'Code d'assemblage', 'Marque de type',\n",
      "       'Co√ªt', 'Nature_Ouvrage', 'Diam√®tre poteau', 'h', 'b',\n",
      "       'hauteur_section', 'largeur_section',\n",
      "       'D√©calage de l'attachement √† la base',\n",
      "       'Justification de l'attachement en bas',\n",
      "       'D√©calage de l'attachement en haut',\n",
      "       'Justification de l'attachement en haut'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import excel_data_loader\n",
    "data = excel_data_loader.load_data(maquettes_path)\n",
    "\n",
    "\n",
    "# Access the DataFrames\n",
    "murs_df = data.get('Murs')\n",
    "sols_df = data.get('Sols')\n",
    "poutres_df = data.get('Poutres')\n",
    "poteaux_df = data.get('Poteaux')\n",
    "\n",
    "# Display basic info\n",
    "print(\"Murs DataFrame Shape:\", murs_df.shape)\n",
    "print(\"Sols DataFrame Shape:\", sols_df.shape)\n",
    "print(\"Poutres DataFrame Shape:\", poutres_df.shape)\n",
    "print(\"Poteaux DataFrame Shape:\", poteaux_df.shape)\n",
    "\n",
    "# Display column names\n",
    "for sheet_name, df in data.items():\n",
    "    print(f\"\\n{sheet_name} DataFrame Preview:\")\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd8d5c",
   "metadata": {},
   "source": [
    "## Part 2: Data Preprocessing and Relationship Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c47058e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Murs: Kept 31/31 columns | New shape: (312, 31)\n",
      "‚úÖ Sols: Kept 23/23 columns | New shape: (107, 23)\n",
      "‚úÖ Poutres: Kept 23/23 columns | New shape: (246, 23)\n",
      "‚úÖ Poteaux: Kept 26/26 columns | New shape: (68, 26)\n",
      "\n",
      "Cleaned DataFrame Shapes:\n",
      "Murs: (312, 31)\n",
      "Sols: (107, 23)\n",
      "Poutres: (246, 23)\n",
      "Poteaux: (68, 26)\n",
      "\n",
      "Column Names in Cleaned DataFrames:\n",
      "\n",
      "Columns in Murs:\n",
      "Index(['Id', '011EC_Lot', '012EC_Ouvrage', '013EC_Localisation',\n",
      "       '014EC_Mode Constructif', 'Hauteur', 'Epaisseur', 'AI', 'AS',\n",
      "       'Sols en intersection', 'Sols coup√©s (u)', 'Sols coup√©s (Ids)',\n",
      "       'Sols coupants (u)', 'Sols coupants (Ids)', 'Sol au-dessus',\n",
      "       'Sol en-dessous', 'Fen√™tres', 'Portes', 'Ouvertures', 'Murs imbriqu√©s',\n",
      "       'Mur multicouche', 'Profil modifi√©', 'Extension inf√©rieure',\n",
      "       'Extension sup√©rieure', 'Volume', 'Surface',\n",
      "       'Partie inf√©rieure attach√©e', 'Partie sup√©rieure attach√©e',\n",
      "       'D√©calage sup√©rieur', 'D√©calage inf√©rieur', 'Mat√©riau structurel'],\n",
      "      dtype='object')\n",
      "\n",
      "Columns in Sols:\n",
      "Index(['Id', '011EC_Lot', '012EC_Ouvrage', '013EC_Localisation',\n",
      "       '014EC_Mode Constructif', 'Murs en intersection', 'Murs coup√©s (u)',\n",
      "       'Murs coup√©s (Ids)', 'Murs coupants (u)', 'Murs coupants (Ids)',\n",
      "       'Poutres en intersection', 'Poutres coup√©s (u)', 'Poutres coup√©s (Ids)',\n",
      "       'Poutres coupants (u)', 'Poutres coupants (Ids)',\n",
      "       'Poteaux en intersection', 'Poteaux coup√©s (u)', 'Poteaux coup√©s (Ids)',\n",
      "       'Poteaux coupants (u)', 'Poteaux coupants (Ids)', 'Volume', 'Surface',\n",
      "       'Mat√©riau structurel'],\n",
      "      dtype='object')\n",
      "\n",
      "Columns in Poutres:\n",
      "Index(['Id', '011EC_Lot', '012EC_Ouvrage', '013EC_Localisation',\n",
      "       '014EC_Mode Constructif', 'AI', 'AS', 'Hauteur totale', 'Hauteur',\n",
      "       'Sols en intersection', 'Sols coup√©s (u)', 'Sols coup√©s (Ids)',\n",
      "       'Sols coupants (u)', 'Sols coupants (Ids)', 'Sol au-dessus',\n",
      "       'Sol en-dessous', 'Poteaux en intersection', 'Poteaux coup√©s (u)',\n",
      "       'Poteaux coup√©s (Ids)', 'Poteaux coupants (u)', 'Mat√©riau structurel',\n",
      "       'El√©vation √† la base', 'Longueur de coupe'],\n",
      "      dtype='object')\n",
      "\n",
      "Columns in Poteaux:\n",
      "Index(['Id', '011EC_Lot', '012EC_Ouvrage', '013EC_Localisation',\n",
      "       '014EC_Mode Constructif', 'Nom', 'AI', 'AS', 'Hauteur', 'Longueur',\n",
      "       'Partie inf√©rieure attach√©e', 'Partie sup√©rieure attach√©e',\n",
      "       'Sols en intersection', 'Sols coup√©s (u)', 'Sols coup√©s (Ids)',\n",
      "       'Sols coupants (u)', 'Sols coupants (Ids)', 'Poutres en intersection',\n",
      "       'Poutres coup√©s (u)', 'Poutres coup√©s (Ids)', 'Poutres coupants (u)',\n",
      "       'Poutres coupants (Ids)', 'Mat√©riau structurel',\n",
      "       'Marque d'emplacement du poteau', 'D√©calage sup√©rieur',\n",
      "       'D√©calage inf√©rieur'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "### ====================\n",
    "### Cleaned DataFrames Loading\n",
    "### ====================\n",
    "import excel_essential_columns_cleaner\n",
    "dataframes = excel_essential_columns_cleaner.load_and_clean_data(maquettes_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5887172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataframes))  # Debugging check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a3e656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Murs': [], 'Sols': [], 'Poutres': [], 'Poteaux': []}\n",
      "(312, 31) (107, 23) (246, 23) (68, 26)\n"
     ]
    }
   ],
   "source": [
    "### ====================\n",
    "### Verify that all critical columns are present\n",
    "### ====================\n",
    "import excel_critical_columns_validator\n",
    "\n",
    "dataframes, missing_critical, murs_df, sols_df, poutres_df, poteaux_df = excel_critical_columns_validator.validate_critical_columns(dataframes)\n",
    "\n",
    "# Print results\n",
    "print(missing_critical)\n",
    "print(murs_df.shape, sols_df.shape, poutres_df.shape, poteaux_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb458ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Processing Murs...\n",
      "‚úÖ Processing Column Pair: Sols coup√©s (u) ‚Üí Sols coup√©s (Ids)\n",
      "‚úÖ Cleaned 0 rows in Sols coup√©s (Ids)\n",
      "‚úÖ Processing Column Pair: Sols coupants (u) ‚Üí Sols coupants (Ids)\n",
      "‚úÖ Cleaned 0 rows in Sols coupants (Ids)\n",
      "üö® Skipping: Murs coup√©s (u) or Murs coup√©s (Ids) not found in Murs\n",
      "üö® Skipping: Murs coupants (u) or Murs coupants (Ids) not found in Murs\n",
      "üö® Skipping: Poutres coup√©s (u) or Poutres coup√©s (Ids) not found in Murs\n",
      "üö® Skipping: Poutres coupants (u) or Poutres coupants (Ids) not found in Murs\n",
      "üö® Skipping: Poteaux coup√©s (u) or Poteaux coup√©s (Ids) not found in Murs\n",
      "üö® Skipping: Poteaux coupants (u) or Poteaux coupants (Ids) not found in Murs\n",
      "\n",
      "üîç Processing Sols...\n",
      "üö® Skipping: Sols coup√©s (u) or Sols coup√©s (Ids) not found in Sols\n",
      "üö® Skipping: Sols coupants (u) or Sols coupants (Ids) not found in Sols\n",
      "‚úÖ Processing Column Pair: Murs coup√©s (u) ‚Üí Murs coup√©s (Ids)\n",
      "‚úÖ Cleaned 0 rows in Murs coup√©s (Ids)\n",
      "‚úÖ Processing Column Pair: Murs coupants (u) ‚Üí Murs coupants (Ids)\n",
      "‚úÖ Cleaned 0 rows in Murs coupants (Ids)\n",
      "‚úÖ Processing Column Pair: Poutres coup√©s (u) ‚Üí Poutres coup√©s (Ids)\n",
      "‚úÖ Cleaned 0 rows in Poutres coup√©s (Ids)\n",
      "‚úÖ Processing Column Pair: Poutres coupants (u) ‚Üí Poutres coupants (Ids)\n",
      "‚úÖ Cleaned 0 rows in Poutres coupants (Ids)\n",
      "‚úÖ Processing Column Pair: Poteaux coup√©s (u) ‚Üí Poteaux coup√©s (Ids)\n",
      "‚úÖ Cleaned 0 rows in Poteaux coup√©s (Ids)\n",
      "‚úÖ Processing Column Pair: Poteaux coupants (u) ‚Üí Poteaux coupants (Ids)\n",
      "‚úÖ Cleaned 0 rows in Poteaux coupants (Ids)\n",
      "\n",
      "üîç Processing Poutres...\n",
      "‚úÖ Processing Column Pair: Sols coup√©s (u) ‚Üí Sols coup√©s (Ids)\n",
      "‚úÖ Cleaned 0 rows in Sols coup√©s (Ids)\n",
      "‚úÖ Processing Column Pair: Sols coupants (u) ‚Üí Sols coupants (Ids)\n",
      "‚úÖ Cleaned 0 rows in Sols coupants (Ids)\n",
      "üö® Skipping: Murs coup√©s (u) or Murs coup√©s (Ids) not found in Poutres\n",
      "üö® Skipping: Murs coupants (u) or Murs coupants (Ids) not found in Poutres\n",
      "üö® Skipping: Poutres coup√©s (u) or Poutres coup√©s (Ids) not found in Poutres\n",
      "üö® Skipping: Poutres coupants (u) or Poutres coupants (Ids) not found in Poutres\n",
      "‚úÖ Processing Column Pair: Poteaux coup√©s (u) ‚Üí Poteaux coup√©s (Ids)\n",
      "‚úÖ Cleaned 0 rows in Poteaux coup√©s (Ids)\n",
      "üö® Skipping: Poteaux coupants (u) or Poteaux coupants (Ids) not found in Poutres\n",
      "\n",
      "üîç Processing Poteaux...\n",
      "‚úÖ Processing Column Pair: Sols coup√©s (u) ‚Üí Sols coup√©s (Ids)\n",
      "‚úÖ Cleaned 0 rows in Sols coup√©s (Ids)\n",
      "‚úÖ Processing Column Pair: Sols coupants (u) ‚Üí Sols coupants (Ids)\n",
      "‚úÖ Cleaned 0 rows in Sols coupants (Ids)\n",
      "üö® Skipping: Murs coup√©s (u) or Murs coup√©s (Ids) not found in Poteaux\n",
      "üö® Skipping: Murs coupants (u) or Murs coupants (Ids) not found in Poteaux\n",
      "‚úÖ Processing Column Pair: Poutres coup√©s (u) ‚Üí Poutres coup√©s (Ids)\n",
      "‚úÖ Cleaned 0 rows in Poutres coup√©s (Ids)\n",
      "‚úÖ Processing Column Pair: Poutres coupants (u) ‚Üí Poutres coupants (Ids)\n",
      "‚úÖ Cleaned 0 rows in Poutres coupants (Ids)\n",
      "üö® Skipping: Poteaux coup√©s (u) or Poteaux coup√©s (Ids) not found in Poteaux\n",
      "üö® Skipping: Poteaux coupants (u) or Poteaux coupants (Ids) not found in Poteaux\n"
     ]
    }
   ],
   "source": [
    "import excel_clean_ids_columns\n",
    "\n",
    "df_dict = {\n",
    "    'Murs': murs_df,\n",
    "    'Sols': sols_df,\n",
    "    'Poutres': poutres_df,\n",
    "    'Poteaux': poteaux_df\n",
    "}\n",
    "\n",
    "cleaned_dfs = excel_clean_ids_columns.clean_ids_columns(df_dict)\n",
    "\n",
    "# Update DataFrames\n",
    "murs_df = cleaned_dfs['Murs']\n",
    "sols_df = cleaned_dfs['Sols']\n",
    "poutres_df = cleaned_dfs['Poutres']\n",
    "poteaux_df = cleaned_dfs['Poteaux']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relationship_features(main_df, related_df, relation_config, prefix):\n",
    "    \"\"\"\n",
    "    Robust relationship feature creation with:\n",
    "    - Better column name handling\n",
    "    - NaN/empty value protection\n",
    "    - Clear validation\n",
    "    \"\"\"\n",
    "    for relation_col, feature_cols in relation_config.items():\n",
    "        # 1. Find matching column (case insensitive, handles typos)\n",
    "        matching_cols = [col for col in main_df.columns\n",
    "                        if relation_col.lower() in col.lower()]\n",
    "\n",
    "        if not matching_cols:\n",
    "            print(f\"‚ö†Ô∏è No column matching '{relation_col}' found in DataFrame\")\n",
    "            continue\n",
    "\n",
    "        actual_col = matching_cols[0]\n",
    "        print(f\"üîß Processing {actual_col} (matched from {relation_col})\")\n",
    "\n",
    "        # 2. Clean and explode relationship IDs\n",
    "        try:\n",
    "            # Convert to string and clean\n",
    "            main_df[actual_col] = main_df[actual_col].astype(str)\n",
    "            main_df[actual_col] = (main_df[actual_col]\n",
    "                                  .str.replace(r'[\\[\\]]', '', regex=True)\n",
    "                                  .replace(['nan', 'None', 'NaN', ''], '0'))\n",
    "\n",
    "            # Explode and convert to integers\n",
    "            exploded = main_df[[actual_col]].explode(actual_col)\n",
    "            exploded[actual_col] = pd.to_numeric(exploded[actual_col], errors='coerce')\n",
    "            exploded = exploded.dropna()\n",
    "\n",
    "            if exploded.empty:\n",
    "                print(f\"‚ö†Ô∏è No valid relationships in {actual_col}\")\n",
    "                continue\n",
    "\n",
    "            # 3. Merge with related features\n",
    "            for feature in feature_cols:\n",
    "                if feature not in related_df.columns:\n",
    "                    print(f\"‚ö†Ô∏è Feature '{feature}' not in related DataFrame\")\n",
    "                    continue\n",
    "\n",
    "                # Perform the merge\n",
    "                merged = exploded.merge(\n",
    "                    related_df[[feature]],\n",
    "                    left_on=actual_col,\n",
    "                    right_index=True,\n",
    "                    how='left'\n",
    "                )\n",
    "\n",
    "                # Aggregate back to original\n",
    "                new_col = f\"{prefix}_{feature}\"\n",
    "                if np.issubdtype(merged[feature].dtype, np.number):\n",
    "                    main_df[new_col] = merged.groupby(merged.index)[feature].mean()\n",
    "                else:\n",
    "                    main_df[new_col] = merged.groupby(merged.index)[feature].agg(\n",
    "                        lambda x: x.mode()[0] if not x.empty else np.nan\n",
    "                    )\n",
    "\n",
    "                print(f\"‚úÖ Created {new_col}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {actual_col}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82837828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your relationship configs (corrected for typos)\n",
    "mur_relations = {\n",
    "    'Sols coup√©s (Ids)': ['Hauteur', 'Epaisseur', 'Volume', 'Surface'],\n",
    "    'Sols coupants (Ids)': ['Hauteur', 'Epaisseur', 'Volume', 'Surface']\n",
    "}\n",
    "\n",
    "# Process with corrected function\n",
    "print(\"\\nProcessing Murs relationships:\")\n",
    "murs_df = create_relationship_features(murs_df, sols_df, mur_relations, 'sol')\n",
    "\n",
    "# Verify\n",
    "print(\"\\nCreated features in Murs:\")\n",
    "print([col for col in murs_df.columns if col.startswith('sol_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2be75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_validation():\n",
    "    \"\"\"Run complete validation suite with robust checks\"\"\"\n",
    "    dfs = {\n",
    "        'Murs': murs_df,\n",
    "        'Sols': sols_df,\n",
    "        'Poutres': poutres_df,\n",
    "        'Poteaux': poteaux_df\n",
    "    }\n",
    "\n",
    "    # 1. Basic DataFrame verification\n",
    "    print(\"=\"*50 + \"\\nBasic DataFrame Verification\\n\" + \"=\"*50)\n",
    "    for name, df in dfs.items():\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"\\n‚ùå {name}: Not a DataFrame\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüîç {name} DataFrame:\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "        # Check relationship columns\n",
    "        rel_cols = [c for c in df.columns if 'coup' in c.lower()]\n",
    "        print(f\"\\nRelationship columns ({len(rel_cols)}):\")\n",
    "        print(rel_cols)\n",
    "\n",
    "        # Check created features\n",
    "        created_features = [c for c in df.columns if any(x in c for x in ['sol_', 'mur_', 'poutre_', 'poteau_'])]\n",
    "        print(f\"\\nCreated features ({len(created_features)}):\")\n",
    "        if created_features:\n",
    "            print(df[created_features].head(2))\n",
    "        else:\n",
    "            print(\"No relationship features created\")\n",
    "\n",
    "    # 2. Detailed relationship validation\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\nDetailed Relationship Validation\\n\" + \"=\"*50)\n",
    "\n",
    "    def safe_validate(main_df, related_df, relation_col, prefix):\n",
    "        \"\"\"Validate relationships with error handling\"\"\"\n",
    "        try:\n",
    "            if relation_col not in main_df.columns:\n",
    "                print(f\"‚ùå Missing relation column: {relation_col}\")\n",
    "                return\n",
    "\n",
    "            created_cols = [f\"{prefix}_{f}\" for f in ['Hauteur', 'Epaisseur', 'Volume', 'Surface']\n",
    "                          if f\"{prefix}_{f}\" in main_df.columns]\n",
    "\n",
    "            if not created_cols:\n",
    "                print(f\"‚ö†Ô∏è No features created for {relation_col}\")\n",
    "                return\n",
    "\n",
    "            print(f\"\\n‚úÖ Validating {relation_col}:\")\n",
    "            print(f\"Created {len(created_cols)} features\")\n",
    "            print(\"Sample values:\")\n",
    "            print(main_df[created_cols].head(2))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Validation failed for {relation_col}: {str(e)}\")\n",
    "\n",
    "    # Validate each relationship type\n",
    "    if isinstance(murs_df, pd.DataFrame):\n",
    "        safe_validate(murs_df, sols_df, 'Sols coup√©s (Ids)', 'sol')\n",
    "        safe_validate(murs_df, sols_df, 'Sols coupants (Ids)', 'sol')\n",
    "\n",
    "    if isinstance(sols_df, pd.DataFrame):\n",
    "        safe_validate(sols_df, murs_df, 'Murs coup√©s (Ids)', 'murs')\n",
    "        safe_validate(sols_df, poutres_df, 'Poutres coup√©s (Ids)', 'poutres')\n",
    "\n",
    "    if isinstance(poutres_df, pd.DataFrame):\n",
    "        safe_validate(poutres_df, sols_df, 'Sols coup√©s (Ids)', 'sol')\n",
    "        safe_validate(poutres_df, poteaux_df, 'Poteaux coup√©s (Ids)', 'poteaux')\n",
    "\n",
    "    if isinstance(poteaux_df, pd.DataFrame):\n",
    "        safe_validate(poteaux_df, sols_df, 'Sols coup√©s (Ids)', 'sol')\n",
    "        safe_validate(poteaux_df, poutres_df, 'Poutres coup√©s (Ids)', 'poutres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03686e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f4d6b",
   "metadata": {},
   "source": [
    "## Part 3: Feature Engineering and Target Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd559d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for Murs DataFrame\n",
    "# We'll exclude the target columns and ID columns from features\n",
    "excluded_features = target_columns + ['Id', 'Sols coup√©s (Ids)', 'Sols coupants (Ids)']\n",
    "features = [col for col in mur_df.columns if col not in excluded_features]\n",
    "\n",
    "# Separate features and targets\n",
    "X = mur_df[features]\n",
    "y = mur_df[target_columns]\n",
    "\n",
    "# Handle categorical features (text with special French characters)\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "numeric_cols = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Numeric transformer\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical transformer\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# For multi-label classification, we'll use separate models for each target\n",
    "# Or we can combine them into a single target (less recommended due to different natures)\n",
    "# Here we'll proceed with separate models\n",
    "\n",
    "# Get feature names after one-hot encoding\n",
    "# For numeric features\n",
    "numeric_feature_names = numeric_cols.tolist()\n",
    "\n",
    "# For categorical features\n",
    "if len(categorical_cols) > 0:\n",
    "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    categorical_feature_names = ohe.get_feature_names_out(categorical_cols).tolist()\n",
    "    all_feature_names = numeric_feature_names + categorical_feature_names\n",
    "else:\n",
    "    all_feature_names = numeric_feature_names\n",
    "\n",
    "print(f\"Total features after preprocessing: {len(all_feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95fecb",
   "metadata": {},
   "source": [
    "## Part 4: Exploratory Data Analysis and Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for each target variable\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# For numeric features only (correlation requires numeric data)\n",
    "numeric_df = X[numeric_cols]\n",
    "\n",
    "# Add targets to the numeric_df for correlation\n",
    "for target in target_columns:\n",
    "    if target in mur_df.columns:\n",
    "        # Encode target for correlation\n",
    "        le = LabelEncoder()\n",
    "        encoded_target = le.fit_transform(mur_df[target])\n",
    "        numeric_df[target] = encoded_target\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot heatmap for each target\n",
    "for i, target in enumerate(target_columns, 1):\n",
    "    if target in numeric_df.columns:\n",
    "        plt.subplot(2, 2, i)\n",
    "        target_corr = corr_matrix[target].sort_values(ascending=False)\n",
    "        sns.barplot(x=target_corr.values[1:11], y=target_corr.index[1:11])\n",
    "        plt.title(f'Top 10 Features Correlated with {target}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Analyze distribution of target variables\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, target in enumerate(target_columns, 1):\n",
    "    if target in mur_df.columns:\n",
    "        plt.subplot(2, 2, i)\n",
    "        sns.countplot(y=mur_df[target], order=mur_df[target].value_counts().index)\n",
    "        plt.title(f'Distribution of {target}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# SHAP analysis for feature importance (sample for one target)\n",
    "if '012EC_Ouvrage' in mur_df.columns:\n",
    "    # Sample a subset for faster SHAP computation\n",
    "    X_sample = X_processed[:1000] if X_processed.shape[0] > 1000 else X_processed\n",
    "\n",
    "    # Train a model for this target\n",
    "    y_target = mur_df['012EC_Ouvrage']\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_target)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_sample, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Compute SHAP values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "    # Plot summary\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=all_feature_names, class_names=le.classes_)\n",
    "    plt.title('SHAP Summary for 012EC_Ouvrage Prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a9247",
   "metadata": {},
   "source": [
    "## Part 5: Model Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate models for a target variable\n",
    "def train_evaluate_models(X, y, target_name, models):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple models for a target variable\n",
    "\n",
    "    Args:\n",
    "        X: Features (processed)\n",
    "        y: Target variable\n",
    "        target_name: Name of the target variable\n",
    "        models: Dictionary of models to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of model performances\n",
    "    \"\"\"\n",
    "    # Encode target if categorical\n",
    "    if y.dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining {model_name} for {target_name}...\")\n",
    "\n",
    "        try:\n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Evaluate\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "            # Store results\n",
    "            results[model_name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': report['weighted avg']['precision'],\n",
    "                'recall': report['weighted avg']['recall'],\n",
    "                'f1': report['weighted avg']['f1-score']\n",
    "            }\n",
    "\n",
    "            print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            # Save model based on type\n",
    "            if hasattr(model, 'layers'):  # Keras model\n",
    "                model_path = f\"dlmodels/{target_name}_{model_name}.h5\"\n",
    "                model.save(model_path)\n",
    "            elif 'boost' in model_name.lower() or 'forest' in model_name.lower():\n",
    "                model_path = f\"mlmodels/{target_name}_{model_name}.pkl\"\n",
    "                import joblib\n",
    "                joblib.dump(model, model_path)\n",
    "            else:\n",
    "                model_path = f\"simplemodels/{target_name}_{model_name}.pkl\"\n",
    "                import joblib\n",
    "                joblib.dump(model, model_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {model_name}: {e}\")\n",
    "            results[model_name] = None\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "# Add a simple neural network\n",
    "def create_nn_model(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(0.001),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# For each target variable, train and evaluate models\n",
    "all_results = {}\n",
    "\n",
    "for target in target_columns:\n",
    "    if target in mur_df.columns:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training models for target: {target}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        y_target = mur_df[target]\n",
    "\n",
    "        # Skip if all values are the same\n",
    "        if len(y_target.unique()) == 1:\n",
    "            print(f\"Skipping {target} - only one class present.\")\n",
    "            continue\n",
    "\n",
    "        # Add neural network to models\n",
    "        output_dim = len(y_target.unique())\n",
    "        nn_model = create_nn_model(X_processed.shape[1], output_dim)\n",
    "        models['NeuralNetwork'] = nn_model\n",
    "\n",
    "        # Train and evaluate\n",
    "        results = train_evaluate_models(X_processed, y_target, target, models)\n",
    "        all_results[target] = results\n",
    "\n",
    "        # Remove NN for next target (to recreate with correct output dim)\n",
    "        del models['NeuralNetwork']\n",
    "\n",
    "        # Plot model comparison\n",
    "        if results:\n",
    "            df_results = pd.DataFrame(results).T\n",
    "            df_results['accuracy'].plot(kind='bar', title=f'Model Accuracy for {target}')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Display all results\n",
    "for target, results in all_results.items():\n",
    "    print(f\"\\nResults for {target}:\")\n",
    "    if results:\n",
    "        display(pd.DataFrame(results).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b79ed",
   "metadata": {},
   "source": [
    "## Part 6: Model Interpretation and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490629b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interpret best model for each target\n",
    "def interpret_best_model(target, results, X_processed, y_target):\n",
    "    \"\"\"\n",
    "    Interpret the best model for a target using SHAP\n",
    "\n",
    "    Args:\n",
    "        target: Target variable name\n",
    "        results: Dictionary of model results\n",
    "        X_processed: Processed features\n",
    "        y_target: Target values\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "\n",
    "    # Find best model by accuracy\n",
    "    df_results = pd.DataFrame(results).T\n",
    "    best_model_name = df_results['accuracy'].idxmax()\n",
    "    best_model_accuracy = df_results.loc[best_model_name, 'accuracy']\n",
    "\n",
    "    print(f\"\\nInterpreting best model for {target}: {best_model_name} (Accuracy: {best_model_accuracy:.4f})\")\n",
    "\n",
    "    # Load the best model\n",
    "    if 'NeuralNetwork' in best_model_name:\n",
    "        model_path = f\"dlmodels/{target}_{best_model_name}.h5\"\n",
    "        best_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "        # For neural networks, we'll use a different explainer\n",
    "        # Sample data for faster computation\n",
    "        X_sample = X_processed[:100] if X_processed.shape[0] > 100 else X_processed\n",
    "\n",
    "        # Create a SHAP explainer\n",
    "        explainer = shap.DeepExplainer(best_model, X_sample)\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "        # Plot summary\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X_sample, feature_names=all_feature_names)\n",
    "        plt.title(f'SHAP Summary for {target} ({best_model_name})')\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        if 'boost' in best_model_name.lower() or 'forest' in best_model_name.lower():\n",
    "            model_path = f\"mlmodels/{target}_{best_model_name}.pkl\"\n",
    "        else:\n",
    "            model_path = f\"simplemodels/{target}_{best_model_name}.pkl\"\n",
    "\n",
    "        import joblib\n",
    "        best_model = joblib.load(model_path)\n",
    "\n",
    "        # Create SHAP explainer\n",
    "        X_sample = X_processed[:100] if X_processed.shape[0] > 100 else X_processed\n",
    "\n",
    "        if hasattr(best_model, 'predict_proba'):\n",
    "            explainer = shap.TreeExplainer(best_model)\n",
    "            shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "            # Plot summary\n",
    "            plt.figure()\n",
    "            shap.summary_plot(shap_values, X_sample, feature_names=all_feature_names)\n",
    "            plt.title(f'SHAP Summary for {target} ({best_model_name})')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Cannot create SHAP explainer for {best_model_name}\")\n",
    "\n",
    "# Interpret best models for each target\n",
    "for target, results in all_results.items():\n",
    "    y_target = mur_df[target]\n",
    "    interpret_best_model(target, results, X_processed, y_target)\n",
    "\n",
    "# Final recommendations\n",
    "print(\"\\nFinal Recommendations:\")\n",
    "print(\"1. The best performing models have been saved in their respective folders (simplemodels/, mlmodels/, dlmodels/)\")\n",
    "print(\"2. SHAP analysis has been provided for model interpretability\")\n",
    "print(\"3. Consider feature engineering based on the correlation and SHAP analysis\")\n",
    "print(\"4. For deployment, use the best model for each target variable\")\n",
    "print(\"5. Monitor model performance over time as new data becomes available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0590ec",
   "metadata": {},
   "source": [
    "## Part 7: Learning Curves and Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c2ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot learning curves\n",
    "def plot_learning_curve(model, X, y, model_name, target_name):\n",
    "    \"\"\"\n",
    "    Plot learning curves for a model\n",
    "\n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        X: Features\n",
    "        y: Target\n",
    "        model_name: Name of the model\n",
    "        target_name: Name of the target variable\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import learning_curve\n",
    "\n",
    "    # If y is categorical, encode it\n",
    "    if y.dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # Create CV training and test scores for various training set sizes\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X, y, cv=5, scoring='accuracy',\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5))\n",
    "\n",
    "    # Calculate mean and standard deviation for training set scores\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "    # Calculate mean and standard deviation for test set scores\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')\n",
    "    plt.plot(train_sizes, test_mean, 'o-', color='green', label='Cross-validation score')\n",
    "\n",
    "    # Draw bands\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\n",
    "\n",
    "    # Create plot\n",
    "    plt.title(f'Learning Curve for {model_name} ({target_name})')\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves for best models\n",
    "for target, results in all_results.items():\n",
    "    if results:\n",
    "        # Find best model by accuracy\n",
    "        df_results = pd.DataFrame(results).T\n",
    "        best_model_name = df_results['accuracy'].idxmax()\n",
    "\n",
    "        # Load the best model\n",
    "        if 'NeuralNetwork' in best_model_name:\n",
    "            model_path = f\"dlmodels/{target}_{best_model_name}.h5\"\n",
    "            best_model = tf.keras.models.load_model(model_path)\n",
    "        elif 'boost' in best_model_name.lower() or 'forest' in best_model_name.lower():\n",
    "            model_path = f\"mlmodels/{target}_{best_model_name}.pkl\"\n",
    "            import joblib\n",
    "            best_model = joblib.load(model_path)\n",
    "        else:\n",
    "            model_path = f\"simplemodels/{target}_{best_model_name}.pkl\"\n",
    "            import joblib\n",
    "            best_model = joblib.load(model_path)\n",
    "\n",
    "        # Get target data\n",
    "        y_target = mur_df[target]\n",
    "\n",
    "        # Plot learning curve\n",
    "        plot_learning_curve(best_model, X_processed, y_target, best_model_name, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad02ea",
   "metadata": {},
   "source": [
    "Explanation and Next Steps\n",
    "This comprehensive solution provides:\n",
    "\n",
    "Data Loading and Preprocessing: Handles the complex relationships between different BIM elements (Murs, Sols, Poutres, Poteaux) and processes the French text data with special characters.\n",
    "\n",
    "Feature Engineering: Creates relationship features between different BIM elements based on their intersections and cuts.\n",
    "\n",
    "Exploratory Data Analysis: Includes correlation analysis and target distribution visualization.\n",
    "\n",
    "Model Training: Evaluates multiple machine learning models (Logistic Regression, Random Forest, SVM, XGBoost, LightGBM) and a neural network for each target variable.\n",
    "\n",
    "Model Interpretation: Uses SHAP values to explain model predictions and identify important features.\n",
    "\n",
    "Model Saving: Saves the best models in appropriate folders based on their complexity (simplemodels/, mlmodels/, dlmodels/).\n",
    "\n",
    "Learning Curves: Visualizes model performance with increasing training data size.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "Deploy the best models for each target variable in your BIM system.\n",
    "\n",
    "Set up monitoring to track model performance over time.\n",
    "\n",
    "Consider implementing an ensemble approach if prediction accuracy needs improvement.\n",
    "\n",
    "Explore more sophisticated deep learning architectures if you have sufficient data.\n",
    "\n",
    "Regularly update the models with new project data to maintain accuracy.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BImpredict2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
